{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets  # for vscode\n",
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#KoBERT\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "# model = AutoModel.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# KoELECTRA-Base\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\")\n",
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-base-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentencEmojiDataset(Dataset):\n",
    "    def __init__(self, directory, tokenizer):\n",
    "        \n",
    "        data = pd.read_csv(directory, encoding='UTF-8')\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = list(data.iloc[:,0])\n",
    "       \n",
    "        emojis = list(data.iloc[:,1])\n",
    "        emojis_unique = list(set(emojis))\n",
    "        \n",
    "        self.labels = [emojis_unique.index(i) for i in emojis]\n",
    " \n",
    "        self.labels_dict = {'key': range(len(emojis_unique)), 'value': emojis_unique}\n",
    "        \n",
    "    def __getitem__(self, i): #collate ì´ì „ ë¯¸ë¦¬ tokenizeë¥¼ ì‹œì¼œì£¼ì\n",
    "        tokenized = self.tokenizer(str(self.sentences[i]), return_tensors='pt')\n",
    "        \n",
    "        #ì•„ë˜ ì„¸ ê°œëŠ” tokenizerê°€ ê¸°ë³¸ì ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ì •ë³´. BERTì˜ inputì´ê¸°ë„ í•¨\n",
    "        input_ids = tokenized['input_ids']\n",
    "        token_type_ids = tokenized['token_type_ids']\n",
    "        attention_mask = tokenized['attention_mask']\n",
    "        \n",
    "#         print(str(self.sentences[i]) +' : ')\n",
    "#         print(tokenized)\n",
    "        \n",
    "        return {'input_ids': input_ids, 'token_type_ids': token_type_ids, \n",
    "                'attention_mask': attention_mask, 'label': self.labels[i]}\n",
    "         \n",
    "    def __len__(self): #data loaderê°€ í•„ìš”ë¡œ í•˜ì—¬ í•„ìˆ˜ì ìœ¼ë¡œ ìˆì–´ì•¼ í•˜ëŠ” í•¨ìˆ˜\n",
    "        return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class collate_fn:\n",
    "    def __init__(self, labels_dict):\n",
    "        self.num_labels = len(labels_dict)\n",
    "        \n",
    "    def __call__(self, batch): #batchëŠ” dataset.getitemì˜ return ê°’ì˜ List. eg. [{}, {}. ...]\n",
    "        #batchë‚´ ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´(í† í° ê°œìˆ˜)ë¥¼ ë¨¼ì € êµ¬í•´ì„œ paddingí•  ìˆ˜ ìˆë„ë¡ í•˜ê¸°\n",
    "        batchlen = [sample['input_ids'].size(1) for sample in batch] #tensorê°’ì„ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì— 1ë²ˆì§¸ ì°¨ì›ì˜ ê¸¸ì´ë¥¼ êµ¬í•¨\n",
    "        maxlen = max(batchlen)\n",
    "        input_ids = []\n",
    "        token_type_ids = []\n",
    "        attention_mask = []\n",
    "        #padding: [5, 6] [0, 0,  ...]ì„ concatenate í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ íŒ¨ë”©\n",
    "        for sample in batch:\n",
    "            pad_len = maxlen - sample['input_ids'].size(1)\n",
    "            pad = torch.zeros((1, pad_len), dtype=torch.int)            \n",
    "            input_ids.append(torch.cat([sample['input_ids'], pad], dim=1))\n",
    "            token_type_ids.append(torch.cat([sample['token_type_ids'], pad], dim=1))\n",
    "            attention_mask.append(torch.cat([sample['attention_mask'], pad], dim=1))\n",
    "        #batch êµ¬ì„±\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "        attention_mask = torch.cat(attention_mask, dim=0)\n",
    "        \n",
    "        #one-hot encoding\n",
    "        #batch ë‚´ ë¼ë²¨ì„ tensorë¡œ ë³€í™˜\n",
    "        tensor_label = torch.tensor([sample['label'] for sample in batch])\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask, tensor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "N                     2349\n",
       "{love}                 141\n",
       "{kind-smile}           119\n",
       "{laughing-out}          96\n",
       "{open-mouth-smile}      84\n",
       "{good-job}              81\n",
       "{solid-sad}             68\n",
       "ğŸ˜­                       49\n",
       "{star}                  38\n",
       "{dunno}                 36\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/twitter_clean.csv', encoding=\"UTF-8\")\n",
    "print(len(df['y'].value_counts()))\n",
    "df['y'].value_counts(sort = True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'] = np.random.randn(df.shape[0], 1)\n",
    "msk = np.random.rand(len(df)) <= 0.7\n",
    "\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "\n",
    "train.to_csv('data/train.csv', index=False)\n",
    "test.to_csv('data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.collate_fn at 0x190f8134a20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = sentencEmojiDataset('data/train.csv', tokenizer)\n",
    "test = sentencEmojiDataset('data/test.csv', tokenizer)\n",
    "\n",
    "train_collate_fn = collate_fn(train.labels_dict)\n",
    "test_collate_fn = collate_fn(test.labels_dict)\n",
    "\n",
    "train_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 20  \n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x190f836a390>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=batch_size, collate_fn=train_collate_fn, shuffle = True, drop_last = True)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size, collate_fn=test_collate_fn, shuffle = False, drop_last = False)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERTClassifier(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  bert,\n",
    "#                  hidden_size = 768,\n",
    "#                  num_classes=2,\n",
    "#                  dr_rate=None,\n",
    "#                  params=None):\n",
    "#         super(BERTClassifier, self).__init__()\n",
    "#         self.bert = bert\n",
    "#         # do not train bert parameters\n",
    "#         for p in self.bert.parameters():\n",
    "#             p.requires_grad = False\n",
    "#         self.dr_rate = dr_rate\n",
    "                 \n",
    "#         self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        \n",
    "#         if dr_rate:\n",
    "#             self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "#     def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "#         #eval: drop out ì¤‘ì§€, batch norm ê³ ì •ê³¼ ê°™ì´ evaluationìœ¼ë¡œ ëª¨ë¸ ë³€ê²½\n",
    "#         self.bert.eval()\n",
    "#         #gradient ê³„ì‚°ì„ ì¤‘ì§€\n",
    "#         with torch.no_grad():\n",
    "#             x = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
    "# #         x = self.dropout(pooler)\n",
    "#         return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELECTRAClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 electra,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        \n",
    "        super(ELECTRAClassifier, self).__init__()\n",
    "        \n",
    "        self.electra = electra\n",
    "        \n",
    "        # do not train electra parameters\n",
    "        for p in self.electra.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        self.dr_rate = dr_rate\n",
    "        \n",
    "        \n",
    "        #self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        \n",
    "#         # ë°©ë²• 1 -> forwardì—ì„œ ì²˜ë¦¬í•´ì¤˜ì•¼ í•¨. \n",
    "#         self.classifier1 = nn.Linear(hidden_size, 100) # y = Wx\n",
    "#         self.classifier2 = nn.Linear(100, num_classes) # z = Uy\n",
    "#         #layer ì¶”ê°€ ì‹œ activation functionì„ ì£¼ì§€ ì•Šìœ¼ë©´ ì˜ë¯¸ê°€ ì—†ìŒ. \n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "        #ë°©ë²• 2 -> forwardì—ì„œ ë³„ë„ ì²˜ë¦¬ í•„ìš” X\n",
    "        self.classifier = nn.Sequential(nn.Linear(hidden_size, 100), nn.ReLU(), nn.Linear(100, num_classes))\n",
    "        \n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        #eval: drop out ì¤‘ì§€, batch norm ê³ ì •ê³¼ ê°™ì´ evaluationìœ¼ë¡œ ëª¨ë¸ ë³€ê²½\n",
    "        self.electra.eval()\n",
    "        \n",
    "        #gradient ê³„ì‚°ì„ ì¤‘ì§€\n",
    "        with torch.no_grad():\n",
    "            #ElectraModelì€ pooled_outputì„ ë¦¬í„´í•˜ì§€ ì•ŠëŠ” ê²ƒì„ ì œì™¸í•˜ê³  BertModelê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤.\n",
    "#            x = self.electra(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
    "\n",
    "            x = self.electra(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :] \n",
    "            #.last_hidden_state[:, 0, :]: [batch , CLS ìœ„ì¹˜, depth]\n",
    "            \n",
    "            #Sentence Embeddingìœ¼ë¡œ ë¬´ì—‡ì„ ë„£ì„ê¹Œ? CLS, average, ... (Bertì˜ ê²½ìš°ì—ëŠ” Sentence BERTë¼ëŠ” ê²Œ ì œì•ˆë˜ì—ˆë‹¤ê³  í•¨)\n",
    "                   \n",
    "        x = self.dropout(x)\n",
    "      \n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(set(list(df.iloc[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BERTClassifier(model,  dr_rate=0.5, num_classes = len(label))\n",
    "model = ELECTRAClassifier(model,  dr_rate=0.2, num_classes = len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizerì™€ schedule ì„¤ì •\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 24,\n",
       " 15,\n",
       " 24,\n",
       " 26,\n",
       " 26,\n",
       " 19,\n",
       " 24,\n",
       " 26,\n",
       " 29,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 10,\n",
       " 11,\n",
       " 26,\n",
       " 26]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5810, 6.5111, 2.1117, 8.6815, 1.8603, 7.8133, 8.6815, 7.1030, 8.6815,\n",
      "        3.2556, 7.8133, 0.8312, 3.5515, 7.8133, 1.3022, 0.9302, 8.6815, 8.6815,\n",
      "        7.1030, 1.2602, 9.7667, 5.5810, 9.7667, 2.6943, 1.0852, 3.1253, 0.0490,\n",
      "        7.8133, 5.2089, 3.5515])\n"
     ]
    }
   ],
   "source": [
    "#Class Imbalance ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ weighted cross entropy \n",
    "class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(train.labels), y = train.labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(class_weights) #([1.0000, 1.0000, 4.0000, 1.0000, 0.5714])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': range(0, 30),\n",
       " 'value': ['ğŸ‘‰',\n",
       "  'ğŸ˜‘',\n",
       "  'ğŸ˜­',\n",
       "  'ğŸ˜…',\n",
       "  '{solid-sad}',\n",
       "  'ğŸ˜’',\n",
       "  'ğŸ˜',\n",
       "  'ğŸ˜‹',\n",
       "  'â—',\n",
       "  '{star}',\n",
       "  'ğŸ™Œ',\n",
       "  '{love}',\n",
       "  '{hope}',\n",
       "  'ğŸ˜£',\n",
       "  '{good-job}',\n",
       "  '{kind-smile}',\n",
       "  'ğŸ™‹',\n",
       "  'ğŸ˜ª',\n",
       "  'ğŸ¤¤',\n",
       "  '{open-mouth-smile}',\n",
       "  'ğŸ‘€',\n",
       "  'ğŸ˜',\n",
       "  'ğŸ˜ˆ',\n",
       "  '{dunno}',\n",
       "  '{laughing-out}',\n",
       "  '{flower}',\n",
       "  'N',\n",
       "  'ğŸ¤¦',\n",
       "  '{mad}',\n",
       "  'ğŸ”¥']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight = class_weights, reduction = 'mean') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1/36 loss 3.4873974323272705 train acc 0.0\n",
      "epoch 1 batch id 2/36 loss 3.425734519958496 train acc 0.015625\n",
      "epoch 1 batch id 3/36 loss 3.4722392559051514 train acc 0.015625\n",
      "epoch 1 batch id 4/36 loss 3.3767757415771484 train acc 0.03125\n",
      "epoch 1 batch id 5/36 loss 3.436350107192993 train acc 0.015625\n",
      "epoch 1 batch id 6/36 loss 3.3984856605529785 train acc 0.0625\n",
      "epoch 1 batch id 7/36 loss 3.404128074645996 train acc 0.015625\n",
      "epoch 1 batch id 8/36 loss 3.4327847957611084 train acc 0.015625\n",
      "epoch 1 batch id 9/36 loss 3.3609459400177 train acc 0.03125\n",
      "epoch 1 batch id 10/36 loss 3.4111757278442383 train acc 0.03125\n",
      "epoch 1 batch id 11/36 loss 3.481961727142334 train acc 0.015625\n",
      "epoch 1 batch id 12/36 loss 3.4477055072784424 train acc 0.015625\n",
      "epoch 1 batch id 13/36 loss 3.4153988361358643 train acc 0.0\n",
      "epoch 1 batch id 14/36 loss 3.4706804752349854 train acc 0.015625\n",
      "epoch 1 batch id 15/36 loss 3.411259651184082 train acc 0.03125\n",
      "epoch 1 batch id 16/36 loss 3.4362828731536865 train acc 0.015625\n",
      "epoch 1 batch id 17/36 loss 3.4401156902313232 train acc 0.015625\n",
      "epoch 1 batch id 18/36 loss 3.4037880897521973 train acc 0.015625\n",
      "epoch 1 batch id 19/36 loss 3.3541295528411865 train acc 0.03125\n",
      "epoch 1 batch id 20/36 loss 3.4013121128082275 train acc 0.015625\n",
      "epoch 1 batch id 21/36 loss 3.4121243953704834 train acc 0.015625\n",
      "epoch 1 batch id 22/36 loss 3.4619221687316895 train acc 0.0\n",
      "epoch 1 batch id 23/36 loss 3.44126033782959 train acc 0.03125\n",
      "epoch 1 batch id 24/36 loss 3.4212586879730225 train acc 0.015625\n",
      "epoch 1 batch id 25/36 loss 3.358691930770874 train acc 0.03125\n",
      "epoch 1 batch id 26/36 loss 3.4094085693359375 train acc 0.0625\n",
      "epoch 1 batch id 27/36 loss 3.3894569873809814 train acc 0.03125\n",
      "epoch 1 batch id 28/36 loss 3.3856558799743652 train acc 0.015625\n",
      "epoch 1 batch id 29/36 loss 3.4468483924865723 train acc 0.03125\n",
      "epoch 1 batch id 30/36 loss 3.3215785026550293 train acc 0.046875\n",
      "epoch 1 batch id 31/36 loss 3.3980159759521484 train acc 0.03125\n",
      "epoch 1 batch id 32/36 loss 3.356678009033203 train acc 0.0\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    loss_sum = 0\n",
    "    model.train()\n",
    "    for batch_id, (input_ids, token_type_ids, attention_mask, tensor_label) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(input_ids, token_type_ids, attention_mask)\n",
    "        loss = loss_fn(out, tensor_label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "#         scheduler.step()  # Update learning rate schedule\n",
    "        batch_acc = calc_accuracy(out, tensor_label)\n",
    "        train_acc += batch_acc\n",
    "        loss_sum += loss.data.cpu().numpy()\n",
    "        #f batch_id % log_interval == 0:\n",
    "        print(\"epoch {} batch id {}/{} loss {} train acc {}\".format(e+1, batch_id+1, len(train_dataloader), loss.data.cpu().numpy(), batch_acc))\n",
    "    print(\"epoch {} train acc {} loss mean {}\".format(e+1, train_acc / (batch_id+1), loss_sum / len(train_dataloader)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, (input_ids, token_type_ids, attention_mask, tensor_label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "            out = model(input_ids, token_type_ids, attention_mask)\n",
    "            test_acc += calc_accuracy(out, tensor_label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
