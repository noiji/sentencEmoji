{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets  # for vscode\n",
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "model = AutoModel.from_pretrained(\"monologg/kobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentencEmojiDataset(Dataset):\n",
    "    def __init__(self, directory, tokenizer):\n",
    "        \n",
    "        data = pd.read_csv(directory, encoding='UTF-8')\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = list(data.iloc[:,0])\n",
    "       \n",
    "        emojis = list(data.iloc[:,1])\n",
    "        emojis_unique = list(set(emojis))\n",
    "        \n",
    "        self.labels = [emojis_unique.index(i) for i in emojis]\n",
    " \n",
    "        self.labels_dict = {'key': range(len(emojis_unique)), 'value': emojis_unique}\n",
    "        \n",
    "    def __getitem__(self, i): #collate 이전 미리 tokenize를 시켜주자\n",
    "        tokenized = self.tokenizer(str(self.sentences[i]), return_tensors='pt')\n",
    "        #아래 세 개는 tokenizer가 기본적으로 반환하는 정보. BERT의 input이기도 함\n",
    "        input_ids = tokenized['input_ids']\n",
    "        token_type_ids = tokenized['token_type_ids']\n",
    "        attention_mask = tokenized['attention_mask']\n",
    "        \n",
    "        return {'input_ids': input_ids, 'token_type_ids': token_type_ids, \n",
    "                'attention_mask': attention_mask, 'label': self.labels[i]}\n",
    "         \n",
    "    def __len__(self): #data loader가 필요로 하여 필수적으로 있어야 하는 함수\n",
    "        return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class collate_fn:\n",
    "    def __init__(self, labels_dict):\n",
    "        self.num_labels = len(labels_dict)\n",
    "        \n",
    "    def __call__(self, batch): #batch는 dataset.getitem의 return 값의 List. eg. [{}, {}. ...]\n",
    "        #batch내 최대 문장 길이(토큰 개수)를 먼저 구해서 padding할 수 있도록 하기\n",
    "        batchlen = [sample['input_ids'].size(1) for sample in batch] #tensor값을 반환하기 때문에 1번째 차원의 길이를 구함\n",
    "        maxlen = max(batchlen)\n",
    "        input_ids = []\n",
    "        token_type_ids = []\n",
    "        attention_mask = []\n",
    "        #padding: [5, 6] [0, 0,  ...]을 concatenate 하는 방식으로 패딩\n",
    "        for sample in batch:\n",
    "            pad_len = maxlen - sample['input_ids'].size(1)\n",
    "            pad = torch.zeros((1, pad_len), dtype=torch.int)            \n",
    "            input_ids.append(torch.cat([sample['input_ids'], pad], dim=1))\n",
    "            token_type_ids.append(torch.cat([sample['token_type_ids'], pad], dim=1))\n",
    "            attention_mask.append(torch.cat([sample['attention_mask'], pad], dim=1))\n",
    "        #batch 구성\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "        attention_mask = torch.cat(attention_mask, dim=0)\n",
    "        \n",
    "        #one-hot encoding\n",
    "        #batch 내 라벨을 tensor로 변환\n",
    "        tensor_label = torch.tensor([sample['label'] for sample in batch])\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask, tensor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "N    1690\n",
       "😂      76\n",
       "😭      33\n",
       "❤      32\n",
       "😍      21\n",
       "🔥      21\n",
       "💜      19\n",
       "✨      18\n",
       "💗      16\n",
       "📸      15\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/twitter_clean.csv', encoding=\"UTF-8\")\n",
    "print(len(df['y'].value_counts()))\n",
    "df['y'].value_counts(sort = True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'] = np.random.randn(df.shape[0], 1)\n",
    "msk = np.random.rand(len(df)) <= 0.7\n",
    "\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "\n",
    "train.to_csv('data/train.csv', index=False)\n",
    "test.to_csv('data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.collate_fn at 0x1f71120d5f8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = sentencEmojiDataset('data/train.csv', tokenizer)\n",
    "test = sentencEmojiDataset('data/test.csv', tokenizer)\n",
    "\n",
    "train_collate_fn = collate_fn(train.labels_dict)\n",
    "test_collate_fn = collate_fn(test.labels_dict)\n",
    "\n",
    "train_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 20  \n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1f7112d22b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=batch_size, collate_fn=train_collate_fn, shuffle = True, drop_last = True)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size, collate_fn=test_collate_fn, shuffle = False, drop_last = False)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        # do not train bert parameters\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        \n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        #eval: drop out 중지, batch norm 고정과 같이 evaluation으로 모델 변경\n",
    "        self.bert.eval()\n",
    "        #gradient 계산을 중지\n",
    "        with torch.no_grad():\n",
    "            x = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
    "#         x = self.dropout(pooler)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(set(list(df.iloc[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(model,  dr_rate=0.5, num_classes = len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer와 schedule 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 5, 11, 11, 23, 8, 23, 11, 20, 23, 11, 23, 4, 11, 20, 11, 11, 17, 5, 20]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.0427,  6.3833,  5.1067,  5.1067,  3.9282,  2.6877,  4.6424,  4.6424,\n",
      "         4.6424,  8.5111, 10.2133,  0.0434,  8.5111,  5.6741,  5.1067,  7.2952,\n",
      "         5.6741,  7.2952,  5.6741,  5.6741,  2.1278,  5.6741,  6.3833,  0.9457,\n",
      "         5.6741,  5.1067,  3.1917,  6.3833,  3.6476,  5.6741])\n"
     ]
    }
   ],
   "source": [
    "#Class Imbalance 문제 해결을 위한 weighted cross entropy \n",
    "class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(train.labels), y = train.labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(class_weights) #([1.0000, 1.0000, 4.0000, 1.0000, 0.5714])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': range(0, 30),\n",
       " 'value': ['😭',\n",
       "  '🙄',\n",
       "  '💕',\n",
       "  '🤤',\n",
       "  '🔥',\n",
       "  '😍',\n",
       "  '😘',\n",
       "  '💙',\n",
       "  '🥺',\n",
       "  '😑',\n",
       "  '🎉',\n",
       "  'N',\n",
       "  '🏻',\n",
       "  '📸',\n",
       "  '😩',\n",
       "  '🤣',\n",
       "  '💗',\n",
       "  '👍',\n",
       "  '✌',\n",
       "  '💯',\n",
       "  '❤',\n",
       "  '❣',\n",
       "  '😁',\n",
       "  '😂',\n",
       "  '😆',\n",
       "  '😊',\n",
       "  '💜',\n",
       "  '💖',\n",
       "  '✨',\n",
       "  '🙏']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight = class_weights, reduction = 'mean') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1/23 loss 3.4549572467803955 train acc 0.0\n",
      "epoch 1 batch id 2/23 loss 3.4629199504852295 train acc 0.0\n",
      "epoch 1 batch id 3/23 loss 3.3897719383239746 train acc 0.03125\n",
      "epoch 1 batch id 4/23 loss 3.4274215698242188 train acc 0.0\n",
      "epoch 1 batch id 5/23 loss 3.281050205230713 train acc 0.0\n",
      "epoch 1 batch id 6/23 loss 3.3971595764160156 train acc 0.015625\n",
      "epoch 1 batch id 7/23 loss 3.3611748218536377 train acc 0.015625\n",
      "epoch 1 batch id 8/23 loss 3.5217983722686768 train acc 0.0\n",
      "epoch 1 batch id 9/23 loss 3.4654290676116943 train acc 0.015625\n",
      "epoch 1 batch id 10/23 loss 3.4421963691711426 train acc 0.0\n",
      "epoch 1 batch id 11/23 loss 3.4965920448303223 train acc 0.0\n",
      "epoch 1 batch id 12/23 loss 3.4427907466888428 train acc 0.0\n",
      "epoch 1 batch id 13/23 loss 3.4203364849090576 train acc 0.015625\n",
      "epoch 1 batch id 14/23 loss 3.410829782485962 train acc 0.03125\n",
      "epoch 1 batch id 15/23 loss 3.4675986766815186 train acc 0.015625\n",
      "epoch 1 batch id 16/23 loss 3.5574512481689453 train acc 0.0\n",
      "epoch 1 batch id 17/23 loss 3.2200756072998047 train acc 0.03125\n",
      "epoch 1 batch id 18/23 loss 3.5008373260498047 train acc 0.0\n",
      "epoch 1 batch id 19/23 loss 3.343245029449463 train acc 0.0\n",
      "epoch 1 batch id 20/23 loss 3.3966736793518066 train acc 0.0\n",
      "epoch 1 batch id 21/23 loss 3.3912367820739746 train acc 0.0\n",
      "epoch 1 batch id 22/23 loss 3.4494829177856445 train acc 0.0\n",
      "epoch 1 batch id 23/23 loss 3.6008076667785645 train acc 0.0\n",
      "epoch 1 train acc 0.007472826086956522 loss mean 3.430514656979105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:33<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.004261363636363636\n",
      "epoch 2 batch id 1/23 loss 3.5113120079040527 train acc 0.0\n",
      "epoch 2 batch id 2/23 loss 3.3815982341766357 train acc 0.0\n",
      "epoch 2 batch id 3/23 loss 3.4776947498321533 train acc 0.0\n",
      "epoch 2 batch id 4/23 loss 3.3555214405059814 train acc 0.015625\n",
      "epoch 2 batch id 5/23 loss 3.3867571353912354 train acc 0.015625\n",
      "epoch 2 batch id 6/23 loss 3.466486930847168 train acc 0.015625\n",
      "epoch 2 batch id 7/23 loss 3.5295748710632324 train acc 0.0\n",
      "epoch 2 batch id 8/23 loss 3.4656667709350586 train acc 0.015625\n",
      "epoch 2 batch id 9/23 loss 3.621302604675293 train acc 0.0\n",
      "epoch 2 batch id 10/23 loss 3.429151773452759 train acc 0.0\n",
      "epoch 2 batch id 11/23 loss 3.4153623580932617 train acc 0.015625\n",
      "epoch 2 batch id 12/23 loss 3.4252240657806396 train acc 0.015625\n",
      "epoch 2 batch id 13/23 loss 3.443650722503662 train acc 0.0\n",
      "epoch 2 batch id 14/23 loss 3.34031343460083 train acc 0.0\n",
      "epoch 2 batch id 15/23 loss 3.3701040744781494 train acc 0.03125\n",
      "epoch 2 batch id 16/23 loss 3.372580051422119 train acc 0.015625\n",
      "epoch 2 batch id 17/23 loss 3.3894972801208496 train acc 0.0\n",
      "epoch 2 batch id 18/23 loss 3.4889135360717773 train acc 0.0\n",
      "epoch 2 batch id 19/23 loss 3.4055802822113037 train acc 0.0\n",
      "epoch 2 batch id 20/23 loss 3.4128665924072266 train acc 0.015625\n",
      "epoch 2 batch id 21/23 loss 3.4659576416015625 train acc 0.0\n",
      "epoch 2 batch id 22/23 loss 3.5838265419006348 train acc 0.0\n",
      "epoch 2 batch id 23/23 loss 3.4454727172851562 train acc 0.015625\n",
      "epoch 2 train acc 0.007472826086956522 loss mean 3.4428006877069888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:33<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.004261363636363636\n",
      "epoch 3 batch id 1/23 loss 3.5000338554382324 train acc 0.015625\n",
      "epoch 3 batch id 2/23 loss 3.488783597946167 train acc 0.0\n",
      "epoch 3 batch id 3/23 loss 3.5340259075164795 train acc 0.0\n",
      "epoch 3 batch id 4/23 loss 3.380797863006592 train acc 0.015625\n",
      "epoch 3 batch id 5/23 loss 3.4941303730010986 train acc 0.0\n",
      "epoch 3 batch id 6/23 loss 3.434579372406006 train acc 0.0\n",
      "epoch 3 batch id 7/23 loss 3.2965009212493896 train acc 0.015625\n",
      "epoch 3 batch id 8/23 loss 3.4618799686431885 train acc 0.0\n",
      "epoch 3 batch id 9/23 loss 3.3332927227020264 train acc 0.03125\n",
      "epoch 3 batch id 10/23 loss 3.5063023567199707 train acc 0.015625\n",
      "epoch 3 batch id 11/23 loss 3.3889665603637695 train acc 0.015625\n",
      "epoch 3 batch id 12/23 loss 3.369523525238037 train acc 0.015625\n",
      "epoch 3 batch id 13/23 loss 3.449453353881836 train acc 0.0\n",
      "epoch 3 batch id 14/23 loss 3.473227024078369 train acc 0.015625\n",
      "epoch 3 batch id 15/23 loss 3.365506172180176 train acc 0.015625\n",
      "epoch 3 batch id 16/23 loss 3.3997538089752197 train acc 0.015625\n",
      "epoch 3 batch id 17/23 loss 3.481679916381836 train acc 0.0\n",
      "epoch 3 batch id 18/23 loss 3.512897253036499 train acc 0.0\n",
      "epoch 3 batch id 19/23 loss 3.5096523761749268 train acc 0.0\n",
      "epoch 3 batch id 20/23 loss 3.4661099910736084 train acc 0.0\n",
      "epoch 3 batch id 21/23 loss 3.408806085586548 train acc 0.0\n",
      "epoch 3 batch id 22/23 loss 3.428387403488159 train acc 0.015625\n",
      "epoch 3 batch id 23/23 loss 3.448519468307495 train acc 0.0\n",
      "epoch 3 train acc 0.008152173913043478 loss mean 3.4405569511911143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:34<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.004261363636363636\n",
      "epoch 4 batch id 1/23 loss 3.4663219451904297 train acc 0.015625\n",
      "epoch 4 batch id 2/23 loss 3.434060573577881 train acc 0.0\n",
      "epoch 4 batch id 3/23 loss 3.3811848163604736 train acc 0.0\n",
      "epoch 4 batch id 4/23 loss 3.375319242477417 train acc 0.015625\n",
      "epoch 4 batch id 5/23 loss 3.318955898284912 train acc 0.03125\n",
      "epoch 4 batch id 6/23 loss 3.4309093952178955 train acc 0.0\n",
      "epoch 4 batch id 7/23 loss 3.2549614906311035 train acc 0.03125\n",
      "epoch 4 batch id 8/23 loss 3.501028060913086 train acc 0.0\n",
      "epoch 4 batch id 9/23 loss 3.624546766281128 train acc 0.0\n",
      "epoch 4 batch id 10/23 loss 3.462674617767334 train acc 0.0\n",
      "epoch 4 batch id 11/23 loss 3.408342123031616 train acc 0.0\n",
      "epoch 4 batch id 12/23 loss 3.6264843940734863 train acc 0.0\n",
      "epoch 4 batch id 13/23 loss 3.4862983226776123 train acc 0.0\n",
      "epoch 4 batch id 14/23 loss 3.349769115447998 train acc 0.03125\n",
      "epoch 4 batch id 15/23 loss 3.556368827819824 train acc 0.0\n",
      "epoch 4 batch id 16/23 loss 3.3728137016296387 train acc 0.015625\n",
      "epoch 4 batch id 17/23 loss 3.4769768714904785 train acc 0.0\n",
      "epoch 4 batch id 18/23 loss 3.4752118587493896 train acc 0.0\n",
      "epoch 4 batch id 19/23 loss 3.338143825531006 train acc 0.0\n",
      "epoch 4 batch id 20/23 loss 3.4432077407836914 train acc 0.0\n",
      "epoch 4 batch id 21/23 loss 3.3660879135131836 train acc 0.015625\n",
      "epoch 4 batch id 22/23 loss 3.4743237495422363 train acc 0.0\n",
      "epoch 4 batch id 23/23 loss 3.402182102203369 train acc 0.03125\n",
      "epoch 4 train acc 0.008152173913043478 loss mean 3.435920580573704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:37<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.004261363636363636\n",
      "epoch 5 batch id 1/23 loss 3.5253114700317383 train acc 0.0\n",
      "epoch 5 batch id 2/23 loss 3.392733097076416 train acc 0.0\n",
      "epoch 5 batch id 3/23 loss 3.4926090240478516 train acc 0.0\n",
      "epoch 5 batch id 4/23 loss 3.52701997756958 train acc 0.015625\n",
      "epoch 5 batch id 5/23 loss 3.433424711227417 train acc 0.0\n",
      "epoch 5 batch id 6/23 loss 3.4745945930480957 train acc 0.015625\n",
      "epoch 5 batch id 7/23 loss 3.363084077835083 train acc 0.015625\n",
      "epoch 5 batch id 8/23 loss 3.4568803310394287 train acc 0.015625\n",
      "epoch 5 batch id 9/23 loss 3.3389763832092285 train acc 0.0\n",
      "epoch 5 batch id 10/23 loss 3.492065668106079 train acc 0.0\n",
      "epoch 5 batch id 11/23 loss 3.3737099170684814 train acc 0.0\n",
      "epoch 5 batch id 12/23 loss 3.533153533935547 train acc 0.0\n",
      "epoch 5 batch id 13/23 loss 3.372884750366211 train acc 0.0\n",
      "epoch 5 batch id 14/23 loss 3.486675262451172 train acc 0.015625\n",
      "epoch 5 batch id 15/23 loss 3.3350839614868164 train acc 0.046875\n",
      "epoch 5 batch id 16/23 loss 3.511228561401367 train acc 0.0\n",
      "epoch 5 batch id 17/23 loss 3.3903794288635254 train acc 0.03125\n",
      "epoch 5 batch id 18/23 loss 3.493778944015503 train acc 0.0\n",
      "epoch 5 batch id 19/23 loss 3.465200424194336 train acc 0.015625\n",
      "epoch 5 batch id 20/23 loss 3.4250473976135254 train acc 0.0\n",
      "epoch 5 batch id 21/23 loss 3.5209808349609375 train acc 0.0\n",
      "epoch 5 batch id 22/23 loss 3.337308168411255 train acc 0.0\n",
      "epoch 5 batch id 23/23 loss 3.3085556030273438 train acc 0.0\n",
      "epoch 5 train acc 0.007472826086956522 loss mean 3.4369863530863887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:35<00:00,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.004261363636363636\n",
      "epoch 6 batch id 1/23 loss 3.4605236053466797 train acc 0.03125\n",
      "epoch 6 batch id 2/23 loss 3.5289387702941895 train acc 0.0\n",
      "epoch 6 batch id 3/23 loss 3.4965386390686035 train acc 0.0\n",
      "epoch 6 batch id 4/23 loss 3.388655662536621 train acc 0.0\n",
      "epoch 6 batch id 5/23 loss 3.477313756942749 train acc 0.015625\n",
      "epoch 6 batch id 6/23 loss 3.5159316062927246 train acc 0.0\n",
      "epoch 6 batch id 7/23 loss 3.51442289352417 train acc 0.015625\n",
      "epoch 6 batch id 8/23 loss 3.370028257369995 train acc 0.015625\n",
      "epoch 6 batch id 9/23 loss 3.486786365509033 train acc 0.0\n",
      "epoch 6 batch id 10/23 loss 3.4422662258148193 train acc 0.015625\n",
      "epoch 6 batch id 11/23 loss 3.450613498687744 train acc 0.0\n",
      "epoch 6 batch id 12/23 loss 3.423661470413208 train acc 0.0\n",
      "epoch 6 batch id 13/23 loss 3.5033674240112305 train acc 0.0\n",
      "epoch 6 batch id 14/23 loss 3.4997315406799316 train acc 0.0\n",
      "epoch 6 batch id 15/23 loss 3.385765552520752 train acc 0.0\n",
      "epoch 6 batch id 16/23 loss 3.4201443195343018 train acc 0.015625\n",
      "epoch 6 batch id 17/23 loss 3.3604748249053955 train acc 0.0\n",
      "epoch 6 batch id 18/23 loss 3.3423008918762207 train acc 0.015625\n",
      "epoch 6 batch id 19/23 loss 3.3477330207824707 train acc 0.03125\n",
      "epoch 6 batch id 20/23 loss 3.372805118560791 train acc 0.03125\n",
      "epoch 6 batch id 21/23 loss 3.4610226154327393 train acc 0.0\n",
      "epoch 6 batch id 22/23 loss 3.4605119228363037 train acc 0.0\n",
      "epoch 6 batch id 23/23 loss 3.372636079788208 train acc 0.0\n",
      "epoch 6 train acc 0.008152173913043478 loss mean 3.4383553940316904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:38<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 test acc 0.004261363636363636\n",
      "epoch 7 batch id 1/23 loss 3.5069327354431152 train acc 0.0\n",
      "epoch 7 batch id 2/23 loss 3.47760272026062 train acc 0.015625\n",
      "epoch 7 batch id 3/23 loss 3.2869694232940674 train acc 0.015625\n",
      "epoch 7 batch id 4/23 loss 3.4166648387908936 train acc 0.0\n",
      "epoch 7 batch id 5/23 loss 3.4913463592529297 train acc 0.0\n",
      "epoch 7 batch id 6/23 loss 3.475839376449585 train acc 0.0\n",
      "epoch 7 batch id 7/23 loss 3.5293960571289062 train acc 0.0\n",
      "epoch 7 batch id 8/23 loss 3.5301096439361572 train acc 0.0\n",
      "epoch 7 batch id 9/23 loss 3.362197160720825 train acc 0.03125\n",
      "epoch 7 batch id 10/23 loss 3.374936103820801 train acc 0.015625\n",
      "epoch 7 batch id 11/23 loss 3.377686023712158 train acc 0.015625\n",
      "epoch 7 batch id 12/23 loss 3.447532892227173 train acc 0.0\n",
      "epoch 7 batch id 13/23 loss 3.460367202758789 train acc 0.015625\n",
      "epoch 7 batch id 14/23 loss 3.3135344982147217 train acc 0.0\n",
      "epoch 7 batch id 15/23 loss 3.4966225624084473 train acc 0.015625\n",
      "epoch 7 batch id 16/23 loss 3.433262348175049 train acc 0.015625\n",
      "epoch 7 batch id 17/23 loss 3.3382654190063477 train acc 0.0\n",
      "epoch 7 batch id 18/23 loss 3.4539079666137695 train acc 0.015625\n",
      "epoch 7 batch id 19/23 loss 3.3857572078704834 train acc 0.0\n",
      "epoch 7 batch id 20/23 loss 3.442124128341675 train acc 0.0\n",
      "epoch 7 batch id 21/23 loss 3.4838078022003174 train acc 0.0\n",
      "epoch 7 batch id 22/23 loss 3.4787282943725586 train acc 0.0\n",
      "epoch 7 batch id 23/23 loss 3.461505651473999 train acc 0.0\n",
      "epoch 7 train acc 0.006793478260869565 loss mean 3.4358737572379736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:32<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 test acc 0.004261363636363636\n",
      "epoch 8 batch id 1/23 loss 3.394768714904785 train acc 0.015625\n",
      "epoch 8 batch id 2/23 loss 3.5246450901031494 train acc 0.0\n",
      "epoch 8 batch id 3/23 loss 3.4434659481048584 train acc 0.015625\n",
      "epoch 8 batch id 4/23 loss 3.459472179412842 train acc 0.0\n",
      "epoch 8 batch id 5/23 loss 3.3868846893310547 train acc 0.015625\n",
      "epoch 8 batch id 6/23 loss 3.527589797973633 train acc 0.0\n",
      "epoch 8 batch id 7/23 loss 3.512089252471924 train acc 0.0\n",
      "epoch 8 batch id 8/23 loss 3.3622944355010986 train acc 0.0\n",
      "epoch 8 batch id 9/23 loss 3.502382278442383 train acc 0.0\n",
      "epoch 8 batch id 10/23 loss 3.4843075275421143 train acc 0.0\n",
      "epoch 8 batch id 11/23 loss 3.400731086730957 train acc 0.015625\n",
      "epoch 8 batch id 12/23 loss 3.3820183277130127 train acc 0.015625\n",
      "epoch 8 batch id 13/23 loss 3.269564628601074 train acc 0.03125\n",
      "epoch 8 batch id 14/23 loss 3.3954689502716064 train acc 0.0\n",
      "epoch 8 batch id 15/23 loss 3.3508260250091553 train acc 0.0\n",
      "epoch 8 batch id 16/23 loss 3.4976460933685303 train acc 0.0\n",
      "epoch 8 batch id 17/23 loss 3.382402181625366 train acc 0.03125\n",
      "epoch 8 batch id 18/23 loss 3.3548779487609863 train acc 0.015625\n",
      "epoch 8 batch id 19/23 loss 3.48650860786438 train acc 0.0\n",
      "epoch 8 batch id 20/23 loss 3.4651753902435303 train acc 0.0\n",
      "epoch 8 batch id 21/23 loss 3.542015314102173 train acc 0.0\n",
      "epoch 8 batch id 22/23 loss 3.4833388328552246 train acc 0.015625\n",
      "epoch 8 batch id 23/23 loss 3.418524742126465 train acc 0.0\n",
      "epoch 8 train acc 0.007472826086956522 loss mean 3.435956436654796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:33<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 test acc 0.004261363636363636\n",
      "epoch 9 batch id 1/23 loss 3.4672365188598633 train acc 0.0\n",
      "epoch 9 batch id 2/23 loss 3.464799642562866 train acc 0.0\n",
      "epoch 9 batch id 3/23 loss 3.554474353790283 train acc 0.0\n",
      "epoch 9 batch id 4/23 loss 3.4775314331054688 train acc 0.0\n",
      "epoch 9 batch id 5/23 loss 3.466677188873291 train acc 0.0\n",
      "epoch 9 batch id 6/23 loss 3.2717065811157227 train acc 0.015625\n",
      "epoch 9 batch id 7/23 loss 3.452512264251709 train acc 0.015625\n",
      "epoch 9 batch id 8/23 loss 3.403635263442993 train acc 0.03125\n",
      "epoch 9 batch id 9/23 loss 3.5392072200775146 train acc 0.0\n",
      "epoch 9 batch id 10/23 loss 3.4982492923736572 train acc 0.0\n",
      "epoch 9 batch id 11/23 loss 3.414479970932007 train acc 0.03125\n",
      "epoch 9 batch id 12/23 loss 3.537642002105713 train acc 0.0\n",
      "epoch 9 batch id 13/23 loss 3.497823715209961 train acc 0.015625\n",
      "epoch 9 batch id 14/23 loss 3.4723386764526367 train acc 0.0\n",
      "epoch 9 batch id 15/23 loss 3.3272593021392822 train acc 0.015625\n",
      "epoch 9 batch id 16/23 loss 3.369804859161377 train acc 0.0\n",
      "epoch 9 batch id 17/23 loss 3.4390289783477783 train acc 0.0\n",
      "epoch 9 batch id 18/23 loss 3.426905870437622 train acc 0.0\n",
      "epoch 9 batch id 19/23 loss 3.4074387550354004 train acc 0.0\n",
      "epoch 9 batch id 20/23 loss 3.3585338592529297 train acc 0.0\n",
      "epoch 9 batch id 21/23 loss 3.407042980194092 train acc 0.03125\n",
      "epoch 9 batch id 22/23 loss 3.4863882064819336 train acc 0.015625\n",
      "epoch 9 batch id 23/23 loss 3.2907979488372803 train acc 0.015625\n",
      "epoch 9 train acc 0.008152173913043478 loss mean 3.4361528210017993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:33<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 test acc 0.004261363636363636\n",
      "epoch 10 batch id 1/23 loss 3.3917696475982666 train acc 0.015625\n",
      "epoch 10 batch id 2/23 loss 3.437828302383423 train acc 0.015625\n",
      "epoch 10 batch id 3/23 loss 3.477713108062744 train acc 0.0\n",
      "epoch 10 batch id 4/23 loss 3.527576446533203 train acc 0.0\n",
      "epoch 10 batch id 5/23 loss 3.505939245223999 train acc 0.0\n",
      "epoch 10 batch id 6/23 loss 3.4437339305877686 train acc 0.03125\n",
      "epoch 10 batch id 7/23 loss 3.560321569442749 train acc 0.0\n",
      "epoch 10 batch id 8/23 loss 3.374302625656128 train acc 0.0\n",
      "epoch 10 batch id 9/23 loss 3.4538912773132324 train acc 0.0\n",
      "epoch 10 batch id 10/23 loss 3.367922306060791 train acc 0.015625\n",
      "epoch 10 batch id 11/23 loss 3.3908307552337646 train acc 0.0\n",
      "epoch 10 batch id 12/23 loss 3.3325626850128174 train acc 0.03125\n",
      "epoch 10 batch id 13/23 loss 3.493860960006714 train acc 0.0\n",
      "epoch 10 batch id 14/23 loss 3.408151388168335 train acc 0.0\n",
      "epoch 10 batch id 15/23 loss 3.419408082962036 train acc 0.0\n",
      "epoch 10 batch id 16/23 loss 3.3796839714050293 train acc 0.015625\n",
      "epoch 10 batch id 17/23 loss 3.424441337585449 train acc 0.0\n",
      "epoch 10 batch id 18/23 loss 3.373600721359253 train acc 0.015625\n",
      "epoch 10 batch id 19/23 loss 3.4252541065216064 train acc 0.015625\n",
      "epoch 10 batch id 20/23 loss 3.466064214706421 train acc 0.0\n",
      "epoch 10 batch id 21/23 loss 3.50072979927063 train acc 0.0\n",
      "epoch 10 batch id 22/23 loss 3.466064929962158 train acc 0.015625\n",
      "epoch 10 batch id 23/23 loss 3.4088692665100098 train acc 0.015625\n",
      "epoch 10 train acc 0.008152173913043478 loss mean 3.4361095946768057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:32<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 test acc 0.004261363636363636\n",
      "epoch 11 batch id 1/23 loss 3.4039785861968994 train acc 0.0\n",
      "epoch 11 batch id 2/23 loss 3.389486789703369 train acc 0.015625\n",
      "epoch 11 batch id 3/23 loss 3.414552927017212 train acc 0.015625\n",
      "epoch 11 batch id 4/23 loss 3.3949358463287354 train acc 0.0\n",
      "epoch 11 batch id 5/23 loss 3.3499996662139893 train acc 0.015625\n",
      "epoch 11 batch id 6/23 loss 3.3319458961486816 train acc 0.015625\n",
      "epoch 11 batch id 7/23 loss 3.436568260192871 train acc 0.0\n",
      "epoch 11 batch id 8/23 loss 3.361441135406494 train acc 0.0\n",
      "epoch 11 batch id 9/23 loss 3.4828271865844727 train acc 0.015625\n",
      "epoch 11 batch id 10/23 loss 3.469836711883545 train acc 0.0\n",
      "epoch 11 batch id 11/23 loss 3.3270647525787354 train acc 0.015625\n",
      "epoch 11 batch id 12/23 loss 3.4850196838378906 train acc 0.0\n",
      "epoch 11 batch id 13/23 loss 3.451691150665283 train acc 0.015625\n",
      "epoch 11 batch id 14/23 loss 3.4622883796691895 train acc 0.0\n",
      "epoch 11 batch id 15/23 loss 3.5049760341644287 train acc 0.0\n",
      "epoch 11 batch id 16/23 loss 3.4836606979370117 train acc 0.0\n",
      "epoch 11 batch id 17/23 loss 3.4268195629119873 train acc 0.03125\n",
      "epoch 11 batch id 18/23 loss 3.3256494998931885 train acc 0.0\n",
      "epoch 11 batch id 19/23 loss 3.5215344429016113 train acc 0.0\n",
      "epoch 11 batch id 20/23 loss 3.357409715652466 train acc 0.03125\n",
      "epoch 11 batch id 21/23 loss 3.492720127105713 train acc 0.0\n",
      "epoch 11 batch id 22/23 loss 3.591972827911377 train acc 0.0\n",
      "epoch 11 batch id 23/23 loss 3.5403215885162354 train acc 0.0\n",
      "epoch 11 train acc 0.007472826086956522 loss mean 3.4350739769313647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:33<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 test acc 0.004261363636363636\n",
      "epoch 12 batch id 1/23 loss 3.5552868843078613 train acc 0.0\n",
      "epoch 12 batch id 2/23 loss 3.3624908924102783 train acc 0.015625\n",
      "epoch 12 batch id 3/23 loss 3.2545957565307617 train acc 0.03125\n",
      "epoch 12 batch id 4/23 loss 3.4618515968322754 train acc 0.015625\n",
      "epoch 12 batch id 5/23 loss 3.377579689025879 train acc 0.0\n",
      "epoch 12 batch id 6/23 loss 3.459718942642212 train acc 0.0\n",
      "epoch 12 batch id 7/23 loss 3.5084497928619385 train acc 0.0\n",
      "epoch 12 batch id 8/23 loss 3.4307398796081543 train acc 0.015625\n",
      "epoch 12 batch id 9/23 loss 3.278613805770874 train acc 0.015625\n",
      "epoch 12 batch id 10/23 loss 3.365391969680786 train acc 0.0\n",
      "epoch 12 batch id 11/23 loss 3.4884591102600098 train acc 0.0\n",
      "epoch 12 batch id 12/23 loss 3.3507776260375977 train acc 0.046875\n",
      "epoch 12 batch id 13/23 loss 3.4205446243286133 train acc 0.0\n",
      "epoch 12 batch id 14/23 loss 3.410128355026245 train acc 0.0\n",
      "epoch 12 batch id 15/23 loss 3.4505090713500977 train acc 0.015625\n",
      "epoch 12 batch id 16/23 loss 3.4848239421844482 train acc 0.0\n",
      "epoch 12 batch id 17/23 loss 3.5207815170288086 train acc 0.0\n",
      "epoch 12 batch id 18/23 loss 3.4689853191375732 train acc 0.0\n",
      "epoch 12 batch id 19/23 loss 3.414313316345215 train acc 0.015625\n",
      "epoch 12 batch id 20/23 loss 3.453273057937622 train acc 0.0\n",
      "epoch 12 batch id 21/23 loss 3.3747520446777344 train acc 0.015625\n",
      "epoch 12 batch id 22/23 loss 3.352370500564575 train acc 0.0\n",
      "epoch 12 batch id 23/23 loss 3.5421833992004395 train acc 0.0\n",
      "epoch 12 train acc 0.008152173913043478 loss mean 3.4255052649456523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:33<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 test acc 0.004261363636363636\n",
      "epoch 13 batch id 1/23 loss 3.488541841506958 train acc 0.0\n",
      "epoch 13 batch id 2/23 loss 3.404971122741699 train acc 0.015625\n",
      "epoch 13 batch id 3/23 loss 3.392439842224121 train acc 0.015625\n",
      "epoch 13 batch id 4/23 loss 3.3900821208953857 train acc 0.015625\n",
      "epoch 13 batch id 5/23 loss 3.3301470279693604 train acc 0.03125\n",
      "epoch 13 batch id 6/23 loss 3.4485836029052734 train acc 0.015625\n",
      "epoch 13 batch id 7/23 loss 3.4020133018493652 train acc 0.0\n",
      "epoch 13 batch id 8/23 loss 3.4505491256713867 train acc 0.0\n",
      "epoch 13 batch id 9/23 loss 3.504425525665283 train acc 0.015625\n",
      "epoch 13 batch id 10/23 loss 3.3816325664520264 train acc 0.0\n",
      "epoch 13 batch id 11/23 loss 3.4806630611419678 train acc 0.0\n",
      "epoch 13 batch id 12/23 loss 3.4741270542144775 train acc 0.0\n",
      "epoch 13 batch id 13/23 loss 3.52160906791687 train acc 0.0\n",
      "epoch 13 batch id 14/23 loss 3.364490270614624 train acc 0.0\n",
      "epoch 13 batch id 15/23 loss 3.417506217956543 train acc 0.015625\n",
      "epoch 13 batch id 16/23 loss 3.4547083377838135 train acc 0.0\n",
      "epoch 13 batch id 17/23 loss 3.492790460586548 train acc 0.0\n",
      "epoch 13 batch id 18/23 loss 3.449110507965088 train acc 0.015625\n",
      "epoch 13 batch id 19/23 loss 3.2383484840393066 train acc 0.015625\n",
      "epoch 13 batch id 20/23 loss 3.4647891521453857 train acc 0.015625\n",
      "epoch 13 batch id 21/23 loss 3.508633852005005 train acc 0.0\n",
      "epoch 13 batch id 22/23 loss 3.450801134109497 train acc 0.0\n",
      "epoch 13 batch id 23/23 loss 3.506913423538208 train acc 0.0\n",
      "epoch 13 train acc 0.007472826086956522 loss mean 3.4355598739955737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:36<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 test acc 0.004261363636363636\n",
      "epoch 14 batch id 1/23 loss 3.522768020629883 train acc 0.0\n",
      "epoch 14 batch id 2/23 loss 3.576038360595703 train acc 0.0\n",
      "epoch 14 batch id 3/23 loss 3.4250335693359375 train acc 0.0\n",
      "epoch 14 batch id 4/23 loss 3.4230642318725586 train acc 0.015625\n",
      "epoch 14 batch id 5/23 loss 3.382864236831665 train acc 0.0\n",
      "epoch 14 batch id 6/23 loss 3.385202407836914 train acc 0.03125\n",
      "epoch 14 batch id 7/23 loss 3.527888298034668 train acc 0.0\n",
      "epoch 14 batch id 8/23 loss 3.3856496810913086 train acc 0.03125\n",
      "epoch 14 batch id 9/23 loss 3.3890833854675293 train acc 0.0\n",
      "epoch 14 batch id 10/23 loss 3.5570530891418457 train acc 0.0\n",
      "epoch 14 batch id 11/23 loss 3.3731589317321777 train acc 0.0\n",
      "epoch 14 batch id 12/23 loss 3.4137656688690186 train acc 0.015625\n",
      "epoch 14 batch id 13/23 loss 3.3981640338897705 train acc 0.015625\n",
      "epoch 14 batch id 14/23 loss 3.427125930786133 train acc 0.0\n",
      "epoch 14 batch id 15/23 loss 3.441467523574829 train acc 0.0\n",
      "epoch 14 batch id 16/23 loss 3.444798707962036 train acc 0.0\n",
      "epoch 14 batch id 17/23 loss 3.4096109867095947 train acc 0.015625\n",
      "epoch 14 batch id 18/23 loss 3.408681631088257 train acc 0.015625\n",
      "epoch 14 batch id 19/23 loss 3.5215954780578613 train acc 0.0\n",
      "epoch 14 batch id 20/23 loss 3.300938129425049 train acc 0.015625\n",
      "epoch 14 batch id 21/23 loss 3.584836959838867 train acc 0.0\n",
      "epoch 14 batch id 22/23 loss 3.362531900405884 train acc 0.0\n",
      "epoch 14 batch id 23/23 loss 3.3606672286987305 train acc 0.015625\n",
      "epoch 14 train acc 0.007472826086956522 loss mean 3.4357386257337486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:34<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 test acc 0.004261363636363636\n",
      "epoch 15 batch id 1/23 loss 3.4741806983947754 train acc 0.0\n",
      "epoch 15 batch id 2/23 loss 3.382072925567627 train acc 0.0\n",
      "epoch 15 batch id 3/23 loss 3.488633871078491 train acc 0.0\n",
      "epoch 15 batch id 4/23 loss 3.4834721088409424 train acc 0.0\n",
      "epoch 15 batch id 5/23 loss 3.4054224491119385 train acc 0.015625\n",
      "epoch 15 batch id 6/23 loss 3.3311514854431152 train acc 0.0\n",
      "epoch 15 batch id 7/23 loss 3.2816548347473145 train acc 0.015625\n",
      "epoch 15 batch id 8/23 loss 3.463719606399536 train acc 0.0\n",
      "epoch 15 batch id 9/23 loss 3.464935779571533 train acc 0.0\n",
      "epoch 15 batch id 10/23 loss 3.407379388809204 train acc 0.0\n",
      "epoch 15 batch id 11/23 loss 3.405430316925049 train acc 0.015625\n",
      "epoch 15 batch id 12/23 loss 3.4853873252868652 train acc 0.0\n",
      "epoch 15 batch id 13/23 loss 3.2772274017333984 train acc 0.03125\n",
      "epoch 15 batch id 14/23 loss 3.4725019931793213 train acc 0.015625\n",
      "epoch 15 batch id 15/23 loss 3.4384348392486572 train acc 0.0\n",
      "epoch 15 batch id 16/23 loss 3.3004143238067627 train acc 0.015625\n",
      "epoch 15 batch id 17/23 loss 3.407658576965332 train acc 0.0\n",
      "epoch 15 batch id 18/23 loss 3.5507428646087646 train acc 0.0\n",
      "epoch 15 batch id 19/23 loss 3.3193604946136475 train acc 0.046875\n",
      "epoch 15 batch id 20/23 loss 3.4407482147216797 train acc 0.015625\n",
      "epoch 15 batch id 21/23 loss 3.4834542274475098 train acc 0.0\n",
      "epoch 15 batch id 22/23 loss 3.45641827583313 train acc 0.0\n",
      "epoch 15 batch id 23/23 loss 3.4803466796875 train acc 0.0\n",
      "epoch 15 train acc 0.007472826086956522 loss mean 3.4217716818270474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:34<00:00,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 test acc 0.004261363636363636\n",
      "epoch 16 batch id 1/23 loss 3.389923095703125 train acc 0.03125\n",
      "epoch 16 batch id 2/23 loss 3.463848352432251 train acc 0.0\n",
      "epoch 16 batch id 3/23 loss 3.550621509552002 train acc 0.0\n",
      "epoch 16 batch id 4/23 loss 3.358124256134033 train acc 0.0\n",
      "epoch 16 batch id 5/23 loss 3.4676437377929688 train acc 0.015625\n",
      "epoch 16 batch id 6/23 loss 3.436434030532837 train acc 0.0\n",
      "epoch 16 batch id 7/23 loss 3.4344887733459473 train acc 0.0\n",
      "epoch 16 batch id 8/23 loss 3.5297868251800537 train acc 0.0\n",
      "epoch 16 batch id 9/23 loss 3.4298031330108643 train acc 0.0\n",
      "epoch 16 batch id 10/23 loss 3.405683994293213 train acc 0.015625\n",
      "epoch 16 batch id 11/23 loss 3.484005928039551 train acc 0.0\n",
      "epoch 16 batch id 12/23 loss 3.369858503341675 train acc 0.015625\n",
      "epoch 16 batch id 13/23 loss 3.329671621322632 train acc 0.046875\n",
      "epoch 16 batch id 14/23 loss 3.4010705947875977 train acc 0.0\n",
      "epoch 16 batch id 15/23 loss 3.4960975646972656 train acc 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36476/1746033033.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36476/2675219235.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, attention_mask)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#gradient 계산을 중지\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;31m#         x = self.dropout(pooler)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1006\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1007\u001b[0m         )\n\u001b[0;32m   1008\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    590\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m                 )\n\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         )\n\u001b[0;32m    479\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m         )\n\u001b[0;32m    411\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0mkey_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[0mvalue_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    loss_sum = 0\n",
    "    model.train()\n",
    "    for batch_id, (input_ids, token_type_ids, attention_mask, tensor_label) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(input_ids, token_type_ids, attention_mask)\n",
    "        loss = loss_fn(out, tensor_label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "#         scheduler.step()  # Update learning rate schedule\n",
    "        batch_acc = calc_accuracy(out, tensor_label)\n",
    "        train_acc += batch_acc\n",
    "        loss_sum += loss.data.cpu().numpy()\n",
    "        #f batch_id % log_interval == 0:\n",
    "        print(\"epoch {} batch id {}/{} loss {} train acc {}\".format(e+1, batch_id+1, len(train_dataloader), loss.data.cpu().numpy(), batch_acc))\n",
    "    print(\"epoch {} train acc {} loss mean {}\".format(e+1, train_acc / (batch_id+1), loss_sum / len(train_dataloader)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, (input_ids, token_type_ids, attention_mask, tensor_label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "            out = model(input_ids, token_type_ids, attention_mask)\n",
    "            test_acc += calc_accuracy(out, tensor_label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
