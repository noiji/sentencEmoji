{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets  # for vscode\n",
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#KoBERT\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "# model = AutoModel.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# KoELECTRA-Base\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\")\n",
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-base-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentencEmojiDataset(Dataset):\n",
    "    def __init__(self, directory, tokenizer):\n",
    "        \n",
    "        data = pd.read_csv(directory, encoding='UTF-8')\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = list(data.iloc[:,0])\n",
    "       \n",
    "        emojis = list(data.iloc[:,1])\n",
    "        emojis_unique = list(set(emojis))\n",
    "        \n",
    "        self.labels = [emojis_unique.index(i) for i in emojis]\n",
    " \n",
    "        self.labels_dict = {'key': range(len(emojis_unique)), 'value': emojis_unique}\n",
    "        \n",
    "    def __getitem__(self, i): #collate ì´ì „ ë¯¸ë¦¬ tokenizeë¥¼ ì‹œì¼œì£¼ì\n",
    "        tokenized = self.tokenizer(str(self.sentences[i]), return_tensors='pt')\n",
    "        \n",
    "        #ì•„ë˜ ì„¸ ê°œëŠ” tokenizerê°€ ê¸°ë³¸ì ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ì •ë³´. BERTì˜ inputì´ê¸°ë„ í•¨\n",
    "        input_ids = tokenized['input_ids']\n",
    "        token_type_ids = tokenized['token_type_ids']\n",
    "        attention_mask = tokenized['attention_mask']\n",
    "        \n",
    "#         print(str(self.sentences[i]) +' : ')\n",
    "#         print(tokenized)\n",
    "        \n",
    "        return {'input_ids': input_ids, 'token_type_ids': token_type_ids, \n",
    "                'attention_mask': attention_mask, 'label': self.labels[i]}\n",
    "         \n",
    "    def __len__(self): #data loaderê°€ í•„ìš”ë¡œ í•˜ì—¬ í•„ìˆ˜ì ìœ¼ë¡œ ìˆì–´ì•¼ í•˜ëŠ” í•¨ìˆ˜\n",
    "        return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class collate_fn:\n",
    "    def __init__(self, labels_dict):\n",
    "        self.num_labels = len(labels_dict)\n",
    "        \n",
    "    def __call__(self, batch): #batchëŠ” dataset.getitemì˜ return ê°’ì˜ List. eg. [{}, {}. ...]\n",
    "        #batchë‚´ ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´(í† í° ê°œìˆ˜)ë¥¼ ë¨¼ì € êµ¬í•´ì„œ paddingí•  ìˆ˜ ìˆë„ë¡ í•˜ê¸°\n",
    "        batchlen = [sample['input_ids'].size(1) for sample in batch] #tensorê°’ì„ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì— 1ë²ˆì§¸ ì°¨ì›ì˜ ê¸¸ì´ë¥¼ êµ¬í•¨\n",
    "        maxlen = max(batchlen)\n",
    "        input_ids = []\n",
    "        token_type_ids = []\n",
    "        attention_mask = []\n",
    "        #padding: [5, 6] [0, 0,  ...]ì„ concatenate í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ íŒ¨ë”©\n",
    "        for sample in batch:\n",
    "            pad_len = maxlen - sample['input_ids'].size(1)\n",
    "            pad = torch.zeros((1, pad_len), dtype=torch.int)            \n",
    "            input_ids.append(torch.cat([sample['input_ids'], pad], dim=1))\n",
    "            token_type_ids.append(torch.cat([sample['token_type_ids'], pad], dim=1))\n",
    "            attention_mask.append(torch.cat([sample['attention_mask'], pad], dim=1))\n",
    "        #batch êµ¬ì„±\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "        attention_mask = torch.cat(attention_mask, dim=0)\n",
    "        \n",
    "        #one-hot encoding\n",
    "        #batch ë‚´ ë¼ë²¨ì„ tensorë¡œ ë³€í™˜\n",
    "        tensor_label = torch.tensor([sample['label'] for sample in batch])\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask, tensor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "N    235\n",
       "ğŸ˜‚     80\n",
       "â¤     53\n",
       "ğŸ˜­     49\n",
       "ğŸ˜     32\n",
       "ğŸ’•     28\n",
       "ğŸ’œ     26\n",
       "ğŸ”¥     25\n",
       "ğŸ‘     24\n",
       "ğŸ¥º     24\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/twitter_clean.csv', encoding=\"UTF-8\")\n",
    "print(len(df['y'].value_counts()))\n",
    "df['y'].value_counts(sort = True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'] = np.random.randn(df.shape[0], 1)\n",
    "msk = np.random.rand(len(df)) <= 0.7\n",
    "\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "\n",
    "train.to_csv('data/train.csv', index=False)\n",
    "test.to_csv('data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.collate_fn at 0x1b5dd34ec50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = sentencEmojiDataset('data/train.csv', tokenizer)\n",
    "test = sentencEmojiDataset('data/test.csv', tokenizer)\n",
    "\n",
    "train_collate_fn = collate_fn(train.labels_dict)\n",
    "test_collate_fn = collate_fn(test.labels_dict)\n",
    "\n",
    "train_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 20  \n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1b5e474bef0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=batch_size, collate_fn=train_collate_fn, shuffle = True, drop_last = True)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size, collate_fn=test_collate_fn, shuffle = False, drop_last = False)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERTClassifier(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  bert,\n",
    "#                  hidden_size = 768,\n",
    "#                  num_classes=2,\n",
    "#                  dr_rate=None,\n",
    "#                  params=None):\n",
    "#         super(BERTClassifier, self).__init__()\n",
    "#         self.bert = bert\n",
    "#         # do not train bert parameters\n",
    "#         for p in self.bert.parameters():\n",
    "#             p.requires_grad = False\n",
    "#         self.dr_rate = dr_rate\n",
    "                 \n",
    "#         self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        \n",
    "#         if dr_rate:\n",
    "#             self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "#     def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "#         #eval: drop out ì¤‘ì§€, batch norm ê³ ì •ê³¼ ê°™ì´ evaluationìœ¼ë¡œ ëª¨ë¸ ë³€ê²½\n",
    "#         self.bert.eval()\n",
    "#         #gradient ê³„ì‚°ì„ ì¤‘ì§€\n",
    "#         with torch.no_grad():\n",
    "#             x = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
    "# #         x = self.dropout(pooler)\n",
    "#         return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELECTRAClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 electra,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        \n",
    "        super(ELECTRAClassifier, self).__init__()\n",
    "        \n",
    "        self.electra = electra\n",
    "        \n",
    "        # do not train electra parameters\n",
    "        for p in self.electra.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        \n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        #eval: drop out ì¤‘ì§€, batch norm ê³ ì •ê³¼ ê°™ì´ evaluationìœ¼ë¡œ ëª¨ë¸ ë³€ê²½\n",
    "        self.electra.eval()\n",
    "        \n",
    "        #gradient ê³„ì‚°ì„ ì¤‘ì§€\n",
    "        with torch.no_grad():\n",
    "            #ElectraModelì€ pooled_outputì„ ë¦¬í„´í•˜ì§€ ì•ŠëŠ” ê²ƒì„ ì œì™¸í•˜ê³  BertModelê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤.\n",
    "#            x = self.electra(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
    "\n",
    "            x = self.electra(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :] \n",
    "            #.last_hidden_state[:, 0, :]: [batch , CLS ìœ„ì¹˜, depth]\n",
    "            \n",
    "            #Sentence Embeddingìœ¼ë¡œ ë¬´ì—‡ì„ ë„£ì„ê¹Œ? CLS, average, ... (Bertì˜ ê²½ìš°ì—ëŠ” Sentence BERTë¼ëŠ” ê²Œ ì œì•ˆë˜ì—ˆë‹¤ê³  í•¨)\n",
    "                   \n",
    "        x = self.dropout(x)\n",
    "      \n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(set(list(df.iloc[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BERTClassifier(model,  dr_rate=0.5, num_classes = len(label))\n",
    "model = ELECTRAClassifier(model,  dr_rate=0.5, num_classes = len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizerì™€ schedule ì„¤ì •\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 21, 5, 13, 10, 17, 5, 28, 5, 5, 5, 17, 27, 28, 5, 5, 5, 22, 28, 5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.7417, 2.4370, 1.8278, 1.5667, 1.6872, 0.1313, 1.3708, 1.6872, 1.8278,\n",
      "        1.3708, 1.5667, 1.4622, 1.0444, 1.1544, 2.1933, 1.2902, 1.2902, 0.4387,\n",
      "        1.6872, 1.6872, 1.6872, 0.9536, 1.2902, 1.6872, 1.2185, 1.8278, 1.6872,\n",
      "        1.3708, 0.6267, 0.7563])\n"
     ]
    }
   ],
   "source": [
    "#Class Imbalance ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ weighted cross entropy \n",
    "class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(train.labels), y = train.labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(class_weights) #([1.0000, 1.0000, 4.0000, 1.0000, 0.5714])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': range(0, 30),\n",
       " 'value': ['ğŸ’—',\n",
       "  'ğŸ˜‘',\n",
       "  'ğŸ¤¤',\n",
       "  'ğŸ’™',\n",
       "  'ğŸ˜˜',\n",
       "  'N',\n",
       "  'ğŸ˜Š',\n",
       "  'ğŸ¹',\n",
       "  'ğŸ˜',\n",
       "  'ğŸ’š',\n",
       "  'ğŸ¥°',\n",
       "  'ğŸ˜†',\n",
       "  'ğŸ’•',\n",
       "  'ğŸ¥º',\n",
       "  'ğŸ‘‰',\n",
       "  'â£',\n",
       "  'âœ¨',\n",
       "  'ğŸ˜‚',\n",
       "  'ğŸ’–',\n",
       "  'âœŒ',\n",
       "  'ğŸ’¯',\n",
       "  'ğŸ˜',\n",
       "  'ğŸ‘',\n",
       "  'ğŸ¤­',\n",
       "  'ğŸ’œ',\n",
       "  'ğŸ˜',\n",
       "  'ğŸ™',\n",
       "  'ğŸ”¥',\n",
       "  'â¤',\n",
       "  'ğŸ˜­']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight = class_weights, reduction = 'mean') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1/10 loss 3.572819232940674 train acc 0.03125\n",
      "epoch 1 batch id 2/10 loss 3.4056732654571533 train acc 0.03125\n",
      "epoch 1 batch id 3/10 loss 3.5389044284820557 train acc 0.0\n",
      "epoch 1 batch id 4/10 loss 3.518869400024414 train acc 0.015625\n",
      "epoch 1 batch id 5/10 loss 3.4964404106140137 train acc 0.015625\n",
      "epoch 1 batch id 6/10 loss 3.394656181335449 train acc 0.046875\n",
      "epoch 1 batch id 7/10 loss 3.6041438579559326 train acc 0.0\n",
      "epoch 1 batch id 8/10 loss 3.5636298656463623 train acc 0.046875\n",
      "epoch 1 batch id 9/10 loss 3.5690269470214844 train acc 0.015625\n",
      "epoch 1 batch id 10/10 loss 3.528172254562378 train acc 0.03125\n",
      "epoch 1 train acc 0.0234375 loss mean 3.5192335844039917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:36<00:00,  7.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.02267156862745098\n",
      "epoch 2 batch id 1/10 loss 3.5905935764312744 train acc 0.0\n",
      "epoch 2 batch id 2/10 loss 3.590209722518921 train acc 0.03125\n",
      "epoch 2 batch id 3/10 loss 3.442885160446167 train acc 0.03125\n",
      "epoch 2 batch id 4/10 loss 3.4319005012512207 train acc 0.03125\n",
      "epoch 2 batch id 5/10 loss 3.698812484741211 train acc 0.015625\n",
      "epoch 2 batch id 6/10 loss 3.493004083633423 train acc 0.03125\n",
      "epoch 2 batch id 7/10 loss 3.682891845703125 train acc 0.0\n",
      "epoch 2 batch id 8/10 loss 3.550597906112671 train acc 0.03125\n",
      "epoch 2 batch id 9/10 loss 3.4911088943481445 train acc 0.046875\n",
      "epoch 2 batch id 10/10 loss 3.493788719177246 train acc 0.0\n",
      "epoch 2 train acc 0.021875 loss mean 3.54657928943634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:35<00:00,  7.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.02267156862745098\n",
      "epoch 3 batch id 1/10 loss 3.528674602508545 train acc 0.03125\n",
      "epoch 3 batch id 2/10 loss 3.4830775260925293 train acc 0.015625\n",
      "epoch 3 batch id 3/10 loss 3.5366852283477783 train acc 0.015625\n",
      "epoch 3 batch id 4/10 loss 3.523273229598999 train acc 0.03125\n",
      "epoch 3 batch id 5/10 loss 3.532040596008301 train acc 0.03125\n",
      "epoch 3 batch id 6/10 loss 3.5267746448516846 train acc 0.0\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    loss_sum = 0\n",
    "    model.train()\n",
    "    for batch_id, (input_ids, token_type_ids, attention_mask, tensor_label) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(input_ids, token_type_ids, attention_mask)\n",
    "        loss = loss_fn(out, tensor_label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "#         scheduler.step()  # Update learning rate schedule\n",
    "        batch_acc = calc_accuracy(out, tensor_label)\n",
    "        train_acc += batch_acc\n",
    "        loss_sum += loss.data.cpu().numpy()\n",
    "        #f batch_id % log_interval == 0:\n",
    "        print(\"epoch {} batch id {}/{} loss {} train acc {}\".format(e+1, batch_id+1, len(train_dataloader), loss.data.cpu().numpy(), batch_acc))\n",
    "    print(\"epoch {} train acc {} loss mean {}\".format(e+1, train_acc / (batch_id+1), loss_sum / len(train_dataloader)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, (input_ids, token_type_ids, attention_mask, tensor_label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "            out = model(input_ids, token_type_ids, attention_mask)\n",
    "            test_acc += calc_accuracy(out, tensor_label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
