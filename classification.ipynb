{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets  # for vscode\n",
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "model = AutoModel.from_pretrained(\"monologg/kobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentencEmojiDataset(Dataset):\n",
    "    def __init__(self, directory, tokenizer):\n",
    "        \n",
    "        data = pd.read_csv(directory, encoding='UTF-8')\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = list(data.iloc[:,0])\n",
    "       \n",
    "        emojis = list(data.iloc[:,1])\n",
    "        emojis_unique = list(set(emojis))\n",
    "        \n",
    "        self.labels = [emojis_unique.index(i) for i in emojis]\n",
    " \n",
    "        self.labels_dict = {'key': range(len(emojis_unique)), 'value': emojis_unique}\n",
    "        \n",
    "    def __getitem__(self, i): #collate 이전 미리 tokenize를 시켜주자\n",
    "        tokenized = self.tokenizer(str(self.sentences[i]), return_tensors='pt')\n",
    "        \n",
    "        #아래 세 개는 tokenizer가 기본적으로 반환하는 정보. BERT의 input이기도 함\n",
    "        input_ids = tokenized['input_ids']\n",
    "        token_type_ids = tokenized['token_type_ids']\n",
    "        attention_mask = tokenized['attention_mask']\n",
    "        \n",
    "        print(str(self.sentences[i]) +' : ')\n",
    "        print(tokenized)\n",
    "        \n",
    "        return {'input_ids': input_ids, 'token_type_ids': token_type_ids, \n",
    "                'attention_mask': attention_mask, 'label': self.labels[i]}\n",
    "         \n",
    "    def __len__(self): #data loader가 필요로 하여 필수적으로 있어야 하는 함수\n",
    "        return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class collate_fn:\n",
    "    def __init__(self, labels_dict):\n",
    "        self.num_labels = len(labels_dict)\n",
    "        \n",
    "    def __call__(self, batch): #batch는 dataset.getitem의 return 값의 List. eg. [{}, {}. ...]\n",
    "        #batch내 최대 문장 길이(토큰 개수)를 먼저 구해서 padding할 수 있도록 하기\n",
    "        batchlen = [sample['input_ids'].size(1) for sample in batch] #tensor값을 반환하기 때문에 1번째 차원의 길이를 구함\n",
    "        maxlen = max(batchlen)\n",
    "        input_ids = []\n",
    "        token_type_ids = []\n",
    "        attention_mask = []\n",
    "        #padding: [5, 6] [0, 0,  ...]을 concatenate 하는 방식으로 패딩\n",
    "        for sample in batch:\n",
    "            pad_len = maxlen - sample['input_ids'].size(1)\n",
    "            pad = torch.zeros((1, pad_len), dtype=torch.int)            \n",
    "            input_ids.append(torch.cat([sample['input_ids'], pad], dim=1))\n",
    "            token_type_ids.append(torch.cat([sample['token_type_ids'], pad], dim=1))\n",
    "            attention_mask.append(torch.cat([sample['attention_mask'], pad], dim=1))\n",
    "        #batch 구성\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "        attention_mask = torch.cat(attention_mask, dim=0)\n",
    "        \n",
    "        #one-hot encoding\n",
    "        #batch 내 라벨을 tensor로 변환\n",
    "        tensor_label = torch.tensor([sample['label'] for sample in batch])\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask, tensor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "N    235\n",
       "😂     80\n",
       "❤     53\n",
       "😭     49\n",
       "😍     32\n",
       "💕     28\n",
       "💜     26\n",
       "🔥     25\n",
       "👍     24\n",
       "🥺     24\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/twitter_clean.csv', encoding=\"UTF-8\")\n",
    "print(len(df['y'].value_counts()))\n",
    "df['y'].value_counts(sort = True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'] = np.random.randn(df.shape[0], 1)\n",
    "msk = np.random.rand(len(df)) <= 0.7\n",
    "\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "\n",
    "train.to_csv('data/train.csv', index=False)\n",
    "test.to_csv('data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.collate_fn at 0x25cd19c15f8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = sentencEmojiDataset('data/train.csv', tokenizer)\n",
    "test = sentencEmojiDataset('data/test.csv', tokenizer)\n",
    "\n",
    "train_collate_fn = collate_fn(train.labels_dict)\n",
    "test_collate_fn = collate_fn(test.labels_dict)\n",
    "\n",
    "train_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 20  \n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x25cd19d2dd8>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=batch_size, collate_fn=train_collate_fn, shuffle = True, drop_last = True)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size, collate_fn=test_collate_fn, shuffle = False, drop_last = False)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        # do not train bert parameters\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        \n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        #eval: drop out 중지, batch norm 고정과 같이 evaluation으로 모델 변경\n",
    "        self.bert.eval()\n",
    "        #gradient 계산을 중지\n",
    "        with torch.no_grad():\n",
    "            x = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
    "#         x = self.dropout(pooler)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(set(list(df.iloc[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(model,  dr_rate=0.5, num_classes = len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer와 schedule 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19,\n",
       " 15,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 15,\n",
       " 26,\n",
       " 18,\n",
       " 15,\n",
       " 19,\n",
       " 21,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 15,\n",
       " 19,\n",
       " 19,\n",
       " 21]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1807, 0.7010, 1.8694, 2.4926, 1.7256, 1.1217, 1.3196, 1.7256, 1.2463,\n",
      "        2.2433, 2.2433, 1.8694, 3.2048, 1.0197, 1.1217, 0.4314, 2.0394, 2.4926,\n",
      "        1.3196, 0.1297, 1.3196, 0.6231, 1.1807, 1.1807, 1.3196, 1.7256, 1.3196,\n",
      "        1.6024, 1.8694, 1.7256])\n"
     ]
    }
   ],
   "source": [
    "#Class Imbalance 문제 해결을 위한 weighted cross entropy \n",
    "class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(train.labels), y = train.labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(class_weights) #([1.0000, 1.0000, 4.0000, 1.0000, 0.5714])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': range(0, 30),\n",
       " 'value': ['🙏',\n",
       "  '😭',\n",
       "  '💯',\n",
       "  '🐹',\n",
       "  '😁',\n",
       "  '💖',\n",
       "  '😆',\n",
       "  '💗',\n",
       "  '👍',\n",
       "  '✨',\n",
       "  '👉',\n",
       "  '😑',\n",
       "  '🤤',\n",
       "  '💕',\n",
       "  '😍',\n",
       "  '😂',\n",
       "  '😎',\n",
       "  '✌',\n",
       "  '🥰',\n",
       "  'N',\n",
       "  '😊',\n",
       "  '❤',\n",
       "  '❣',\n",
       "  '💜',\n",
       "  '😘',\n",
       "  '🤭',\n",
       "  '🥺',\n",
       "  '🔥',\n",
       "  '💙',\n",
       "  '💚']}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight = class_weights, reduction = 'mean') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "월이 너무 기다려져요 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "다음에두 엘다님이 맘찍해둘 수 있는 예쁜곳을 만들어보겠어욥 배세진생일가뵤자고세진이의눈부시고도아름다운제막배세진 생일 카페에 협력한 그림이에요생일축하해 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0, 6629, 7142,    0,    0,    0,    0,    0,    0,\n",
      "            0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "해적 도깨비 깃발 출장십오야열정 해적선 단원들과 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "신고해도 별 소용도 없나봐신발은 다들 어케 보관 하시지 : \n",
      "{'input_ids': tensor([[   2,    0, 6356,    0,    0,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "민서에게 나는 당신의 차선으로 끌려가고 있다고 말하고 싶어요 사랑해요 : \n",
      "{'input_ids': tensor([[   2,    0, 5658,    0,    0,    0,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "벌칙영상 모모랜드   배속 댄스 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "도나는 그렇거든 : \n",
      "{'input_ids': tensor([[2, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "사람 한 명 살려주세요 : \n",
      "{'input_ids': tensor([[   2, 6498, 7828, 6204,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "얍 색조합 잘 못하는데 원작하고 비슷하게 갑니다 : \n",
      "{'input_ids': tensor([[   2,    0,    0, 7174,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "금요일은 리스팅 쉬어가는날토요일 저녁엔 명화 샤냥이 경매가 있으니많은 관심 부탁드립니다 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "진환이가우리행복이야 평생 진환이팬 평생 진환이편오래오래 응원할게 사랑해 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "시린 가을은 청소년관람가 이기 때문에나쁜놈 업보 후회공을 좋아 하시는 모든분들이 가볍게 읽기 좋답니다알라딘 외 타 플랫폼은 투에즈님 너무 감사해요옹 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 6995, 7581,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "일간 도 가능하도록 열스밍은 계속된다 : \n",
      "{'input_ids': tensor([[   2, 7127, 5859,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "엠카운트다운 출연 안내 잠시 후 여섯시 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0, 7968,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "도영이 커버곡이 들어간 도 함 오늘도 고생했어요 트레저메이커 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0, 5859, 7837,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "김호중나태주자서전주중주말호중사랑 김호중 김호중자서전 빛나는 별 김호중 사랑스런 별우주 끝까지 빛나세요 소중한 우리 별 김호중지금까지처럼 당당하고 거침없이 달리는거야가자 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0, 6356,    0,    0,    0,    0,    0,    0,\n",
      "         7007, 6356,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "오늘 내 꿈에서 놀러와주어 : \n",
      "{'input_ids': tensor([[   2, 6966, 5678,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "너무 좋아요 진짜 : \n",
      "{'input_ids': tensor([[   2,    0,    0, 7347,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "멘션은 내일 마저 달게요오다들 좋은꿈 꾸시기를 : \n",
      "{'input_ids': tensor([[   2,    0,    0, 6145,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "그냥 사랑을 전파하는거야 월요일에 갔었어   페이스북에서 오래전에 본 사진이에요 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0,    0,    0, 6383,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "유주 놀이   꿈꾸는라디오 완료 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "이 에피소드에서 내가 가장 좋아하는 노래 중 하나입니다  염소를 데리고 처음으로 몸치장을 했어요 : \n",
      "{'input_ids': tensor([[   2, 7096,    0, 5679,    0,    0,    0, 7295,    0,    0,    0,    0,\n",
      "            0, 7874,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "행복한 하루 마무리 잘 하길 바래요 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0, 7174,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "리트윗 해주시면 랜덤사진 발송해드립니다 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "남은 인생을 걸고 말할게 두번은 없어 넌 나의 마지마악 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0,    0, 5695,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "마침내 주를 만들고 버그 사이언스에 있을 때 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0,    0, 5965,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "임영웅 가온차트뮤직어워즈 : \n",
      "{'input_ids': tensor([[2, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "컴백 축하해 : \n",
      "{'input_ids': tensor([[2, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "드리핀 드리핀과함께하는멘션파티 올려드린 세 사이즈 모두 링크 드릴께욥예쁘게 사용해주세요 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0, 6579,    0,    0, 6140,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "트친과입맛궁합알아보기마라탕이랑 민초는 많이 안먹어봐서 맛을 몰라용 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "추첨 주접티커 보내드려요 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "헛 좋은 정보 감사합니다 : \n",
      "{'input_ids': tensor([[   2, 7893, 7272, 7229,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "라고 말했다 오메가아아앗 : \n",
      "{'input_ids': tensor([[   2, 6004,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "카엘오하화  아오이 쌍둥이 생일 컵홀더 협력진 공개 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0,    0, 5454,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "반크 해냈다한복은 한국 전통의상 콜린스 사전에 등재 : \n",
      "{'input_ids': tensor([[   2,    0,    0, 7829,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "생방송 분 전 이라는 말에 긴장하는스페셜 민지 진희 : \n",
      "{'input_ids': tensor([[   2,    0, 6416, 7207, 7103,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "사랑뷔님 잠금화면 태형이도 넘 이뻐요 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0, 5698,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "담주 위 하면 주 연속 신기록이에요 : \n",
      "{'input_ids': tensor([[   2,    0, 7044, 7811, 7276,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "친구들이 좋은 취지로 시작한 프로젝트에 응원 보내주시면 감사드리겠슴댜 병 리싸이클 된 소재로 제작 무려 웨이로 제작을 했다네여 아직 오픈 전이라 알림신청 하시면 오픈시 알림받고 얼리버드가에 내가 뭐랬어 이길거랬잖아 이 기적아닌 기적을 우리가 만들었ㅏㅏㅏ위탈환 : \n",
      "{'input_ids': tensor([[   2,    0, 7272,    0,    0,    0,    0,    0,    0, 6361,    0, 5899,\n",
      "            0,    0,    0,    0,    0,    0, 6806, 6969,    0,    0,    0,    0,\n",
      "            0,    0, 5679,    0,    0, 7096,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "첫 번째 시도 두 번째 앨범에서 영감을 얻음 : \n",
      "{'input_ids': tensor([[   2, 7430, 6329,    0, 5907, 6329,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "테스트컷   혹시라도 빌보드 블락 하신 분들 계시면 해제 하세요 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0,    0,    0,    0, 7814,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "배세진생일가뵤자고세진이의눈부시고도아름다운제막배세진 생일 카페에 협력한 그림이에요생일축하해 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "유겸 갓세븐  다가오는 재현이의 살 생일 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0, 6519,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "친해지면 선물도 퍼줍니다 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "솔로 주년    같지만 달라요 : \n",
      "{'input_ids': tensor([[   2,    0, 7279,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "리더즈가 행복하면 된거야국가대표 댄서들의 본캐찾기 여행해치지않아스우파 목 밤  새해 첫 프리스타일 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0,    0, 6217, 6303,    0, 7430,    0,\n",
      "            3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "크래비티 앨런 정모 태영 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0, 7599,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "오늘도 도경수 엑소디오 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "의 기운받아 위까지 : \n",
      "{'input_ids': tensor([[   2, 7095,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "애교쟁이 요하니가 오늘 하루의 힘이 되셨기를 바라며 감기 조심하시고 내일 만나요 : \n",
      "{'input_ids': tensor([[   2,    0,    0, 6966,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "유겸 갓세븐   팔로우 이벤트 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "하하 고마워요 토리 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "오늘은 좀 더 편안해졌는지 모르겠네 무리하지말고 천천히 회복하자 지민이가 최고 : \n",
      "{'input_ids': tensor([[   2,    0, 7266, 5837,    0,    0,    0,    0,    0,    0, 7459,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "포카라도 나눔해야할까 : \n",
      "{'input_ids': tensor([[2, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "여전히 가장 현실적인 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "진진 라키 몬스타엑스 기현 의 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0, 7095,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "특전은 나눔존에 두었으니 편하게 챙겨가세요 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "넵넵 저 늦게 해시 투표 동참한거 사실 : \n",
      "{'input_ids': tensor([[   2,    0, 7199,    0,    0, 7647,    0, 6502,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "놀라운 목표  이 언급되었을 때 그의 미소는 말 그대로 큰 소리로 : \n",
      "{'input_ids': tensor([[   2,    0, 6218, 7096,    0, 5965,    0,    0, 6160,    0, 7567,    0,\n",
      "            3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "어느 쪽이든 너무 깊게 생각 하는듯 : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "주학년 라포티셀앳스타일 : \n",
      "{'input_ids': tensor([[2, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "김남준 손 넘 귀여운거있죠 : \n",
      "{'input_ids': tensor([[   2,    0, 6616, 5698,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "어르신 분들이 입은 보라색 외투도 예 는 정말 성실하고 착한 거 같아요 : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0, 6957, 5760,    0,    0,    0, 5377,\n",
      "            0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "내가 좋아하는 국민 착장 : \n",
      "{'input_ids': tensor([[   2, 5679,    0, 5507,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'pooler_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33772/1746033033.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33772/2675219235.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, attention_mask)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#gradient 계산을 중지\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;31m#         x = self.dropout(pooler)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33772/2675219235.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, attention_mask)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#gradient 계산을 중지\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;31m#         x = self.dropout(pooler)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33772/2675219235.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, attention_mask)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#gradient 계산을 중지\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;31m#         x = self.dropout(pooler)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'pooler_output'"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    loss_sum = 0\n",
    "    model.train()\n",
    "    for batch_id, (input_ids, token_type_ids, attention_mask, tensor_label) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(input_ids, token_type_ids, attention_mask)\n",
    "        loss = loss_fn(out, tensor_label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "#         scheduler.step()  # Update learning rate schedule\n",
    "        batch_acc = calc_accuracy(out, tensor_label)\n",
    "        train_acc += batch_acc\n",
    "        loss_sum += loss.data.cpu().numpy()\n",
    "        #f batch_id % log_interval == 0:\n",
    "        print(\"epoch {} batch id {}/{} loss {} train acc {}\".format(e+1, batch_id+1, len(train_dataloader), loss.data.cpu().numpy(), batch_acc))\n",
    "    print(\"epoch {} train acc {} loss mean {}\".format(e+1, train_acc / (batch_id+1), loss_sum / len(train_dataloader)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, (input_ids, token_type_ids, attention_mask, tensor_label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "            out = model(input_ids, token_type_ids, attention_mask)\n",
    "            test_acc += calc_accuracy(out, tensor_label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
