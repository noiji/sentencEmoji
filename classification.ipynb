{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets  # for vscode\n",
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#KoBERT\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "# model = AutoModel.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# KoELECTRA-Base\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\")\n",
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-base-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentencEmojiDataset(Dataset):\n",
    "    def __init__(self, directory, tokenizer):\n",
    "        \n",
    "        data = pd.read_csv(directory, encoding='UTF-8')\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = list(data.iloc[:,0])\n",
    "       \n",
    "        emojis = list(data.iloc[:,1])\n",
    "        emojis_unique = list(set(emojis))\n",
    "        \n",
    "        self.labels = [emojis_unique.index(i) for i in emojis]\n",
    " \n",
    "        self.labels_dict = {'key': range(len(emojis_unique)), 'value': emojis_unique}\n",
    "        \n",
    "    def __getitem__(self, i): #collate 이전 미리 tokenize를 시켜주자\n",
    "        tokenized = self.tokenizer(str(self.sentences[i]), return_tensors='pt')\n",
    "        \n",
    "        #아래 세 개는 tokenizer가 기본적으로 반환하는 정보. BERT의 input이기도 함\n",
    "        input_ids = tokenized['input_ids']\n",
    "        token_type_ids = tokenized['token_type_ids']\n",
    "        attention_mask = tokenized['attention_mask']\n",
    "        \n",
    "#         print(str(self.sentences[i]) +' : ')\n",
    "#         print(tokenized)\n",
    "        \n",
    "        return {'input_ids': input_ids, 'token_type_ids': token_type_ids, \n",
    "                'attention_mask': attention_mask, 'label': self.labels[i]}\n",
    "         \n",
    "    def __len__(self): #data loader가 필요로 하여 필수적으로 있어야 하는 함수\n",
    "        return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class collate_fn:\n",
    "    def __init__(self, labels_dict):\n",
    "        self.num_labels = len(labels_dict)\n",
    "        \n",
    "    def __call__(self, batch): #batch는 dataset.getitem의 return 값의 List. eg. [{}, {}. ...]\n",
    "        #batch내 최대 문장 길이(토큰 개수)를 먼저 구해서 padding할 수 있도록 하기\n",
    "        batchlen = [sample['input_ids'].size(1) for sample in batch] #tensor값을 반환하기 때문에 1번째 차원의 길이를 구함\n",
    "        maxlen = max(batchlen)\n",
    "        input_ids = []\n",
    "        token_type_ids = []\n",
    "        attention_mask = []\n",
    "        #padding: [5, 6] [0, 0,  ...]을 concatenate 하는 방식으로 패딩\n",
    "        for sample in batch:\n",
    "            pad_len = maxlen - sample['input_ids'].size(1)\n",
    "            pad = torch.zeros((1, pad_len), dtype=torch.int)            \n",
    "            input_ids.append(torch.cat([sample['input_ids'], pad], dim=1))\n",
    "            token_type_ids.append(torch.cat([sample['token_type_ids'], pad], dim=1))\n",
    "            attention_mask.append(torch.cat([sample['attention_mask'], pad], dim=1))\n",
    "        #batch 구성\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "        attention_mask = torch.cat(attention_mask, dim=0)\n",
    "        \n",
    "        #one-hot encoding\n",
    "        #batch 내 라벨을 tensor로 변환\n",
    "        tensor_label = torch.tensor([sample['label'] for sample in batch])\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask, tensor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "N                     2349\n",
       "{love}                 141\n",
       "{kind-smile}           119\n",
       "{laughing-out}          96\n",
       "{open-mouth-smile}      84\n",
       "{good-job}              81\n",
       "{solid-sad}             68\n",
       "😭                       49\n",
       "{star}                  38\n",
       "{dunno}                 36\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/twitter_clean.csv', encoding=\"UTF-8\")\n",
    "print(len(df['y'].value_counts()))\n",
    "df['y'].value_counts(sort = True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'] = np.random.randn(df.shape[0], 1)\n",
    "msk = np.random.rand(len(df)) <= 0.7\n",
    "\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "\n",
    "train.to_csv('data/train.csv', index=False)\n",
    "test.to_csv('data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.collate_fn at 0x190f8134a20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = sentencEmojiDataset('data/train.csv', tokenizer)\n",
    "test = sentencEmojiDataset('data/test.csv', tokenizer)\n",
    "\n",
    "train_collate_fn = collate_fn(train.labels_dict)\n",
    "test_collate_fn = collate_fn(test.labels_dict)\n",
    "\n",
    "train_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 20  \n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x190f836a390>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=batch_size, collate_fn=train_collate_fn, shuffle = True, drop_last = True)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size, collate_fn=test_collate_fn, shuffle = False, drop_last = False)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERTClassifier(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  bert,\n",
    "#                  hidden_size = 768,\n",
    "#                  num_classes=2,\n",
    "#                  dr_rate=None,\n",
    "#                  params=None):\n",
    "#         super(BERTClassifier, self).__init__()\n",
    "#         self.bert = bert\n",
    "#         # do not train bert parameters\n",
    "#         for p in self.bert.parameters():\n",
    "#             p.requires_grad = False\n",
    "#         self.dr_rate = dr_rate\n",
    "                 \n",
    "#         self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        \n",
    "#         if dr_rate:\n",
    "#             self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "#     def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "#         #eval: drop out 중지, batch norm 고정과 같이 evaluation으로 모델 변경\n",
    "#         self.bert.eval()\n",
    "#         #gradient 계산을 중지\n",
    "#         with torch.no_grad():\n",
    "#             x = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
    "# #         x = self.dropout(pooler)\n",
    "#         return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELECTRAClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 electra,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        \n",
    "        super(ELECTRAClassifier, self).__init__()\n",
    "        \n",
    "        self.electra = electra\n",
    "        \n",
    "        # do not train electra parameters\n",
    "        for p in self.electra.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        self.dr_rate = dr_rate\n",
    "        \n",
    "        \n",
    "        #self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        \n",
    "#         # 방법 1 -> forward에서 처리해줘야 함. \n",
    "#         self.classifier1 = nn.Linear(hidden_size, 100) # y = Wx\n",
    "#         self.classifier2 = nn.Linear(100, num_classes) # z = Uy\n",
    "#         #layer 추가 시 activation function을 주지 않으면 의미가 없음. \n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "        #방법 2 -> forward에서 별도 처리 필요 X\n",
    "        self.classifier = nn.Sequential(nn.Linear(hidden_size, 100), nn.ReLU(), nn.Linear(100, num_classes))\n",
    "        \n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        #eval: drop out 중지, batch norm 고정과 같이 evaluation으로 모델 변경\n",
    "        self.electra.eval()\n",
    "        \n",
    "        #gradient 계산을 중지\n",
    "        with torch.no_grad():\n",
    "            #ElectraModel은 pooled_output을 리턴하지 않는 것을 제외하고 BertModel과 유사합니다.\n",
    "#            x = self.electra(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
    "\n",
    "            x = self.electra(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :] \n",
    "            #.last_hidden_state[:, 0, :]: [batch , CLS 위치, depth]\n",
    "            \n",
    "            #Sentence Embedding으로 무엇을 넣을까? CLS, average, ... (Bert의 경우에는 Sentence BERT라는 게 제안되었다고 함)\n",
    "                   \n",
    "        x = self.dropout(x)\n",
    "      \n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(set(list(df.iloc[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BERTClassifier(model,  dr_rate=0.5, num_classes = len(label))\n",
    "model = ELECTRAClassifier(model,  dr_rate=0.2, num_classes = len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer와 schedule 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 24,\n",
       " 15,\n",
       " 24,\n",
       " 26,\n",
       " 26,\n",
       " 19,\n",
       " 24,\n",
       " 26,\n",
       " 29,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 10,\n",
       " 11,\n",
       " 26,\n",
       " 26]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5810, 6.5111, 2.1117, 8.6815, 1.8603, 7.8133, 8.6815, 7.1030, 8.6815,\n",
      "        3.2556, 7.8133, 0.8312, 3.5515, 7.8133, 1.3022, 0.9302, 8.6815, 8.6815,\n",
      "        7.1030, 1.2602, 9.7667, 5.5810, 9.7667, 2.6943, 1.0852, 3.1253, 0.0490,\n",
      "        7.8133, 5.2089, 3.5515])\n"
     ]
    }
   ],
   "source": [
    "#Class Imbalance 문제 해결을 위한 weighted cross entropy \n",
    "class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(train.labels), y = train.labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(class_weights) #([1.0000, 1.0000, 4.0000, 1.0000, 0.5714])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': range(0, 30),\n",
       " 'value': ['👉',\n",
       "  '😑',\n",
       "  '😭',\n",
       "  '😅',\n",
       "  '{solid-sad}',\n",
       "  '😒',\n",
       "  '😏',\n",
       "  '😋',\n",
       "  '❗',\n",
       "  '{star}',\n",
       "  '🙌',\n",
       "  '{love}',\n",
       "  '{hope}',\n",
       "  '😣',\n",
       "  '{good-job}',\n",
       "  '{kind-smile}',\n",
       "  '🙋',\n",
       "  '😪',\n",
       "  '🤤',\n",
       "  '{open-mouth-smile}',\n",
       "  '👀',\n",
       "  '😎',\n",
       "  '😈',\n",
       "  '{dunno}',\n",
       "  '{laughing-out}',\n",
       "  '{flower}',\n",
       "  'N',\n",
       "  '🤦',\n",
       "  '{mad}',\n",
       "  '🔥']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight = class_weights, reduction = 'mean') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1/36 loss 3.4873974323272705 train acc 0.0\n",
      "epoch 1 batch id 2/36 loss 3.425734519958496 train acc 0.015625\n",
      "epoch 1 batch id 3/36 loss 3.4722392559051514 train acc 0.015625\n",
      "epoch 1 batch id 4/36 loss 3.3767757415771484 train acc 0.03125\n",
      "epoch 1 batch id 5/36 loss 3.436350107192993 train acc 0.015625\n",
      "epoch 1 batch id 6/36 loss 3.3984856605529785 train acc 0.0625\n",
      "epoch 1 batch id 7/36 loss 3.404128074645996 train acc 0.015625\n",
      "epoch 1 batch id 8/36 loss 3.4327847957611084 train acc 0.015625\n",
      "epoch 1 batch id 9/36 loss 3.3609459400177 train acc 0.03125\n",
      "epoch 1 batch id 10/36 loss 3.4111757278442383 train acc 0.03125\n",
      "epoch 1 batch id 11/36 loss 3.481961727142334 train acc 0.015625\n",
      "epoch 1 batch id 12/36 loss 3.4477055072784424 train acc 0.015625\n",
      "epoch 1 batch id 13/36 loss 3.4153988361358643 train acc 0.0\n",
      "epoch 1 batch id 14/36 loss 3.4706804752349854 train acc 0.015625\n",
      "epoch 1 batch id 15/36 loss 3.411259651184082 train acc 0.03125\n",
      "epoch 1 batch id 16/36 loss 3.4362828731536865 train acc 0.015625\n",
      "epoch 1 batch id 17/36 loss 3.4401156902313232 train acc 0.015625\n",
      "epoch 1 batch id 18/36 loss 3.4037880897521973 train acc 0.015625\n",
      "epoch 1 batch id 19/36 loss 3.3541295528411865 train acc 0.03125\n",
      "epoch 1 batch id 20/36 loss 3.4013121128082275 train acc 0.015625\n",
      "epoch 1 batch id 21/36 loss 3.4121243953704834 train acc 0.015625\n",
      "epoch 1 batch id 22/36 loss 3.4619221687316895 train acc 0.0\n",
      "epoch 1 batch id 23/36 loss 3.44126033782959 train acc 0.03125\n",
      "epoch 1 batch id 24/36 loss 3.4212586879730225 train acc 0.015625\n",
      "epoch 1 batch id 25/36 loss 3.358691930770874 train acc 0.03125\n",
      "epoch 1 batch id 26/36 loss 3.4094085693359375 train acc 0.0625\n",
      "epoch 1 batch id 27/36 loss 3.3894569873809814 train acc 0.03125\n",
      "epoch 1 batch id 28/36 loss 3.3856558799743652 train acc 0.015625\n",
      "epoch 1 batch id 29/36 loss 3.4468483924865723 train acc 0.03125\n",
      "epoch 1 batch id 30/36 loss 3.3215785026550293 train acc 0.046875\n",
      "epoch 1 batch id 31/36 loss 3.3980159759521484 train acc 0.03125\n",
      "epoch 1 batch id 32/36 loss 3.356678009033203 train acc 0.0\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    loss_sum = 0\n",
    "    model.train()\n",
    "    for batch_id, (input_ids, token_type_ids, attention_mask, tensor_label) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(input_ids, token_type_ids, attention_mask)\n",
    "        loss = loss_fn(out, tensor_label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "#         scheduler.step()  # Update learning rate schedule\n",
    "        batch_acc = calc_accuracy(out, tensor_label)\n",
    "        train_acc += batch_acc\n",
    "        loss_sum += loss.data.cpu().numpy()\n",
    "        #f batch_id % log_interval == 0:\n",
    "        print(\"epoch {} batch id {}/{} loss {} train acc {}\".format(e+1, batch_id+1, len(train_dataloader), loss.data.cpu().numpy(), batch_acc))\n",
    "    print(\"epoch {} train acc {} loss mean {}\".format(e+1, train_acc / (batch_id+1), loss_sum / len(train_dataloader)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, (input_ids, token_type_ids, attention_mask, tensor_label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "            out = model(input_ids, token_type_ids, attention_mask)\n",
    "            test_acc += calc_accuracy(out, tensor_label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
