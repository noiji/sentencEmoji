{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets  # for vscode\n",
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "model = AutoModel.from_pretrained(\"monologg/kobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentencEmojiDataset(Dataset):\n",
    "    def __init__(self, directory, tokenizer):\n",
    "        \n",
    "        data = pd.read_csv(directory, encoding='UTF-8')\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = list(data.iloc[:,0])\n",
    "       \n",
    "        emojis = list(data.iloc[:,1])\n",
    "        emojis_unique = list(set(emojis))\n",
    "        \n",
    "        self.labels = [emojis_unique.index(i) for i in emojis]\n",
    " \n",
    "        self.labels_dict = {'key': range(len(emojis_unique)), 'value': emojis_unique}\n",
    "        \n",
    "    def __getitem__(self, i): #collate ì´ì „ ë¯¸ë¦¬ tokenizeë¥¼ ì‹œì¼œì£¼ì\n",
    "        tokenized = self.tokenizer(str(self.sentences[i]), return_tensors='pt')\n",
    "        \n",
    "        #ì•„ë˜ ì„¸ ê°œëŠ” tokenizerê°€ ê¸°ë³¸ì ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ì •ë³´. BERTì˜ inputì´ê¸°ë„ í•¨\n",
    "        input_ids = tokenized['input_ids']\n",
    "        token_type_ids = tokenized['token_type_ids']\n",
    "        attention_mask = tokenized['attention_mask']\n",
    "        \n",
    "        print(str(self.sentences[i]) +' : ')\n",
    "        print(tokenized)\n",
    "        \n",
    "        return {'input_ids': input_ids, 'token_type_ids': token_type_ids, \n",
    "                'attention_mask': attention_mask, 'label': self.labels[i]}\n",
    "         \n",
    "    def __len__(self): #data loaderê°€ í•„ìš”ë¡œ í•˜ì—¬ í•„ìˆ˜ì ìœ¼ë¡œ ìˆì–´ì•¼ í•˜ëŠ” í•¨ìˆ˜\n",
    "        return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class collate_fn:\n",
    "    def __init__(self, labels_dict):\n",
    "        self.num_labels = len(labels_dict)\n",
    "        \n",
    "    def __call__(self, batch): #batchëŠ” dataset.getitemì˜ return ê°’ì˜ List. eg. [{}, {}. ...]\n",
    "        #batchë‚´ ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´(í† í° ê°œìˆ˜)ë¥¼ ë¨¼ì € êµ¬í•´ì„œ paddingí•  ìˆ˜ ìˆë„ë¡ í•˜ê¸°\n",
    "        batchlen = [sample['input_ids'].size(1) for sample in batch] #tensorê°’ì„ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì— 1ë²ˆì§¸ ì°¨ì›ì˜ ê¸¸ì´ë¥¼ êµ¬í•¨\n",
    "        maxlen = max(batchlen)\n",
    "        input_ids = []\n",
    "        token_type_ids = []\n",
    "        attention_mask = []\n",
    "        #padding: [5, 6] [0, 0,  ...]ì„ concatenate í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ íŒ¨ë”©\n",
    "        for sample in batch:\n",
    "            pad_len = maxlen - sample['input_ids'].size(1)\n",
    "            pad = torch.zeros((1, pad_len), dtype=torch.int)            \n",
    "            input_ids.append(torch.cat([sample['input_ids'], pad], dim=1))\n",
    "            token_type_ids.append(torch.cat([sample['token_type_ids'], pad], dim=1))\n",
    "            attention_mask.append(torch.cat([sample['attention_mask'], pad], dim=1))\n",
    "        #batch êµ¬ì„±\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "        attention_mask = torch.cat(attention_mask, dim=0)\n",
    "        \n",
    "        #one-hot encoding\n",
    "        #batch ë‚´ ë¼ë²¨ì„ tensorë¡œ ë³€í™˜\n",
    "        tensor_label = torch.tensor([sample['label'] for sample in batch])\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask, tensor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "N    235\n",
       "ğŸ˜‚     80\n",
       "â¤     53\n",
       "ğŸ˜­     49\n",
       "ğŸ˜     32\n",
       "ğŸ’•     28\n",
       "ğŸ’œ     26\n",
       "ğŸ”¥     25\n",
       "ğŸ‘     24\n",
       "ğŸ¥º     24\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/twitter_clean.csv', encoding=\"UTF-8\")\n",
    "print(len(df['y'].value_counts()))\n",
    "df['y'].value_counts(sort = True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'] = np.random.randn(df.shape[0], 1)\n",
    "msk = np.random.rand(len(df)) <= 0.7\n",
    "\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "\n",
    "train.to_csv('data/train.csv', index=False)\n",
    "test.to_csv('data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.collate_fn at 0x25cd19c15f8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = sentencEmojiDataset('data/train.csv', tokenizer)\n",
    "test = sentencEmojiDataset('data/test.csv', tokenizer)\n",
    "\n",
    "train_collate_fn = collate_fn(train.labels_dict)\n",
    "test_collate_fn = collate_fn(test.labels_dict)\n",
    "\n",
    "train_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 20  \n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x25cd19d2dd8>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=batch_size, collate_fn=train_collate_fn, shuffle = True, drop_last = True)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size, collate_fn=test_collate_fn, shuffle = False, drop_last = False)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        # do not train bert parameters\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        \n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        #eval: drop out ì¤‘ì§€, batch norm ê³ ì •ê³¼ ê°™ì´ evaluationìœ¼ë¡œ ëª¨ë¸ ë³€ê²½\n",
    "        self.bert.eval()\n",
    "        #gradient ê³„ì‚°ì„ ì¤‘ì§€\n",
    "        with torch.no_grad():\n",
    "            x = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).pooler_output\n",
    "#         x = self.dropout(pooler)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(set(list(df.iloc[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(model,  dr_rate=0.5, num_classes = len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizerì™€ schedule ì„¤ì •\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19,\n",
       " 15,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 15,\n",
       " 26,\n",
       " 18,\n",
       " 15,\n",
       " 19,\n",
       " 21,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 15,\n",
       " 19,\n",
       " 19,\n",
       " 21]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1807, 0.7010, 1.8694, 2.4926, 1.7256, 1.1217, 1.3196, 1.7256, 1.2463,\n",
      "        2.2433, 2.2433, 1.8694, 3.2048, 1.0197, 1.1217, 0.4314, 2.0394, 2.4926,\n",
      "        1.3196, 0.1297, 1.3196, 0.6231, 1.1807, 1.1807, 1.3196, 1.7256, 1.3196,\n",
      "        1.6024, 1.8694, 1.7256])\n"
     ]
    }
   ],
   "source": [
    "#Class Imbalance ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ weighted cross entropy \n",
    "class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(train.labels), y = train.labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(class_weights) #([1.0000, 1.0000, 4.0000, 1.0000, 0.5714])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': range(0, 30),\n",
       " 'value': ['ğŸ™',\n",
       "  'ğŸ˜­',\n",
       "  'ğŸ’¯',\n",
       "  'ğŸ¹',\n",
       "  'ğŸ˜',\n",
       "  'ğŸ’–',\n",
       "  'ğŸ˜†',\n",
       "  'ğŸ’—',\n",
       "  'ğŸ‘',\n",
       "  'âœ¨',\n",
       "  'ğŸ‘‰',\n",
       "  'ğŸ˜‘',\n",
       "  'ğŸ¤¤',\n",
       "  'ğŸ’•',\n",
       "  'ğŸ˜',\n",
       "  'ğŸ˜‚',\n",
       "  'ğŸ˜',\n",
       "  'âœŒ',\n",
       "  'ğŸ¥°',\n",
       "  'N',\n",
       "  'ğŸ˜Š',\n",
       "  'â¤',\n",
       "  'â£',\n",
       "  'ğŸ’œ',\n",
       "  'ğŸ˜˜',\n",
       "  'ğŸ¤­',\n",
       "  'ğŸ¥º',\n",
       "  'ğŸ”¥',\n",
       "  'ğŸ’™',\n",
       "  'ğŸ’š']}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight = class_weights, reduction = 'mean') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›”ì´ ë„ˆë¬´ ê¸°ë‹¤ë ¤ì ¸ìš” : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "ë‹¤ìŒì—ë‘ ì—˜ë‹¤ë‹˜ì´ ë§˜ì°í•´ë‘˜ ìˆ˜ ìˆëŠ” ì˜ˆìœê³³ì„ ë§Œë“¤ì–´ë³´ê² ì–´ìš¥ ë°°ì„¸ì§„ìƒì¼ê°€ëµ¤ìê³ ì„¸ì§„ì´ì˜ëˆˆë¶€ì‹œê³ ë„ì•„ë¦„ë‹¤ìš´ì œë§‰ë°°ì„¸ì§„ ìƒì¼ ì¹´í˜ì— í˜‘ë ¥í•œ ê·¸ë¦¼ì´ì—ìš”ìƒì¼ì¶•í•˜í•´ : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0, 6629, 7142,    0,    0,    0,    0,    0,    0,\n",
      "            0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "í•´ì  ë„ê¹¨ë¹„ ê¹ƒë°œ ì¶œì¥ì‹­ì˜¤ì•¼ì—´ì • í•´ì ì„  ë‹¨ì›ë“¤ê³¼ : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ì‹ ê³ í•´ë„ ë³„ ì†Œìš©ë„ ì—†ë‚˜ë´ì‹ ë°œì€ ë‹¤ë“¤ ì–´ì¼€ ë³´ê´€ í•˜ì‹œì§€ : \n",
      "{'input_ids': tensor([[   2,    0, 6356,    0,    0,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ë¯¼ì„œì—ê²Œ ë‚˜ëŠ” ë‹¹ì‹ ì˜ ì°¨ì„ ìœ¼ë¡œ ëŒë ¤ê°€ê³  ìˆë‹¤ê³  ë§í•˜ê³  ì‹¶ì–´ìš” ì‚¬ë‘í•´ìš” : \n",
      "{'input_ids': tensor([[   2,    0, 5658,    0,    0,    0,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ë²Œì¹™ì˜ìƒ ëª¨ëª¨ëœë“œ   ë°°ì† ëŒ„ìŠ¤ : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "ë„ë‚˜ëŠ” ê·¸ë ‡ê±°ë“  : \n",
      "{'input_ids': tensor([[2, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "ì‚¬ëŒ í•œ ëª… ì‚´ë ¤ì£¼ì„¸ìš” : \n",
      "{'input_ids': tensor([[   2, 6498, 7828, 6204,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "ì– ìƒ‰ì¡°í•© ì˜ ëª»í•˜ëŠ”ë° ì›ì‘í•˜ê³  ë¹„ìŠ·í•˜ê²Œ ê°‘ë‹ˆë‹¤ : \n",
      "{'input_ids': tensor([[   2,    0,    0, 7174,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ê¸ˆìš”ì¼ì€ ë¦¬ìŠ¤íŒ… ì‰¬ì–´ê°€ëŠ”ë‚ í† ìš”ì¼ ì €ë…ì—” ëª…í™” ìƒ¤ëƒ¥ì´ ê²½ë§¤ê°€ ìˆìœ¼ë‹ˆë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦½ë‹ˆë‹¤ : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ì§„í™˜ì´ê°€ìš°ë¦¬í–‰ë³µì´ì•¼ í‰ìƒ ì§„í™˜ì´íŒ¬ í‰ìƒ ì§„í™˜ì´í¸ì˜¤ë˜ì˜¤ë˜ ì‘ì›í• ê²Œ ì‚¬ë‘í•´ : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ì‹œë¦° ê°€ì„ì€ ì²­ì†Œë…„ê´€ëŒê°€ ì´ê¸° ë•Œë¬¸ì—ë‚˜ìœë†ˆ ì—…ë³´ í›„íšŒê³µì„ ì¢‹ì•„ í•˜ì‹œëŠ” ëª¨ë“ ë¶„ë“¤ì´ ê°€ë³ê²Œ ì½ê¸° ì¢‹ë‹µë‹ˆë‹¤ì•Œë¼ë”˜ ì™¸ íƒ€ í”Œë«í¼ì€ íˆ¬ì—ì¦ˆë‹˜ ë„ˆë¬´ ê°ì‚¬í•´ìš”ì˜¹ : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, 6995, 7581,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ì¼ê°„ ë„ ê°€ëŠ¥í•˜ë„ë¡ ì—´ìŠ¤ë°ì€ ê³„ì†ëœë‹¤ : \n",
      "{'input_ids': tensor([[   2, 7127, 5859,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "ì— ì¹´ìš´íŠ¸ë‹¤ìš´ ì¶œì—° ì•ˆë‚´ ì ì‹œ í›„ ì—¬ì„¯ì‹œ : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0, 7968,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ë„ì˜ì´ ì»¤ë²„ê³¡ì´ ë“¤ì–´ê°„ ë„ í•¨ ì˜¤ëŠ˜ë„ ê³ ìƒí–ˆì–´ìš” íŠ¸ë ˆì €ë©”ì´ì»¤ : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0, 5859, 7837,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ê¹€í˜¸ì¤‘ë‚˜íƒœì£¼ìì„œì „ì£¼ì¤‘ì£¼ë§í˜¸ì¤‘ì‚¬ë‘ ê¹€í˜¸ì¤‘ ê¹€í˜¸ì¤‘ìì„œì „ ë¹›ë‚˜ëŠ” ë³„ ê¹€í˜¸ì¤‘ ì‚¬ë‘ìŠ¤ëŸ° ë³„ìš°ì£¼ ëê¹Œì§€ ë¹›ë‚˜ì„¸ìš” ì†Œì¤‘í•œ ìš°ë¦¬ ë³„ ê¹€í˜¸ì¤‘ì§€ê¸ˆê¹Œì§€ì²˜ëŸ¼ ë‹¹ë‹¹í•˜ê³  ê±°ì¹¨ì—†ì´ ë‹¬ë¦¬ëŠ”ê±°ì•¼ê°€ì : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0, 6356,    0,    0,    0,    0,    0,    0,\n",
      "         7007, 6356,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ì˜¤ëŠ˜ ë‚´ ê¿ˆì—ì„œ ë†€ëŸ¬ì™€ì£¼ì–´ : \n",
      "{'input_ids': tensor([[   2, 6966, 5678,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "ë„ˆë¬´ ì¢‹ì•„ìš” ì§„ì§œ : \n",
      "{'input_ids': tensor([[   2,    0,    0, 7347,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "ë©˜ì…˜ì€ ë‚´ì¼ ë§ˆì € ë‹¬ê²Œìš”ì˜¤ë‹¤ë“¤ ì¢‹ì€ê¿ˆ ê¾¸ì‹œê¸°ë¥¼ : \n",
      "{'input_ids': tensor([[   2,    0,    0, 6145,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ê·¸ëƒ¥ ì‚¬ë‘ì„ ì „íŒŒí•˜ëŠ”ê±°ì•¼ ì›”ìš”ì¼ì— ê°”ì—ˆì–´   í˜ì´ìŠ¤ë¶ì—ì„œ ì˜¤ë˜ì „ì— ë³¸ ì‚¬ì§„ì´ì—ìš” : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0,    0,    0, 6383,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ìœ ì£¼ ë†€ì´   ê¿ˆê¾¸ëŠ”ë¼ë””ì˜¤ ì™„ë£Œ : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "ì´ ì—í”¼ì†Œë“œì—ì„œ ë‚´ê°€ ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ë…¸ë˜ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤  ì—¼ì†Œë¥¼ ë°ë¦¬ê³  ì²˜ìŒìœ¼ë¡œ ëª¸ì¹˜ì¥ì„ í–ˆì–´ìš” : \n",
      "{'input_ids': tensor([[   2, 7096,    0, 5679,    0,    0,    0, 7295,    0,    0,    0,    0,\n",
      "            0, 7874,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "í–‰ë³µí•œ í•˜ë£¨ ë§ˆë¬´ë¦¬ ì˜ í•˜ê¸¸ ë°”ë˜ìš” : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0, 7174,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ë¦¬íŠ¸ìœ— í•´ì£¼ì‹œë©´ ëœë¤ì‚¬ì§„ ë°œì†¡í•´ë“œë¦½ë‹ˆë‹¤ : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "ë‚¨ì€ ì¸ìƒì„ ê±¸ê³  ë§í• ê²Œ ë‘ë²ˆì€ ì—†ì–´ ë„Œ ë‚˜ì˜ ë§ˆì§€ë§ˆì•… : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0,    0, 5695,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ë§ˆì¹¨ë‚´ ì£¼ë¥¼ ë§Œë“¤ê³  ë²„ê·¸ ì‚¬ì´ì–¸ìŠ¤ì— ìˆì„ ë•Œ : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0,    0, 5965,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ì„ì˜ì›… ê°€ì˜¨ì°¨íŠ¸ë®¤ì§ì–´ì›Œì¦ˆ : \n",
      "{'input_ids': tensor([[2, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "ì»´ë°± ì¶•í•˜í•´ : \n",
      "{'input_ids': tensor([[2, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "ë“œë¦¬í•€ ë“œë¦¬í•€ê³¼í•¨ê»˜í•˜ëŠ”ë©˜ì…˜íŒŒí‹° ì˜¬ë ¤ë“œë¦° ì„¸ ì‚¬ì´ì¦ˆ ëª¨ë‘ ë§í¬ ë“œë¦´ê»˜ìš¥ì˜ˆì˜ê²Œ ì‚¬ìš©í•´ì£¼ì„¸ìš” : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0, 6579,    0,    0, 6140,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "íŠ¸ì¹œê³¼ì…ë§›ê¶í•©ì•Œì•„ë³´ê¸°ë§ˆë¼íƒ•ì´ë‘ ë¯¼ì´ˆëŠ” ë§ì´ ì•ˆë¨¹ì–´ë´ì„œ ë§›ì„ ëª°ë¼ìš© : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ì¶”ì²¨ ì£¼ì ‘í‹°ì»¤ ë³´ë‚´ë“œë ¤ìš” : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "í—› ì¢‹ì€ ì •ë³´ ê°ì‚¬í•©ë‹ˆë‹¤ : \n",
      "{'input_ids': tensor([[   2, 7893, 7272, 7229,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "ë¼ê³  ë§í–ˆë‹¤ ì˜¤ë©”ê°€ì•„ì•„ì•— : \n",
      "{'input_ids': tensor([[   2, 6004,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "ì¹´ì—˜ì˜¤í•˜í™”  ì•„ì˜¤ì´ ìŒë‘¥ì´ ìƒì¼ ì»µí™€ë” í˜‘ë ¥ì§„ ê³µê°œ : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0,    0, 5454,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ë°˜í¬ í•´ëƒˆë‹¤í•œë³µì€ í•œêµ­ ì „í†µì˜ìƒ ì½œë¦°ìŠ¤ ì‚¬ì „ì— ë“±ì¬ : \n",
      "{'input_ids': tensor([[   2,    0,    0, 7829,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ìƒë°©ì†¡ ë¶„ ì „ ì´ë¼ëŠ” ë§ì— ê¸´ì¥í•˜ëŠ”ìŠ¤í˜ì…œ ë¯¼ì§€ ì§„í¬ : \n",
      "{'input_ids': tensor([[   2,    0, 6416, 7207, 7103,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ì‚¬ë‘ë·”ë‹˜ ì ê¸ˆí™”ë©´ íƒœí˜•ì´ë„ ë„˜ ì´ë»ìš” : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0, 5698,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "ë‹´ì£¼ ìœ„ í•˜ë©´ ì£¼ ì—°ì† ì‹ ê¸°ë¡ì´ì—ìš” : \n",
      "{'input_ids': tensor([[   2,    0, 7044, 7811, 7276,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ì¹œêµ¬ë“¤ì´ ì¢‹ì€ ì·¨ì§€ë¡œ ì‹œì‘í•œ í”„ë¡œì íŠ¸ì— ì‘ì› ë³´ë‚´ì£¼ì‹œë©´ ê°ì‚¬ë“œë¦¬ê² ìŠ´ëŒœ ë³‘ ë¦¬ì‹¸ì´í´ ëœ ì†Œì¬ë¡œ ì œì‘ ë¬´ë ¤ ì›¨ì´ë¡œ ì œì‘ì„ í–ˆë‹¤ë„¤ì—¬ ì•„ì§ ì˜¤í”ˆ ì „ì´ë¼ ì•Œë¦¼ì‹ ì²­ í•˜ì‹œë©´ ì˜¤í”ˆì‹œ ì•Œë¦¼ë°›ê³  ì–¼ë¦¬ë²„ë“œê°€ì— ë‚´ê°€ ë­ë¬ì–´ ì´ê¸¸ê±°ë¬ì–ì•„ ì´ ê¸°ì ì•„ë‹Œ ê¸°ì ì„ ìš°ë¦¬ê°€ ë§Œë“¤ì—ˆã…ã…ã…ìœ„íƒˆí™˜ : \n",
      "{'input_ids': tensor([[   2,    0, 7272,    0,    0,    0,    0,    0,    0, 6361,    0, 5899,\n",
      "            0,    0,    0,    0,    0,    0, 6806, 6969,    0,    0,    0,    0,\n",
      "            0,    0, 5679,    0,    0, 7096,    0,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ì²« ë²ˆì§¸ ì‹œë„ ë‘ ë²ˆì§¸ ì•¨ë²”ì—ì„œ ì˜ê°ì„ ì–»ìŒ : \n",
      "{'input_ids': tensor([[   2, 7430, 6329,    0, 5907, 6329,    0,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "í…ŒìŠ¤íŠ¸ì»·   í˜¹ì‹œë¼ë„ ë¹Œë³´ë“œ ë¸”ë½ í•˜ì‹  ë¶„ë“¤ ê³„ì‹œë©´ í•´ì œ í•˜ì„¸ìš” : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0,    0,    0,    0, 7814,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ë°°ì„¸ì§„ìƒì¼ê°€ëµ¤ìê³ ì„¸ì§„ì´ì˜ëˆˆë¶€ì‹œê³ ë„ì•„ë¦„ë‹¤ìš´ì œë§‰ë°°ì„¸ì§„ ìƒì¼ ì¹´í˜ì— í˜‘ë ¥í•œ ê·¸ë¦¼ì´ì—ìš”ìƒì¼ì¶•í•˜í•´ : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "ìœ ê²¸ ê°“ì„¸ë¸  ë‹¤ê°€ì˜¤ëŠ” ì¬í˜„ì´ì˜ ì‚´ ìƒì¼ : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0, 6519,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ì¹œí•´ì§€ë©´ ì„ ë¬¼ë„ í¼ì¤ë‹ˆë‹¤ : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "ì†”ë¡œ ì£¼ë…„    ê°™ì§€ë§Œ ë‹¬ë¼ìš” : \n",
      "{'input_ids': tensor([[   2,    0, 7279,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "ë¦¬ë”ì¦ˆê°€ í–‰ë³µí•˜ë©´ ëœê±°ì•¼êµ­ê°€ëŒ€í‘œ ëŒ„ì„œë“¤ì˜ ë³¸ìºì°¾ê¸° ì—¬í–‰í•´ì¹˜ì§€ì•Šì•„ìŠ¤ìš°íŒŒ ëª© ë°¤  ìƒˆí•´ ì²« í”„ë¦¬ìŠ¤íƒ€ì¼ : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0,    0, 6217, 6303,    0, 7430,    0,\n",
      "            3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "í¬ë˜ë¹„í‹° ì•¨ëŸ° ì •ëª¨ íƒœì˜ : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0, 7599,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "ì˜¤ëŠ˜ë„ ë„ê²½ìˆ˜ ì—‘ì†Œë””ì˜¤ : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "ì˜ ê¸°ìš´ë°›ì•„ ìœ„ê¹Œì§€ : \n",
      "{'input_ids': tensor([[   2, 7095,    0,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "ì• êµìŸì´ ìš”í•˜ë‹ˆê°€ ì˜¤ëŠ˜ í•˜ë£¨ì˜ í˜ì´ ë˜ì…¨ê¸°ë¥¼ ë°”ë¼ë©° ê°ê¸° ì¡°ì‹¬í•˜ì‹œê³  ë‚´ì¼ ë§Œë‚˜ìš” : \n",
      "{'input_ids': tensor([[   2,    0,    0, 6966,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ìœ ê²¸ ê°“ì„¸ë¸   íŒ”ë¡œìš° ì´ë²¤íŠ¸ : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "í•˜í•˜ ê³ ë§ˆì›Œìš” í† ë¦¬ : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "ì˜¤ëŠ˜ì€ ì¢€ ë” í¸ì•ˆí•´ì¡ŒëŠ”ì§€ ëª¨ë¥´ê² ë„¤ ë¬´ë¦¬í•˜ì§€ë§ê³  ì²œì²œíˆ íšŒë³µí•˜ì ì§€ë¯¼ì´ê°€ ìµœê³  : \n",
      "{'input_ids': tensor([[   2,    0, 7266, 5837,    0,    0,    0,    0,    0,    0, 7459,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "í¬ì¹´ë¼ë„ ë‚˜ëˆ”í•´ì•¼í• ê¹Œ : \n",
      "{'input_ids': tensor([[2, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "ì—¬ì „íˆ ê°€ì¥ í˜„ì‹¤ì ì¸ : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "ì§„ì§„ ë¼í‚¤ ëª¬ìŠ¤íƒ€ì—‘ìŠ¤ ê¸°í˜„ ì˜ : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0, 7095,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "íŠ¹ì „ì€ ë‚˜ëˆ”ì¡´ì— ë‘ì—ˆìœ¼ë‹ˆ í¸í•˜ê²Œ ì±™ê²¨ê°€ì„¸ìš” : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "ë„µë„µ ì € ëŠ¦ê²Œ í•´ì‹œ íˆ¬í‘œ ë™ì°¸í•œê±° ì‚¬ì‹¤ : \n",
      "{'input_ids': tensor([[   2,    0, 7199,    0,    0, 7647,    0, 6502,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ë†€ë¼ìš´ ëª©í‘œ  ì´ ì–¸ê¸‰ë˜ì—ˆì„ ë•Œ ê·¸ì˜ ë¯¸ì†ŒëŠ” ë§ ê·¸ëŒ€ë¡œ í° ì†Œë¦¬ë¡œ : \n",
      "{'input_ids': tensor([[   2,    0, 6218, 7096,    0, 5965,    0,    0, 6160,    0, 7567,    0,\n",
      "            3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ì–´ëŠ ìª½ì´ë“  ë„ˆë¬´ ê¹Šê²Œ ìƒê° í•˜ëŠ”ë“¯ : \n",
      "{'input_ids': tensor([[2, 0, 0, 0, 0, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ì£¼í•™ë…„ ë¼í¬í‹°ì…€ì•³ìŠ¤íƒ€ì¼ : \n",
      "{'input_ids': tensor([[2, 0, 0, 3]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "ê¹€ë‚¨ì¤€ ì† ë„˜ ê·€ì—¬ìš´ê±°ìˆì£  : \n",
      "{'input_ids': tensor([[   2,    0, 6616, 5698,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "ì–´ë¥´ì‹  ë¶„ë“¤ì´ ì…ì€ ë³´ë¼ìƒ‰ ì™¸íˆ¬ë„ ì˜ˆ ëŠ” ì •ë§ ì„±ì‹¤í•˜ê³  ì°©í•œ ê±° ê°™ì•„ìš” : \n",
      "{'input_ids': tensor([[   2,    0,    0,    0,    0,    0, 6957, 5760,    0,    0,    0, 5377,\n",
      "            0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "ë‚´ê°€ ì¢‹ì•„í•˜ëŠ” êµ­ë¯¼ ì°©ì¥ : \n",
      "{'input_ids': tensor([[   2, 5679,    0, 5507,    0,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'pooler_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33772/1746033033.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33772/2675219235.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, attention_mask)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#gradient ê³„ì‚°ì„ ì¤‘ì§€\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;31m#         x = self.dropout(pooler)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33772/2675219235.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, attention_mask)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#gradient ê³„ì‚°ì„ ì¤‘ì§€\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;31m#         x = self.dropout(pooler)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33772/2675219235.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, attention_mask)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#gradient ê³„ì‚°ì„ ì¤‘ì§€\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;31m#         x = self.dropout(pooler)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'pooler_output'"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    loss_sum = 0\n",
    "    model.train()\n",
    "    for batch_id, (input_ids, token_type_ids, attention_mask, tensor_label) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(input_ids, token_type_ids, attention_mask)\n",
    "        loss = loss_fn(out, tensor_label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "#         scheduler.step()  # Update learning rate schedule\n",
    "        batch_acc = calc_accuracy(out, tensor_label)\n",
    "        train_acc += batch_acc\n",
    "        loss_sum += loss.data.cpu().numpy()\n",
    "        #f batch_id % log_interval == 0:\n",
    "        print(\"epoch {} batch id {}/{} loss {} train acc {}\".format(e+1, batch_id+1, len(train_dataloader), loss.data.cpu().numpy(), batch_acc))\n",
    "    print(\"epoch {} train acc {} loss mean {}\".format(e+1, train_acc / (batch_id+1), loss_sum / len(train_dataloader)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, (input_ids, token_type_ids, attention_mask, tensor_label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "            out = model(input_ids, token_type_ids, attention_mask)\n",
    "            test_acc += calc_accuracy(out, tensor_label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
