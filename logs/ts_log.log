2022-02-09T20:20:04,207 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-09T20:20:04,207 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-09T20:20:04,273 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-09T20:20:04,273 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-09T20:20:04,649 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages
Current directory: C:\Users\enoch9\Desktop\개발\sentencEmoji
Temp directory: C:\Users\enoch9\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4046 M
Python executable: c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe
Config file: logs\config\20220208222825868-startup.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Initial Models: sentencemoji=sentencemoji.mar
Log dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Metrics dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Model config: N/A
2022-02-09T20:20:04,649 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages
Current directory: C:\Users\enoch9\Desktop\개발\sentencEmoji
Temp directory: C:\Users\enoch9\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4046 M
Python executable: c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe
Config file: logs\config\20220208222825868-startup.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Initial Models: sentencemoji=sentencemoji.mar
Log dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Metrics dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Model config: N/A
2022-02-09T20:20:04,664 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220208222825868-startup.cfg",
  "modelCount": 1,
  "created": 1644326905869,
  "models": {
    "sentencemoji": {
      "1.0": {
        "defaultVersion": true,
        "marName": "sentencemoji.mar",
        "minWorkers": 8,
        "maxWorkers": 8,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-02-09T20:20:04,664 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220208222825868-startup.cfg",
  "modelCount": 1,
  "created": 1644326905869,
  "models": {
    "sentencemoji": {
      "1.0": {
        "defaultVersion": true,
        "marName": "sentencemoji.mar",
        "minWorkers": 8,
        "maxWorkers": 8,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-02-09T20:20:04,679 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220208222825868-startup.cfg
2022-02-09T20:20:04,679 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220208222825868-startup.cfg
2022-02-09T20:20:04,683 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220208222825868-startup.cfg validated successfully
2022-02-09T20:20:04,683 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220208222825868-startup.cfg validated successfully
2022-02-09T20:20:04,793 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model sentencemoji
2022-02-09T20:20:04,793 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model sentencemoji
2022-02-09T20:20:04,794 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model sentencemoji
2022-02-09T20:20:04,794 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model sentencemoji
2022-02-09T20:20:04,798 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model sentencemoji
2022-02-09T20:20:04,798 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model sentencemoji
2022-02-09T20:20:04,801 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model sentencemoji loaded.
2022-02-09T20:20:04,801 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model sentencemoji loaded.
2022-02-09T20:20:04,802 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: sentencemoji, count: 8
2022-02-09T20:20:04,802 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: sentencemoji, count: 8
2022-02-09T20:20:04,819 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:20:04,819 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:20:04,819 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:20:04,819 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:20:04,819 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:20:04,819 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:20:04,819 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:20:04,819 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:20:04,819 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:20:04,821 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-02-09T20:20:04,819 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:20:04,819 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:20:04,819 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:20:04,819 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:20:04,819 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:20:04,821 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-02-09T20:20:04,819 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:20:04,819 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:20:05,360 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-09T20:20:05,360 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-09T20:20:05,361 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-02-09T20:20:05,361 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-02-09T20:20:05,369 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-09T20:20:05,369 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-09T20:20:05,370 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-02-09T20:20:05,370 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-02-09T20:20:05,372 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-09T20:20:05,372 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-09T20:20:06,411 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-09T20:20:06,411 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-09T20:20:07,263 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644405607
2022-02-09T20:20:09,108 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:09,089 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:09,071 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:08,902 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:08,840 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:08,828 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:16,339 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - [PID]34108
2022-02-09T20:20:16,339 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - [PID]41900
2022-02-09T20:20:16,344 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,344 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,337 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - [PID]20856
2022-02-09T20:20:16,345 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,336 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - [PID]43448
2022-02-09T20:20:08,844 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:16,330 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - [PID]45372
2022-02-09T20:20:16,346 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,346 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,313 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:17.657737731933594|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644405607
2022-02-09T20:20:16,345 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,369 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:20:16,345 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:16,344 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,375 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:20:16,344 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,343 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:16,378 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:20:16,343 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:16,341 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - [PID]28124
2022-02-09T20:20:08,825 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:16,379 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,378 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:16,378 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:20:16,375 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:20:16,373 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:16,369 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:20:16,347 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:215.18698501586914|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644405607
2022-02-09T20:20:16,346 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,383 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:20:16,383 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:92.4|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644405607
2022-02-09T20:20:16,407 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616407
2022-02-09T20:20:16,346 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:16,425 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616425
2022-02-09T20:20:16,346 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - [PID]46812
2022-02-09T20:20:16,430 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,346 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,430 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:20:16,431 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616431
2022-02-09T20:20:16,346 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:16,430 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,434 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:20:16,429 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:16,425 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616425
2022-02-09T20:20:16,423 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:16,407 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616407
2022-02-09T20:20:16,402 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:20:16,394 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5341.37109375|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644405607
2022-02-09T20:20:16,383 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:20:16,379 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - [PID]42572
2022-02-09T20:20:16,439 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,379 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,439 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:20:16,439 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:16,379 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:16,469 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616469
2022-02-09T20:20:16,379 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:16,440 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:16,439 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:20:16,439 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change null -> WORKER_STARTED
2022-02-09T20:20:16,485 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:20:16,438 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:10836.66015625|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644405607
2022-02-09T20:20:16,486 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616486
2022-02-09T20:20:16,434 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:16,434 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:20:16,433 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:16,431 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616431
2022-02-09T20:20:16,430 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:20:16,431 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:20:16,492 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616492
2022-02-09T20:20:16,492 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:20:16,486 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616486
2022-02-09T20:20:16,564 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616563
2022-02-09T20:20:16,485 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:20:16,482 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:16,480 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:20:16,469 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:20:16,566 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:16,569 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616569
2022-02-09T20:20:16,469 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616469
2022-02-09T20:20:16,467 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:16,564 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616563
2022-02-09T20:20:16,485 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:67.0|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644405607
2022-02-09T20:20:16,492 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616492
2022-02-09T20:20:16,510 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:16,627 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:16,563 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:20:16,601 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:20:16,568 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:20:16,686 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:16,667 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:16,569 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405616569
2022-02-09T20:20:16,707 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:16,750 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:18,059 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,060 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,053 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:18,059 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,110 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,060 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,111 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,110 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,109 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:18,059 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:18,111 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,114 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:18,114 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:18,116 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:18,117 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:18,112 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,119 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:18,112 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,115 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,120 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:18,137 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:18,140 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:18,141 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,140 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:18,115 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,141 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:18,143 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,141 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,141 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:18,146 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,143 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:18,146 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,143 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,147 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:18,152 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,154 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:18,186 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:18,148 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:18,186 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:18,203 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,213 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,152 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:18,152 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,212 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:18,203 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,259 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:18,259 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,202 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:18,260 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:18,200 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:18,154 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:18,262 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:18,262 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:18,261 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:18,267 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,266 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:18,269 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:18,259 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,273 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,259 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:18,274 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:18,257 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:18,213 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,257 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:20:18,276 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,275 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:18,274 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:18,286 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T20:20:18,286 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T20:20:18,273 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,294 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,294 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,298 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,272 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:18,267 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,305 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,309 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,322 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:18,267 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:20:18,266 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:18,341 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:18,262 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:18,342 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T20:20:18,322 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:18,309 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,357 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,309 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:18,305 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,360 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,302 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:18,298 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,278 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:18,364 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:18,276 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,364 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,275 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:20:18,366 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:18,364 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:18,367 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:18,363 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:18,361 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:18,360 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,391 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,357 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,392 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,342 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T20:20:18,341 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:18,341 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:18,396 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:18,404 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,392 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,409 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,379 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:18,359 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:18,369 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:18,367 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:18,427 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T20:20:18,366 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:18,364 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,430 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,427 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T20:20:18,425 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:18,424 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:18,424 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:18,450 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:18,409 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,451 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,404 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,452 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,403 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:18,396 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:18,396 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:18,391 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,456 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,453 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:18,452 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,457 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,451 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,450 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:18,460 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:18,448 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:18,430 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,472 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,443 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:18,460 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:18,473 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:18,457 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,475 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,457 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:18,475 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,456 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,455 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:18,479 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,473 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:18,481 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:18,473 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:18,482 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T20:20:18,472 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,462 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:18,484 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:18,484 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:18,482 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T20:20:18,481 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:18,487 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:18,481 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:18,479 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:18,479 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,478 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:18,492 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:18,489 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:18,494 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:18,504 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,507 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:18,507 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:18,519 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:18,487 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:18,521 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T20:20:18,484 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:18,484 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:18,524 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:18,519 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:18,507 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:18,507 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:18,504 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:18,557 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,503 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:18,494 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:18,493 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:18,562 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:18,492 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:18,492 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:18,564 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:18,490 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:20:18,563 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:18,562 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:18,559 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:18,557 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:18,571 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,524 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:18,521 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T20:20:18,572 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T20:20:18,571 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:18,584 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,570 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:18,568 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:18,566 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:20:18,588 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:18,564 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:18,589 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T20:20:18,587 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:18,590 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:18,585 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:18,584 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:18,592 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,572 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T20:20:18,591 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:18,590 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:18,589 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T20:20:18,588 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:18,594 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:18,592 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:18,597 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:18,600 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:18,599 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:18,600 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:18,601 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:18,601 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:18,601 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:18,603 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T20:20:18,602 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:18,604 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:18,615 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:18,603 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T20:20:18,615 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:18,604 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:19,299 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:20:19,347 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:20:19,439 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:20:19,488 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:20:19,524 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:20:19,574 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:20:19,591 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:20:19,607 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:20:19,299 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:20:19,607 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:20:19,574 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:20:19,524 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:20:19,488 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:20:19,439 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:20:19,347 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:20:19,591 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:20:28,268 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:28,269 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - [PID]42156
2022-02-09T20:20:28,274 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,274 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:28,274 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,277 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:20:28,275 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:28,277 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:20:28,283 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:20:28,284 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628284
2022-02-09T20:20:28,284 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628284
2022-02-09T20:20:28,326 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:28,417 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:28,421 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - [PID]44508
2022-02-09T20:20:28,425 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,425 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:28,425 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,427 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:20:28,426 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:28,427 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:20:28,433 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628433
2022-02-09T20:20:28,433 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628433
2022-02-09T20:20:28,433 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:20:28,462 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:28,471 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:28,473 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - [PID]40256
2022-02-09T20:20:28,475 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,474 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:28,475 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:28,475 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,478 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:20:28,478 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:20:28,484 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628484
2022-02-09T20:20:28,483 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:20:28,484 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628484
2022-02-09T20:20:28,518 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:28,574 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:28,576 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - [PID]29680
2022-02-09T20:20:28,580 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,580 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:28,580 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,583 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:20:28,581 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:28,583 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:20:28,589 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628589
2022-02-09T20:20:28,589 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:20:28,589 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628589
2022-02-09T20:20:28,589 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:28,592 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - [PID]42832
2022-02-09T20:20:28,596 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,595 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:28,596 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,597 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:20:28,597 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:28,597 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:20:28,631 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:20:28,636 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628636
2022-02-09T20:20:28,632 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:28,631 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:28,636 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628636
2022-02-09T20:20:28,638 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - [PID]47796
2022-02-09T20:20:28,644 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,644 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:28,644 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:28,644 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,655 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:20:28,653 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:28,655 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:20:28,666 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:20:28,667 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628667
2022-02-09T20:20:28,667 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628667
2022-02-09T20:20:28,686 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:28,867 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:28,874 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - [PID]41356
2022-02-09T20:20:28,885 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,885 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:28,885 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,889 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:20:28,888 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:28,889 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:20:28,895 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628895
2022-02-09T20:20:28,895 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628895
2022-02-09T20:20:28,895 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:20:28,931 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:28,943 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:28,951 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - [PID]45612
2022-02-09T20:20:28,952 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,952 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:28,952 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:28,954 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:20:28,953 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:28,954 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:20:28,962 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628962
2022-02-09T20:20:28,962 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:20:28,962 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405628962
2022-02-09T20:20:29,008 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:29,874 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:29,873 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:29,874 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:29,883 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:29,877 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:29,884 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:29,883 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:29,887 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:29,885 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:29,887 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:29,891 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:29,889 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:29,891 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:29,894 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:29,892 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:29,894 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:29,895 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:29,898 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:29,898 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:29,898 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:29,912 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:29,911 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:29,913 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:29,912 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:29,916 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T20:20:29,916 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T20:20:29,917 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:20:29,918 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:29,918 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:29,939 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:29,939 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:30,056 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:30,056 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:30,056 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:30,067 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:30,062 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:30,068 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:30,067 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:30,071 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:30,069 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:30,072 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:30,074 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:30,071 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:30,078 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:30,076 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:30,078 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:30,080 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:30,079 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:30,081 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:30,082 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:30,080 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:30,085 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:20:30,086 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:30,086 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:20:30,088 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T20:20:30,089 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn = lambda ctx: entry_point(None, ctx)
2022-02-09T20:20:30,106 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\faf60ff7e0554ef6a5ebe8069c408960\handler.py", line 108, in handle
2022-02-09T20:20:30,107 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     raise e
2022-02-09T20:20:30,109 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:30,108 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\faf60ff7e0554ef6a5ebe8069c408960\handler.py", line 97, in handle
2022-02-09T20:20:30,109 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     _service.initialize(context)
2022-02-09T20:20:30,111 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\faf60ff7e0554ef6a5ebe8069c408960\handler.py", line 31, in initialize
2022-02-09T20:20:30,109 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:30,086 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:30,113 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:30,112 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)
2022-02-09T20:20:30,113 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:30,115 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T20:20:30,114 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\transformers\models\auto\auto_factory.py", line 419, in from_pretrained
2022-02-09T20:20:30,117 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:30,115 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T20:20:30,117 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:30,142 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:30,143 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:30,143 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:30,143 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:30,145 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:30,147 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:30,147 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:30,147 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:30,149 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:30,148 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:30,150 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:30,152 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:30,153 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:30,157 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:30,149 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:30,158 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:30,156 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:30,154 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:30,161 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:30,158 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:30,166 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:30,157 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:30,166 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:30,165 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:20:30,159 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:30,166 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:30,184 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:20:30,166 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:30,189 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:30,189 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:30,186 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:30,189 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:30,194 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:30,189 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:30,195 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:30,189 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T20:20:30,194 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:30,196 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:30,191 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:30,195 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn = lambda ctx: entry_point(None, ctx)
2022-02-09T20:20:30,195 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:30,212 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T20:20:30,197 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:30,196 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:30,216 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:30,212 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T20:20:30,210 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\faf60ff7e0554ef6a5ebe8069c408960\handler.py", line 108, in handle
2022-02-09T20:20:30,217 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:30,213 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:30,219 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:30,219 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:30,220 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:30,219 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:30,217 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:30,242 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:30,221 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:30,216 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:30,250 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:30,242 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:30,250 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:30,264 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T20:20:30,249 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:30,265 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:30,264 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T20:20:30,265 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:30,267 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:30,266 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:30,267 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:30,270 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:30,268 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:30,271 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:30,270 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:30,273 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:30,272 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:30,273 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:30,285 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:30,283 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:30,285 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:30,287 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:30,286 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:30,287 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:30,291 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:30,289 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:30,291 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:30,300 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:30,295 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:30,300 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:30,316 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T20:20:30,314 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:30,317 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:30,317 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:30,316 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T20:20:30,329 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:30,329 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:30,381 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:30,382 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:30,382 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:30,382 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:30,387 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:30,386 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:30,387 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:30,390 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:30,389 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:30,390 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:30,394 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:30,391 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:30,394 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:30,396 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:30,395 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:30,409 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:30,396 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:30,409 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:30,412 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:30,412 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:30,408 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:30,398 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:30,412 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:30,416 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:30,412 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:30,417 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:30,415 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:30,413 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:30,417 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:30,421 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:30,419 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:30,418 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:30,421 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:30,416 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:30,426 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:30,426 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T20:20:30,425 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:30,428 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:30,423 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:30,426 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T20:20:30,428 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:30,435 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:30,429 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:30,426 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:30,435 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:30,449 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:30,447 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:30,449 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:30,451 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:30,450 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:30,452 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:30,453 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:30,455 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:30,451 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:30,457 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T20:20:30,457 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T20:20:30,458 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:20:30,459 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:30,459 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:30,466 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:30,470 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:30,465 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:30,470 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:30,466 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:30,477 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:30,471 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:30,477 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:30,479 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:30,478 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:30,479 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:30,483 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:30,481 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:30,483 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:30,485 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:30,484 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:30,485 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:30,486 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:30,488 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:30,488 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:30,488 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:30,491 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:30,490 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:30,491 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:30,493 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T20:20:30,492 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:30,497 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:30,502 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:30,493 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T20:20:30,502 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:30,497 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:30,924 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:20:30,924 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:20:31,125 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:20:31,125 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:20:31,221 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:20:31,221 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:20:31,269 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:20:31,269 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:20:31,332 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:20:31,332 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:20:31,441 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:20:31,441 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:20:31,470 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:20:31,470 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:20:31,508 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:20:31,508 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:20:34,124 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:34,854 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:34,801 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:34,754 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:34,661 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:34,661 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:34,595 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:34,301 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:46,499 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - [PID]33476
2022-02-09T20:20:46,514 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,497 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - [PID]27076
2022-02-09T20:20:46,515 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,494 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - [PID]34044
2022-02-09T20:20:46,518 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,494 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - [PID]3408
2022-02-09T20:20:46,520 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,491 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - [PID]42428
2022-02-09T20:20:46,525 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,473 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - [PID]19020
2022-02-09T20:20:46,528 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,520 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,532 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:20:46,520 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:46,518 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,536 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:20:46,517 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:46,515 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,551 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:20:46,515 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:46,514 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,552 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:20:46,514 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:46,513 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - [PID]16072
2022-02-09T20:20:46,554 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,505 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - [PID]36852
2022-02-09T20:20:46,555 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,553 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:46,552 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:20:46,559 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646559
2022-02-09T20:20:46,551 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:46,551 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:20:46,549 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:46,563 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646563
2022-02-09T20:20:46,536 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:20:46,534 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:46,565 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646565
2022-02-09T20:20:46,532 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:20:46,568 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646568
2022-02-09T20:20:46,528 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,577 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:20:46,528 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:46,525 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,579 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:20:46,525 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:46,578 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:46,577 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:20:46,568 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:20:46,584 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646584
2022-02-09T20:20:46,568 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646568
2022-02-09T20:20:46,566 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:20:46,565 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646565
2022-02-09T20:20:46,563 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646563
2022-02-09T20:20:46,559 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646559
2022-02-09T20:20:46,559 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:20:46,555 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,592 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:20:46,592 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:20:46,555 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:46,554 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:46,595 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:20:46,595 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:20:46,597 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646597
2022-02-09T20:20:46,597 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646597
2022-02-09T20:20:46,599 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646599
2022-02-09T20:20:46,599 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646599
2022-02-09T20:20:46,554 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:46,594 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:46,591 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:46,628 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:20:46,588 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:46,563 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:20:46,585 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:46,584 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:20:46,584 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646584
2022-02-09T20:20:46,580 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:46,657 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:46,645 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:46,629 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:46,579 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:20:46,629 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:46,668 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:20:46,668 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646668
2022-02-09T20:20:46,668 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405646668
2022-02-09T20:20:46,670 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:46,666 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:20:46,675 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:47,851 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:47,852 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:47,851 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:47,852 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:47,858 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:47,857 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:47,858 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:47,861 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:47,861 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:47,860 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:47,861 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:47,864 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:47,860 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:47,862 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:47,865 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:47,861 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:47,867 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:47,864 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:47,871 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:47,874 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:47,874 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:47,874 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:47,876 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:47,878 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:47,874 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:47,874 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:47,895 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:47,874 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:47,898 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:47,893 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:47,895 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:47,898 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:47,896 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:47,902 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:47,903 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:47,901 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:20:47,910 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:47,903 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:47,916 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:47,902 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:47,902 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:47,924 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:47,916 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:47,925 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-02-09T20:20:47,910 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:47,927 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:47,910 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:47,927 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:47,903 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:20:47,929 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:47,929 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:47,927 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:47,943 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:47,925 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-02-09T20:20:47,950 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:47,954 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:47,924 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:47,955 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:47,921 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:47,954 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:47,958 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:47,953 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:47,950 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:47,943 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:47,962 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:47,927 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:47,927 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:47,962 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:47,965 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:47,958 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:47,958 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:47,967 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:47,956 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:47,955 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:47,989 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.
2022-02-09T20:20:47,967 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:47,991 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:47,966 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:47,965 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:47,964 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:47,995 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:47,993 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:47,991 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:47,998 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:47,989 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.
2022-02-09T20:20:47,970 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:48,000 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:47,998 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:47,996 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:48,016 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:48,016 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:48,015 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:47,995 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:48,018 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:48,018 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:48,021 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2022-02-09T20:20:47,995 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:48,022 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:48,017 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:48,031 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:48,016 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:48,031 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:48,041 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:48,044 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:48,016 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:48,054 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:48,000 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:48,044 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:48,030 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:48,030 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:48,022 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:48,021 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2022-02-09T20:20:48,059 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:48,058 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:48,041 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:48,072 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:48,054 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:48,074 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 2 seconds.
2022-02-09T20:20:48,062 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:48,075 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:48,072 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:48,074 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 2 seconds.
2022-02-09T20:20:48,075 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:48,072 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:48,084 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:48,077 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:48,084 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:48,086 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:48,085 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:48,086 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:48,088 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:48,090 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:48,090 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:48,090 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:48,093 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:48,091 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:48,093 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:48,108 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2022-02-09T20:20:48,109 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:48,094 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:48,110 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:48,109 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:48,112 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:48,108 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2022-02-09T20:20:48,115 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:48,117 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:48,108 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:48,112 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:48,123 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:48,110 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:48,122 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:48,116 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:48,115 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:48,131 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:48,117 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:48,134 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:48,123 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:48,136 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:48,132 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:48,134 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:48,136 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:48,140 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:48,137 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:48,134 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:48,142 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:48,152 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:48,140 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:48,138 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:48,159 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:48,152 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:48,160 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:48,151 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:48,142 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:48,164 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:48,141 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:48,162 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:48,160 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:48,167 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:48,159 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:48,168 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:48,159 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:48,167 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:48,171 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:48,166 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:48,165 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:48,181 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:48,164 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:48,190 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:48,172 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:48,171 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:48,191 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:48,169 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:48,168 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:48,193 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2022-02-09T20:20:48,191 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:48,190 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:48,195 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:48,190 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:48,181 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:48,198 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:48,174 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:48,195 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:48,199 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:48,195 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:48,193 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2022-02-09T20:20:48,192 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:48,202 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:48,200 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:48,199 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:48,204 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 2 seconds.
2022-02-09T20:20:48,211 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:48,212 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:48,199 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:48,198 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:48,221 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:48,212 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:48,211 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:48,204 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 2 seconds.
2022-02-09T20:20:48,203 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:48,225 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:48,202 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:48,221 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:48,227 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2022-02-09T20:20:48,214 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:48,228 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:48,225 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:48,227 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2022-02-09T20:20:48,228 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:49,942 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:20:49,942 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:20:50,005 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:20:50,005 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:20:50,037 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:20:50,037 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:20:50,084 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:20:50,084 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:20:50,120 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:20:50,120 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:20:50,208 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:20:50,209 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:20:50,208 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:20:50,209 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:20:50,248 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:20:50,248 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:20:53,429 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:53,431 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - [PID]38556
2022-02-09T20:20:53,441 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,441 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:53,441 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,443 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:20:53,442 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:53,443 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:20:53,448 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653448
2022-02-09T20:20:53,448 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:20:53,448 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653448
2022-02-09T20:20:53,452 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:53,493 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:53,501 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - [PID]11720
2022-02-09T20:20:53,502 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,502 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:53,502 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,505 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:20:53,503 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:53,505 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:20:53,508 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653508
2022-02-09T20:20:53,508 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653508
2022-02-09T20:20:53,509 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:20:53,510 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:53,524 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:53,525 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - [PID]39252
2022-02-09T20:20:53,526 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,526 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:53,527 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:53,526 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,531 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:20:53,527 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:53,529 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - [PID]39032
2022-02-09T20:20:53,533 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,531 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:20:53,533 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,536 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:20:53,536 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653536
2022-02-09T20:20:53,533 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:53,536 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653536
2022-02-09T20:20:53,536 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:20:53,539 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:53,537 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:53,536 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:20:53,543 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653543
2022-02-09T20:20:53,543 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653543
2022-02-09T20:20:53,543 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:20:53,556 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:53,651 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:53,652 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - [PID]8272
2022-02-09T20:20:53,657 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,656 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:53,657 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,659 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:20:53,657 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:53,659 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:20:53,667 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:20:53,668 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653667
2022-02-09T20:20:53,668 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653667
2022-02-09T20:20:53,681 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:53,717 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:53,718 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - [PID]37344
2022-02-09T20:20:53,724 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,724 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:53,724 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,727 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:20:53,725 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:53,727 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:20:53,731 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653731
2022-02-09T20:20:53,731 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:20:53,731 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653731
2022-02-09T20:20:53,734 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:53,744 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:53,746 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - [PID]34864
2022-02-09T20:20:53,748 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,747 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:53,748 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,757 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:20:53,749 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:53,757 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:20:53,762 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653762
2022-02-09T20:20:53,762 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653762
2022-02-09T20:20:53,762 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:20:53,764 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:53,854 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:20:53,856 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - [PID]9372
2022-02-09T20:20:53,860 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,860 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:20:53,860 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:20:53,861 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:20:53,864 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:20:53,864 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:20:53,868 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:20:53,869 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653869
2022-02-09T20:20:53,869 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405653869
2022-02-09T20:20:53,871 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:20:54,807 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:54,808 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:54,808 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:54,817 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:54,808 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:54,817 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:54,825 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:54,822 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:54,825 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:54,828 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:54,826 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:54,828 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:54,831 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:54,829 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:54,842 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:54,843 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:54,845 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:54,846 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:54,831 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:54,848 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:54,852 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:54,852 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:54,853 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:54,853 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:54,855 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.
2022-02-09T20:20:54,853 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:20:54,856 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:54,855 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.
2022-02-09T20:20:54,856 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:20:54,875 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:54,875 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:54,875 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:54,875 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:54,878 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:54,877 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:54,881 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:54,878 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:54,881 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:54,881 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:20:54,880 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:54,881 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:54,888 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:54,884 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:54,888 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:54,890 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:54,888 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:54,890 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:54,891 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:54,894 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:54,893 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:54,914 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:54,894 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:54,918 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:54,915 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:54,934 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:54,918 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:54,934 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-02-09T20:20:54,930 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:20:54,935 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:54,935 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:54,934 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:54,933 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:54,938 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:54,935 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:20:54,935 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:20:54,934 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-02-09T20:20:54,937 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:54,938 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:54,942 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:54,942 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:54,942 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:54,946 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:54,944 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:54,946 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:54,947 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:54,947 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:54,947 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:54,963 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:54,972 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:54,950 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:54,972 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:54,979 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:54,971 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:54,963 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:54,983 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:54,979 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:54,984 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:54,978 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:54,983 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:54,987 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 3 seconds.
2022-02-09T20:20:54,981 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:54,985 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:54,999 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:54,999 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:20:54,984 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:55,011 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:54,988 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:54,987 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 3 seconds.
2022-02-09T20:20:55,014 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:55,012 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:55,011 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:55,014 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:20:55,017 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:55,015 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:55,019 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:55,017 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:55,032 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:55,036 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:55,037 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:55,038 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:55,041 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:20:55,042 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:20:55,045 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:55,044 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T20:20:55,045 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:55,048 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:55,045 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn = lambda ctx: entry_point(None, ctx)
2022-02-09T20:20:55,048 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:55,052 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.
2022-02-09T20:20:55,049 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\faf60ff7e0554ef6a5ebe8069c408960\handler.py", line 108, in handle
2022-02-09T20:20:55,061 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:55,061 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:20:55,052 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.
2022-02-09T20:20:55,070 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:55,070 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:20:55,097 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:55,097 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:55,097 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:55,097 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:55,102 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:55,105 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:55,105 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:55,105 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:55,107 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:55,107 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:55,109 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:55,107 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:55,114 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:55,110 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:55,114 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:55,131 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:55,129 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:55,131 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:55,132 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:55,134 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:20:55,136 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:55,136 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:55,137 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:55,140 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:55,137 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:55,141 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 3 seconds.
2022-02-09T20:20:55,140 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:55,141 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:55,139 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:55,138 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:20:55,144 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:55,141 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:55,145 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:55,141 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 3 seconds.
2022-02-09T20:20:55,144 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:20:55,143 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:55,145 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:55,164 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:55,161 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:55,164 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:55,167 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:55,165 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:55,167 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:55,171 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:55,173 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:55,168 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:55,173 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:20:55,171 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:55,194 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:55,198 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:55,179 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:55,198 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:20:55,194 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:55,206 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.
2022-02-09T20:20:55,203 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:55,206 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:55,206 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.
2022-02-09T20:20:55,206 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:20:55,234 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:55,235 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:55,235 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:55,235 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:55,238 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:55,236 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:55,240 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:55,239 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:20:55,238 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:55,248 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:55,240 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:55,240 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:20:55,251 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:55,248 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:55,253 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:55,246 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:20:55,251 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:20:55,256 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:55,249 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:55,254 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:20:55,253 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:55,270 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:55,268 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:55,256 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:20:55,273 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:55,270 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:55,269 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:20:55,276 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:55,273 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:20:55,277 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:55,271 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:55,276 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:55,280 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:55,276 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:20:55,278 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:55,277 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:20:55,280 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:20:55,284 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:55,280 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:55,285 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 3 seconds.
2022-02-09T20:20:55,281 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:55,286 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:55,284 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:55,288 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:55,297 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:55,301 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:55,284 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:20:55,286 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:20:55,285 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 3 seconds.
2022-02-09T20:20:55,312 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:20:55,301 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:20:55,297 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:20:55,288 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:55,333 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 3 seconds.
2022-02-09T20:20:55,328 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:20:55,334 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:55,333 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 3 seconds.
2022-02-09T20:20:55,334 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:20:57,861 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:20:57,861 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:20:57,941 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:20:57,941 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:20:58,002 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:20:58,002 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:20:58,065 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:20:58,065 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:20:58,146 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:20:58,146 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:20:58,224 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:20:58,224 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:20:58,302 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:20:58,302 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:20:58,349 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:20:58,349 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:21:01,209 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:01,210 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - [PID]38840
2022-02-09T20:21:01,215 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:01,210 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:01,214 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:01,215 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:01,219 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:21:01,216 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - [PID]35320
2022-02-09T20:21:01,221 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:01,217 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:01,221 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:01,223 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:21:01,219 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:21:01,219 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:01,226 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405661226
2022-02-09T20:21:01,223 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:21:01,226 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405661226
2022-02-09T20:21:01,236 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405661236
2022-02-09T20:21:01,226 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:21:01,225 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:01,236 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405661236
2022-02-09T20:21:01,237 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:01,238 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:21:01,243 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:01,482 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:01,483 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - [PID]14096
2022-02-09T20:21:01,488 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:01,487 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:01,488 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:01,495 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:01,498 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:21:01,498 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:21:01,501 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405661501
2022-02-09T20:21:01,501 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:21:01,501 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405661501
2022-02-09T20:21:01,510 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:01,626 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:01,628 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - [PID]34424
2022-02-09T20:21:01,632 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:01,633 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:01,635 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:01,635 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:01,636 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:21:01,636 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:21:01,640 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405661640
2022-02-09T20:21:01,640 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405661640
2022-02-09T20:21:01,640 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:21:01,642 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:01,648 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:01,652 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - [PID]33888
2022-02-09T20:21:01,654 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:01,653 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:01,654 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:01,656 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:21:01,654 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:01,656 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:21:01,663 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405661663
2022-02-09T20:21:01,661 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:21:01,663 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405661663
2022-02-09T20:21:01,673 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:01,786 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:01,787 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - [PID]37644
2022-02-09T20:21:01,792 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:01,792 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:01,793 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:01,792 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:01,798 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:21:01,798 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:21:01,812 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405661812
2022-02-09T20:21:01,812 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:21:01,812 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405661812
2022-02-09T20:21:01,818 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:01,957 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:01,959 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - [PID]35192
2022-02-09T20:21:01,963 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:01,963 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:01,963 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:01,966 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:21:01,964 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:01,966 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:21:01,970 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405661970
2022-02-09T20:21:01,970 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405661970
2022-02-09T20:21:01,970 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:21:01,973 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:02,053 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:02,054 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - [PID]27340
2022-02-09T20:21:02,060 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:02,060 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:02,060 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:02,062 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:21:02,061 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:02,062 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:21:02,067 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405662067
2022-02-09T20:21:02,067 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405662067
2022-02-09T20:21:02,067 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:21:02,069 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:02,612 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:02,612 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:02,614 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:02,612 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:02,614 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:02,619 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:02,613 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:02,621 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:02,622 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:02,623 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:02,625 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:02,626 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:02,612 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:02,629 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:02,617 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:02,628 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:02,619 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:02,631 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:02,630 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:02,642 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:02,644 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:02,645 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:02,646 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:02,629 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:02,654 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:02,648 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:02,631 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:02,657 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:02,631 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:02,654 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:02,662 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:02,657 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:02,663 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:02,657 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:02,662 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:02,683 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:02,658 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:02,663 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:02,687 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:02,683 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:02,684 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:21:02,684 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:02,687 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:02,694 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:02,694 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:02,692 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:21:02,694 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:21:02,694 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:02,694 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:02,707 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-02-09T20:21:02,707 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:02,704 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:21:02,709 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:02,696 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T20:21:02,707 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:02,717 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.
2022-02-09T20:21:02,717 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.
2022-02-09T20:21:02,719 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:02,707 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-02-09T20:21:02,723 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:02,716 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn = lambda ctx: entry_point(None, ctx)
2022-02-09T20:21:02,739 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:02,709 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:02,723 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:02,719 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:02,739 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:02,857 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:02,858 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:02,858 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:02,858 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:02,865 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:02,863 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:02,865 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:02,865 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:02,868 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:02,867 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:02,868 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:02,872 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:02,870 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:02,873 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:02,875 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:02,872 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:02,878 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:02,876 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:02,878 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:02,878 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:02,883 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:21:02,898 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:02,898 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:21:02,899 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T20:21:02,898 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:02,902 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:02,901 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn = lambda ctx: entry_point(None, ctx)
2022-02-09T20:21:02,903 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\faf60ff7e0554ef6a5ebe8069c408960\handler.py", line 108, in handle
2022-02-09T20:21:02,904 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     raise e
2022-02-09T20:21:02,905 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\faf60ff7e0554ef6a5ebe8069c408960\handler.py", line 97, in handle
2022-02-09T20:21:02,902 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:02,908 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 5 seconds.
2022-02-09T20:21:02,906 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     _service.initialize(context)
2022-02-09T20:21:02,909 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:02,909 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:02,908 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 5 seconds.
2022-02-09T20:21:02,924 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:02,924 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:03,021 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:03,022 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:03,021 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:03,022 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:03,028 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:03,026 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:03,028 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:03,030 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:03,029 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:03,032 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:03,033 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:03,034 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:03,036 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:03,030 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:03,049 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:03,037 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:03,049 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:03,051 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:03,050 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:03,051 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:03,055 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:21:03,056 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:03,055 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:21:03,056 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:03,059 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:03,057 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T20:21:03,059 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:03,062 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.
2022-02-09T20:21:03,063 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:03,061 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:03,060 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn = lambda ctx: entry_point(None, ctx)
2022-02-09T20:21:03,065 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:03,063 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:03,066 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:03,065 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:03,064 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:03,062 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.
2022-02-09T20:21:03,066 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:03,070 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:03,080 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:03,068 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:03,080 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:03,070 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:03,107 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:03,093 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:03,107 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:03,114 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:03,112 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:03,114 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:03,115 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:03,118 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:03,117 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:03,118 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:03,120 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:21:03,119 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:03,120 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:21:03,123 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.
2022-02-09T20:21:03,122 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:03,125 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:21:03,123 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.
2022-02-09T20:21:03,125 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:21:03,141 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:03,141 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:03,196 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:03,198 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:03,197 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:03,198 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:03,203 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:03,201 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:03,203 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:03,205 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:03,204 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:03,205 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:03,208 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:03,206 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:03,208 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:03,211 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:03,210 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:03,211 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:03,222 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:03,224 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:03,224 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:03,224 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:03,227 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:03,226 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:03,227 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:03,229 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 5 seconds.
2022-02-09T20:21:03,228 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:03,230 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:03,229 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 5 seconds.
2022-02-09T20:21:03,230 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:03,241 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:03,241 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:03,279 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:03,280 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:03,280 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:03,280 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:03,285 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:03,284 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:03,285 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:03,288 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:03,287 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:03,288 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:03,292 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:03,289 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:03,292 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:03,300 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:03,297 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:03,300 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:03,309 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:03,304 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:03,309 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:03,323 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:03,320 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:03,323 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:03,326 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 5 seconds.
2022-02-09T20:21:03,324 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:03,327 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:03,326 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 5 seconds.
2022-02-09T20:21:03,327 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:03,331 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:03,331 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:03,375 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:03,376 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:03,376 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:03,376 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:03,381 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:03,380 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:03,381 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:03,383 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:03,382 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:03,383 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:03,386 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:03,384 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:03,386 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:03,389 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:03,387 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:03,389 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:03,390 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:03,393 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:03,392 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:03,393 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:03,394 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:03,393 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:03,394 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:03,396 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 5 seconds.
2022-02-09T20:21:03,395 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:03,397 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:03,396 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 5 seconds.
2022-02-09T20:21:03,406 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:03,397 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:03,406 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:06,719 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644405666
2022-02-09T20:21:06,909 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:17.634624481201172|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644405666
2022-02-09T20:21:06,929 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:215.21009826660156|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644405666
2022-02-09T20:21:06,933 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:92.4|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644405666
2022-02-09T20:21:06,937 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5644.234375|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644405666
2022-02-09T20:21:06,939 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:10533.796875|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644405666
2022-02-09T20:21:06,942 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:65.1|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644405666
2022-02-09T20:21:07,484 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('34424', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=35192)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 35192

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('35192',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('35192', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=27340)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 27340

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('27340',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('27340', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=37644)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 37644

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('37644',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('37644', 0)
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='cp949'>
OSError: [Errno 22] Invalid argument

2022-02-09T20:21:07,484 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('34424', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=35192)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 35192

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('35192',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('35192', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=27340)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 27340

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('27340',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('27340', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=37644)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 37644

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('37644',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('37644', 0)
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='cp949'>
OSError: [Errno 22] Invalid argument

2022-02-09T20:21:07,722 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:21:07,722 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:21:07,722 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:21:07,722 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:21:07,924 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:21:07,924 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:21:08,065 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:21:08,065 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:21:08,128 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:21:08,128 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:21:08,241 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:21:08,241 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:21:08,335 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:21:08,335 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:21:08,413 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:21:08,413 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:21:11,007 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:11,008 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - [PID]36568
2022-02-09T20:21:11,013 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:11,013 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:11,013 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:11,016 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:21:11,014 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:11,016 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:21:11,021 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405671021
2022-02-09T20:21:11,021 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:21:11,021 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405671021
2022-02-09T20:21:11,025 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:11,141 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:11,142 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - [PID]28648
2022-02-09T20:21:11,147 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:11,147 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:11,147 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:11,149 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:21:11,148 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:11,149 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:21:11,154 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405671154
2022-02-09T20:21:11,154 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405671154
2022-02-09T20:21:11,154 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:21:11,157 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:11,526 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:11,527 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - [PID]41116
2022-02-09T20:21:11,532 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:11,532 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:11,532 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:11,540 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:21:11,538 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:11,540 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:21:11,545 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405671545
2022-02-09T20:21:11,545 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:21:11,545 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405671545
2022-02-09T20:21:11,549 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:11,596 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:11,597 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - [PID]35888
2022-02-09T20:21:11,604 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:11,604 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:11,616 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:11,604 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:11,619 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:21:11,619 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:21:11,622 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405671622
2022-02-09T20:21:11,622 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:21:11,622 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405671622
2022-02-09T20:21:11,627 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:11,839 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:11,840 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - [PID]32080
2022-02-09T20:21:11,845 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:11,846 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:11,846 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:11,846 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:11,849 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:21:11,849 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:21:11,853 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:21:11,855 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405671855
2022-02-09T20:21:11,855 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405671855
2022-02-09T20:21:11,858 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:11,994 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:11,995 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - [PID]38196
2022-02-09T20:21:12,000 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:11,998 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:12,000 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:12,007 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:21:12,000 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:12,006 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - [PID]17472
2022-02-09T20:21:12,009 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:12,007 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:21:12,008 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:12,012 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405672012
2022-02-09T20:21:12,009 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:12,013 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:21:12,009 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:12,012 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:21:12,012 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405672012
2022-02-09T20:21:12,014 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:12,013 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:21:12,026 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:12,029 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405672029
2022-02-09T20:21:12,029 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:21:12,029 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405672029
2022-02-09T20:21:12,034 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:12,289 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:12,290 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - [PID]43904
2022-02-09T20:21:12,294 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:12,294 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:12,294 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:12,301 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:21:12,296 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:12,301 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:21:12,312 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405672312
2022-02-09T20:21:12,312 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:21:12,312 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405672312
2022-02-09T20:21:12,325 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:12,436 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:12,437 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:12,437 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:12,437 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:12,443 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:12,442 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:12,443 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:12,446 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:12,445 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:12,446 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:12,450 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:12,447 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:12,450 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:12,464 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:12,452 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:12,464 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:12,466 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:12,470 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:12,469 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:12,470 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:12,473 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:12,471 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:12,473 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:12,475 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-02-09T20:21:12,474 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:12,476 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:12,476 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:12,475 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-02-09T20:21:12,525 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:12,525 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:12,589 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:12,589 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:12,589 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:12,589 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:12,597 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:12,595 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:12,598 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:12,597 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:12,601 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:12,599 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:12,601 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:12,605 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:12,602 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:12,605 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:12,608 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:12,606 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:12,608 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:12,609 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:12,625 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:12,624 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:12,625 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:12,627 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:12,626 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:12,627 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:12,630 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.
2022-02-09T20:21:12,629 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:21:12,632 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:12,632 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:12,630 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.
2022-02-09T20:21:12,651 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:12,651 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:12,994 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:12,995 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:12,995 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:12,995 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:13,008 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:13,006 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:13,008 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:13,009 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:13,014 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:13,015 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:13,015 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:13,016 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:13,022 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:13,015 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:13,028 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:13,022 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:13,029 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:13,021 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:13,018 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:13,032 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:13,033 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:13,035 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:21:13,029 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:13,030 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:13,055 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:13,028 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:13,057 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:13,048 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:21:13,056 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:13,059 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:13,057 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:13,055 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:13,088 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:13,060 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:13,058 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T20:21:13,091 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:13,088 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:13,094 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:13,090 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:13,091 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn = lambda ctx: entry_point(None, ctx)
2022-02-09T20:21:13,091 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:13,099 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:13,095 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:13,094 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:13,116 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:13,099 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:13,097 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\faf60ff7e0554ef6a5ebe8069c408960\handler.py", line 108, in handle
2022-02-09T20:21:13,120 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:13,116 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:13,122 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:13,114 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:13,120 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:13,119 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.
2022-02-09T20:21:13,122 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:13,123 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:13,125 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:21:13,126 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:21:13,127 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T20:21:13,129 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn = lambda ctx: entry_point(None, ctx)
2022-02-09T20:21:13,151 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:13,122 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:13,169 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 8 seconds.
2022-02-09T20:21:13,119 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.
2022-02-09T20:21:13,151 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:13,172 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:13,140 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\faf60ff7e0554ef6a5ebe8069c408960\handler.py", line 108, in handle
2022-02-09T20:21:13,177 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:13,177 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:13,169 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 8 seconds.
2022-02-09T20:21:13,172 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:13,291 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:13,291 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:13,291 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:13,291 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:13,307 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:13,296 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:13,307 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:13,312 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:13,312 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:13,313 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:13,429 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:13,453 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:13,674 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:14,105 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:14,212 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:14,212 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:14,343 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:13,312 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:14,604 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:14,343 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:14,212 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:14,212 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:14,105 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:13,674 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:14,631 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:13,674 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:13,453 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:14,635 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:13,452 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:13,429 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:14,648 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:13,428 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:13,313 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:14,648 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:14,653 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:14,640 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:14,635 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:14,656 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:14,631 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:14,631 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:14,604 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:14,658 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:14,658 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:14,658 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:14,656 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:14,663 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:14,654 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:14,653 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:14,681 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:14,652 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:14,651 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:14,681 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:14,687 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:14,667 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:14,663 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:14,690 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:14,659 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:14,658 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:14,692 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:14,658 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:14,695 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:14,691 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:14,690 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:14,700 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:14,688 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:14,687 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:14,714 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:14,685 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:14,683 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:14,714 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:14,717 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:14,713 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:14,700 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:14,719 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:14,697 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:14,695 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:14,722 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:14,692 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:14,723 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:21:14,720 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:14,719 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:14,725 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 8 seconds.
2022-02-09T20:21:14,718 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:14,726 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:14,717 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:14,727 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 8 seconds.
2022-02-09T20:21:14,716 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:14,715 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:14,729 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:14,727 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 8 seconds.
2022-02-09T20:21:14,726 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:14,725 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 8 seconds.
2022-02-09T20:21:14,724 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:14,723 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:21:14,749 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.
2022-02-09T20:21:14,722 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:14,750 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:14,747 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:14,729 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:14,728 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:14,754 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:21:14,752 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:14,750 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:14,756 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:14,749 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.
2022-02-09T20:21:14,755 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:21:14,754 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:21:14,756 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:14,759 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 8 seconds.
2022-02-09T20:21:14,757 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:21:14,760 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:14,759 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 8 seconds.
2022-02-09T20:21:14,760 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:20,482 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:21:20,482 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:21:20,636 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:21:20,636 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:21:21,137 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:21:21,137 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:21:21,184 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:21:21,184 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:21:22,377 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:22,378 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - [PID]47320
2022-02-09T20:21:22,381 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:22,381 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:22,381 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:22,383 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:21:22,382 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:22,383 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:21:22,386 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405682386
2022-02-09T20:21:22,386 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405682386
2022-02-09T20:21:22,387 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:21:22,388 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:22,607 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:22,608 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - [PID]42588
2022-02-09T20:21:22,611 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:22,611 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:22,611 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:22,614 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:21:22,612 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:22,614 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:21:22,617 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405682617
2022-02-09T20:21:22,617 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405682617
2022-02-09T20:21:22,617 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:21:22,619 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:22,735 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:21:22,735 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:21:22,735 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:21:22,735 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:21:22,769 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:21:22,769 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:21:22,769 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:21:22,769 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:21:23,756 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:23,762 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - [PID]47388
2022-02-09T20:21:23,764 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:23,762 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:23,764 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:23,766 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:21:23,764 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:23,765 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - [PID]48076
2022-02-09T20:21:23,769 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:23,766 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:21:23,768 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:23,774 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405683773
2022-02-09T20:21:23,769 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:23,774 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405683773
2022-02-09T20:21:23,769 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:23,776 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:21:23,773 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:21:23,774 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:23,787 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:23,776 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:21:23,792 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405683792
2022-02-09T20:21:23,792 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405683792
2022-02-09T20:21:23,792 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:21:23,797 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:23,881 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:23,881 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:23,881 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:23,881 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:23,889 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:23,886 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:23,889 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:23,892 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:23,890 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:23,892 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:23,896 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:23,893 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:23,896 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:23,900 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:23,898 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:23,900 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:23,901 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:23,917 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:23,917 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:23,918 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:23,921 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:23,922 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:21:23,917 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:23,926 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:23,924 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:21:23,926 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:23,929 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-02-09T20:21:23,927 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T20:21:23,930 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:23,929 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-02-09T20:21:23,930 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:23,968 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:23,968 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:24,173 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:24,173 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:24,173 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:24,173 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:24,180 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:24,179 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:24,180 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:24,187 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:24,185 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:24,187 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:24,192 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:24,189 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:24,192 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:24,195 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:24,194 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:24,195 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:24,200 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:24,197 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:24,200 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:24,218 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:24,201 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:24,218 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:24,221 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.
2022-02-09T20:21:24,219 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:24,223 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:24,221 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.
2022-02-09T20:21:24,223 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:24,245 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:24,245 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:25,105 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:25,104 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:25,115 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:25,105 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:25,117 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:25,115 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:25,118 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:25,114 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:25,110 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:25,118 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:25,125 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:25,117 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:25,127 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:25,123 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:25,120 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:25,127 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:25,134 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:25,125 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:25,136 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:25,131 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:25,129 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:25,136 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:25,161 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:25,134 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:25,162 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:25,138 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:25,162 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:25,159 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:25,169 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:25,165 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:25,169 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:25,181 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:25,161 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:25,167 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:25,189 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:25,188 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:25,170 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:25,189 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:25,194 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:25,195 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:25,190 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:25,213 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:25,196 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:25,181 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:25,216 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 13 seconds.
2022-02-09T20:21:25,213 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:25,195 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:25,194 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:25,243 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.
2022-02-09T20:21:25,190 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:25,244 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:25,216 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 13 seconds.
2022-02-09T20:21:25,216 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:25,248 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:25,244 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:25,243 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.
2022-02-09T20:21:25,248 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:25,990 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:25,991 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - [PID]47836
2022-02-09T20:21:25,995 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:25,995 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:25,995 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:25,997 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:21:25,996 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:25,997 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:21:26,002 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405686002
2022-02-09T20:21:26,002 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405686002
2022-02-09T20:21:26,002 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:21:26,004 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:26,164 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:26,168 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - [PID]47900
2022-02-09T20:21:26,176 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:26,175 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:26,176 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:26,204 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:21:26,200 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:26,204 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:21:26,208 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405686208
2022-02-09T20:21:26,208 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405686208
2022-02-09T20:21:26,208 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:21:26,209 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:26,376 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:26,378 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - [PID]39228
2022-02-09T20:21:26,382 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:26,382 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:26,382 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:26,385 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:21:26,384 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:26,385 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:21:26,389 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405686389
2022-02-09T20:21:26,389 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405686389
2022-02-09T20:21:26,389 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:21:26,391 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:26,418 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:26,419 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - [PID]244
2022-02-09T20:21:26,421 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:26,421 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:26,421 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:26,422 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:26,423 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:21:26,423 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:21:26,426 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405686426
2022-02-09T20:21:26,426 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405686426
2022-02-09T20:21:26,426 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:21:26,433 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:27,019 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:27,019 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:27,019 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:27,019 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:27,025 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:27,024 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:27,025 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:27,028 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:27,027 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:27,028 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:27,032 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:27,030 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:27,032 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:27,035 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:27,033 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:27,035 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:27,036 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:27,038 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:27,038 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:27,038 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:27,041 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:27,040 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:27,041 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:27,056 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 13 seconds.
2022-02-09T20:21:27,056 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:27,042 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:27,066 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:27,056 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:27,056 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 13 seconds.
2022-02-09T20:21:27,066 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:27,156 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:27,157 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:27,156 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:27,157 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:27,162 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:27,161 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:27,162 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:27,165 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:27,163 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:27,165 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:27,168 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:27,166 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:27,168 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:27,182 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:27,170 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:27,182 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:27,184 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:27,187 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:27,187 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:27,187 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:27,189 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:27,188 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:27,189 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:27,191 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 13 seconds.
2022-02-09T20:21:27,190 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:27,192 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:27,191 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 13 seconds.
2022-02-09T20:21:27,192 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:27,203 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:27,203 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:27,273 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:27,274 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:27,274 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:27,274 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:27,290 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:27,288 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:27,290 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:27,292 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:27,291 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:27,292 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:27,297 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:27,293 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:27,309 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:27,309 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:27,297 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:27,318 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:27,309 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:27,320 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:27,309 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:27,318 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:27,314 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:27,323 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:27,321 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:27,320 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:27,332 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:27,323 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:27,334 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:21:27,323 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:27,332 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:27,335 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:27,324 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:27,334 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:27,334 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:21:27,336 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.
2022-02-09T20:21:27,335 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:27,338 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:21:27,335 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:27,338 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:27,338 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:27,336 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.
2022-02-09T20:21:27,336 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:27,338 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:27,338 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:27,338 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:21:27,352 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:27,349 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:27,352 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:27,354 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:27,366 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:27,353 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:27,366 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:27,354 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:27,375 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 13 seconds.
2022-02-09T20:21:27,372 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:27,376 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:27,375 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 13 seconds.
2022-02-09T20:21:27,376 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:36,932 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:21:36,932 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:21:37,230 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:21:37,230 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:21:38,222 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:21:38,222 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:21:38,254 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:21:38,254 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:21:38,604 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:38,605 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - [PID]46860
2022-02-09T20:21:38,610 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:38,610 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:38,610 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:38,613 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:21:38,611 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:38,613 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:21:38,617 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405698617
2022-02-09T20:21:38,617 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405698617
2022-02-09T20:21:38,617 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:21:38,619 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:39,054 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:39,055 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - [PID]47092
2022-02-09T20:21:39,056 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:39,055 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:39,056 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:39,056 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:39,056 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:21:39,056 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:21:39,058 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405699058
2022-02-09T20:21:39,058 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405699058
2022-02-09T20:21:39,058 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:21:39,060 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:39,371 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:39,371 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:39,371 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:39,371 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:39,372 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:39,372 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:39,372 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:39,374 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:39,373 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:39,374 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:39,377 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:39,375 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:39,377 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:39,380 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:39,378 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:39,380 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:39,381 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:39,383 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:39,383 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:39,383 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:39,386 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:39,385 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:39,386 [WARN ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:39,388 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-02-09T20:21:39,387 [INFO ] W-9000-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:39,389 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:39,388 [INFO ] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-02-09T20:21:39,389 [INFO ] W-9000-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stdout
2022-02-09T20:21:39,418 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:39,418 [INFO ] W-9000-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.0-stderr
2022-02-09T20:21:39,795 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:39,796 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:39,796 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:39,796 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:39,800 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:39,800 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:39,800 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:39,808 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:39,804 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:39,808 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:39,815 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:39,811 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:39,815 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:39,832 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:39,830 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:39,832 [DEBUG] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:39,833 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:39,835 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:39,835 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:39,835 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:39,838 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:39,837 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:39,838 [WARN ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:39,840 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.
2022-02-09T20:21:39,839 [INFO ] W-9001-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:39,841 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:39,840 [INFO ] W-9001-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.
2022-02-09T20:21:39,841 [INFO ] W-9001-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stdout
2022-02-09T20:21:39,854 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:40,070 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:21:40,197 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:21:40,340 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:21:40,376 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:21:39,854 [INFO ] W-9001-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.0-stderr
2022-02-09T20:21:40,376 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:21:40,340 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:21:40,197 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:21:40,180 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:40,096 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:40,070 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:21:40,772 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - [PID]3792
2022-02-09T20:21:40,770 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - [PID]41560
2022-02-09T20:21:40,775 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:40,775 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:40,775 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:40,776 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:21:40,775 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:40,775 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:40,780 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:21:40,775 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:40,777 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:40,776 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:21:40,789 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:40,794 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405700794
2022-02-09T20:21:40,780 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:21:40,797 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405700797
2022-02-09T20:21:40,794 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405700794
2022-02-09T20:21:40,794 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:21:40,805 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:40,798 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:21:40,797 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405700797
2022-02-09T20:21:40,816 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:41,889 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:41,889 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:41,889 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:41,894 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:41,893 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:41,889 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:41,903 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:41,894 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:41,904 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:41,894 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:41,903 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:41,906 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:41,901 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:41,905 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:41,904 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:41,910 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:41,908 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:41,906 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:41,922 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:41,910 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:41,924 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:41,909 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:41,922 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:41,926 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:41,911 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:41,925 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:41,924 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:41,930 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:41,928 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:41,926 [DEBUG] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:41,929 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:41,935 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:41,932 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:41,930 [DEBUG] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:41,935 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:41,938 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:41,938 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:41,935 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:41,937 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:41,938 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:41,942 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:41,938 [WARN ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:41,943 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 21 seconds.
2022-02-09T20:21:41,941 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:41,939 [INFO ] W-9007-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:41,978 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:41,942 [WARN ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:41,979 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.
2022-02-09T20:21:41,979 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:41,976 [INFO ] W-9002-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:41,943 [INFO ] W-9007-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 21 seconds.
2022-02-09T20:21:41,980 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:41,981 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:41,979 [INFO ] W-9007-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stderr
2022-02-09T20:21:41,979 [INFO ] W-9002-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.
2022-02-09T20:21:41,978 [INFO ] W-9007-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.0-stdout
2022-02-09T20:21:41,981 [INFO ] W-9002-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stderr
2022-02-09T20:21:41,980 [INFO ] W-9002-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.0-stdout
2022-02-09T20:21:43,036 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:43,037 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - [PID]38772
2022-02-09T20:21:43,041 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:43,041 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:43,041 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:43,042 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:43,043 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:21:43,043 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:21:43,046 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405703046
2022-02-09T20:21:43,046 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405703046
2022-02-09T20:21:43,046 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:21:43,048 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:43,063 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:43,066 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:43,067 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - [PID]35668
2022-02-09T20:21:43,068 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:43,067 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - [PID]22120
2022-02-09T20:21:43,068 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:43,068 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:43,069 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:21:43,068 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:43,068 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:43,070 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:21:43,068 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:43,069 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:21:43,070 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:43,072 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405703072
2022-02-09T20:21:43,070 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:21:43,070 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:43,072 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405703072
2022-02-09T20:21:43,075 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405703075
2022-02-09T20:21:43,074 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:21:43,075 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:21:43,075 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:43,075 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405703075
2022-02-09T20:21:43,088 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:43,120 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:21:43,121 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - [PID]43524
2022-02-09T20:21:43,122 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:43,122 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:21:43,122 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:21:43,125 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:21:43,123 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:21:43,125 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:21:43,134 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405703134
2022-02-09T20:21:43,134 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:21:43,134 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644405703134
2022-02-09T20:21:43,148 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:21:43,954 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:43,954 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:43,954 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:43,954 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:43,961 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:43,959 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:43,962 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:43,961 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:43,961 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:43,965 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:43,962 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:43,966 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:43,962 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:43,965 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:43,970 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:43,963 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:43,968 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:43,966 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:43,985 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:43,971 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:43,970 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:43,987 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:43,987 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:43,984 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:43,987 [DEBUG] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:43,985 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:43,992 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:43,990 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:43,993 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:43,989 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:43,992 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:43,996 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:43,993 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:43,998 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:43,993 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:43,996 [DEBUG] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:43,995 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:43,999 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:44,002 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:43,998 [WARN ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:44,004 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 21 seconds.
2022-02-09T20:21:44,018 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:44,024 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:44,026 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:44,001 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:44,002 [INFO ] W-9005-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:44,045 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:44,002 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:44,050 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:44,031 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:44,110 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:44,026 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:44,025 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:44,721 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:44,024 [INFO ] W-9004-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stderr
2022-02-09T20:21:44,004 [INFO ] W-9005-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 21 seconds.
2022-02-09T20:21:44,743 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:44,721 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:44,110 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:21:44,754 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:44,109 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:21:44,050 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:44,050 [WARN ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:44,757 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 21 seconds.
2022-02-09T20:21:44,045 [INFO ] W-9005-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stdout
2022-02-09T20:21:44,755 [INFO ] W-9004-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:44,769 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:44,754 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:21:44,769 [INFO ] W-9004-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.0-stdout
2022-02-09T20:21:44,754 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:44,773 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:44,744 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:44,743 [INFO ] W-9003-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:44,018 [INFO ] W-9005-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.0-stderr
2022-02-09T20:21:44,721 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:21:44,776 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:44,774 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:44,773 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:44,779 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:44,769 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:21:44,757 [INFO ] W-9004-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 21 seconds.
2022-02-09T20:21:44,779 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:44,782 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:44,777 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:44,776 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:21:44,786 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:44,782 [DEBUG] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:44,780 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:21:44,788 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:44,786 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:21:44,789 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:44,809 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:44,783 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:44,788 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:44,819 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:44,788 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:21:44,815 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:44,809 [INFO ] W-9006-sentencemoji_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stderr
2022-02-09T20:21:44,789 [DEBUG] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:21:44,834 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:44,830 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:21:44,823 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:21:44,819 [WARN ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:44,837 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 21 seconds.
2022-02-09T20:21:44,835 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:21:44,834 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stderr
2022-02-09T20:21:44,844 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:21:44,837 [INFO ] W-9006-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 21 seconds.
2022-02-09T20:21:44,836 [INFO ] W-9006-sentencemoji_1.0-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:21:44,846 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:44,844 [WARN ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.0-stdout
2022-02-09T20:21:44,847 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.
2022-02-09T20:21:44,843 [INFO ] W-9003-sentencemoji_1.0-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:21:44,848 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:21:44,846 [INFO ] W-9006-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.0-stdout
2022-02-09T20:21:44,847 [INFO ] W-9003-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.
2022-02-09T20:21:44,848 [INFO ] W-9003-sentencemoji_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.0-stdout
2022-02-09T20:22:00,405 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:22:00,405 [DEBUG] W-9000-sentencemoji_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:50:58,825 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-09T20:50:58,825 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-09T20:50:58,931 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-09T20:50:58,931 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-09T20:50:59,642 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages
Current directory: C:\Users\enoch9\Desktop\개발\sentencEmoji
Temp directory: C:\Users\enoch9\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4046 M
Python executable: c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe
Config file: logs\config\20220209202200696-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Initial Models: sentencemoji=sentencemoji.mar
Log dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Metrics dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Model config: N/A
2022-02-09T20:50:59,642 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages
Current directory: C:\Users\enoch9\Desktop\개발\sentencEmoji
Temp directory: C:\Users\enoch9\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4046 M
Python executable: c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe
Config file: logs\config\20220209202200696-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Initial Models: sentencemoji=sentencemoji.mar
Log dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Metrics dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Model config: N/A
2022-02-09T20:50:59,668 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220209202200696-shutdown.cfg",
  "modelCount": 1,
  "created": 1644405720697,
  "models": {
    "sentencemoji": {
      "1.0": {
        "defaultVersion": true,
        "marName": "sentencemoji.mar",
        "minWorkers": 8,
        "maxWorkers": 8,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-02-09T20:50:59,668 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220209202200696-shutdown.cfg",
  "modelCount": 1,
  "created": 1644405720697,
  "models": {
    "sentencemoji": {
      "1.0": {
        "defaultVersion": true,
        "marName": "sentencemoji.mar",
        "minWorkers": 8,
        "maxWorkers": 8,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-02-09T20:50:59,685 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220209202200696-shutdown.cfg
2022-02-09T20:50:59,685 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220209202200696-shutdown.cfg
2022-02-09T20:50:59,689 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220209202200696-shutdown.cfg validated successfully
2022-02-09T20:50:59,689 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220209202200696-shutdown.cfg validated successfully
2022-02-09T20:51:38,263 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.1 for model sentencemoji
2022-02-09T20:51:38,263 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.1 for model sentencemoji
2022-02-09T20:51:38,264 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T20:51:38,264 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T20:51:38,279 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T20:51:38,279 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T20:51:38,282 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model sentencemoji loaded.
2022-02-09T20:51:38,282 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model sentencemoji loaded.
2022-02-09T20:51:38,283 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: sentencemoji, count: 8
2022-02-09T20:51:38,283 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: sentencemoji, count: 8
2022-02-09T20:51:38,323 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-02-09T20:51:38,323 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-02-09T20:51:38,352 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:51:38,352 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:51:38,352 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:51:38,352 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:51:38,352 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:51:38,356 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:51:38,356 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:51:38,355 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:51:38,352 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:51:38,356 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:51:38,352 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:51:38,352 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:51:38,352 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:51:38,352 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:51:38,356 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:51:38,355 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:51:39,496 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-09T20:51:39,496 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-09T20:51:39,607 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-02-09T20:51:39,607 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-02-09T20:51:39,723 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-09T20:51:39,723 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-09T20:51:39,725 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-02-09T20:51:39,725 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-02-09T20:51:39,727 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-09T20:51:39,727 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-09T20:51:40,880 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-09T20:51:40,880 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-09T20:52:17,145 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:52:17,445 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:52:17,870 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:52:17,922 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:52:17,958 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:52:18,050 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:52:18,051 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:52:18,145 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:51:43,541 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407503
2022-02-09T20:52:18,145 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:52:53,849 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:52:18,050 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:52:17,922 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:52:53,852 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:52:53,852 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:52:17,870 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:52:53,855 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:52:17,445 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:52:53,856 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:52:53,855 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:52:53,857 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:51:46,471 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:51:46,327 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:51:46,327 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:51:46,209 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:51:46,124 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:51:46,107 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:51:45,825 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:51:45,727 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:52:53,858 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]45476
2022-02-09T20:52:53,935 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:52:53,856 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:52:17,145 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:52:53,957 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:52:53,958 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:52:53,852 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:52:53,979 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:52:17,958 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:52:53,849 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:52:53,984 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:52:53,984 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:52:53,808 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:16.11066436767578|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407503
2022-02-09T20:52:18,051 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:52:53,984 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:52:54,005 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:52:53,984 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:52:53,979 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:52:53,958 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:52:54,029 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:52:54,013 [ERROR] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:52:54,010 [ERROR] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:52:53,852 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:52:53,957 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:52:54,042 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:52:54,042 [ERROR] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:52:53,935 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:52:54,064 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:52:53,935 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:52:53,890 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]42376
2022-02-09T20:52:53,857 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:52:53,916 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]11164
2022-02-09T20:52:54,103 [ERROR] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:52:54,070 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:52:54,064 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:52:54,042 [ERROR] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:52:54,042 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:52:54,179 [ERROR] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:52:54,013 [ERROR] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:52:54,010 [ERROR] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:52:54,029 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:52:54,186 [ERROR] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:52:54,013 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]39580
2022-02-09T20:52:54,188 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:52:54,010 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]43956
2022-02-09T20:52:54,190 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:52:54,005 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:52:54,192 [ERROR] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:52:54,193 [ERROR] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Failed to connect to worker.
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:359) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: no further information: /127.0.0.1:9000
Caused by: java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.Net.pollConnect(Native Method) ~[?:?]
	at sun.nio.ch.Net.pollConnectNow(Net.java:672) ~[?:?]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946) ~[?:?]
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330) ~[model-server.jar:?]
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334) ~[model-server.jar:?]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702) ~[model-server.jar:?]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) ~[model-server.jar:?]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) ~[model-server.jar:?]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) ~[model-server.jar:?]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) ~[model-server.jar:?]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[model-server.jar:?]
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[model-server.jar:?]
	... 1 more
2022-02-09T20:52:53,986 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:216.73405838012695|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407503
2022-02-09T20:52:54,190 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:52:54,188 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:52:54,186 [ERROR] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:52:54,179 [ERROR] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:52:54,042 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]36972
2022-02-09T20:52:54,179 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]17200
2022-02-09T20:52:54,248 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:52:54,248 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:52:54,104 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker did not receive connection in: 30
2022-02-09T20:52:54,250 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:52:54,103 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:52:54,251 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:52:54,103 [ERROR] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:52:54,103 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]37848
2022-02-09T20:52:54,256 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:52:54,103 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:52:54,279 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:52:54,251 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:52:54,248 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:52:54,248 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:52:54,217 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:93.1|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407503
2022-02-09T20:52:54,192 [ERROR] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:139) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:52:54,193 [ERROR] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Failed to connect to worker.
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:359) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: no further information: /127.0.0.1:9000
Caused by: java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.Net.pollConnect(Native Method) ~[?:?]
	at sun.nio.ch.Net.pollConnectNow(Net.java:672) ~[?:?]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946) ~[?:?]
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330) ~[model-server.jar:?]
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334) ~[model-server.jar:?]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702) ~[model-server.jar:?]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650) ~[model-server.jar:?]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576) ~[model-server.jar:?]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) ~[model-server.jar:?]
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) ~[model-server.jar:?]
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[model-server.jar:?]
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[model-server.jar:?]
	... 1 more
2022-02-09T20:52:54,250 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:52:54,279 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:52:54,256 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:52:54,293 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:4431.63671875|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407503
2022-02-09T20:52:54,334 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:11746.39453125|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407503
2022-02-09T20:52:54,336 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:72.6|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407503
2022-02-09T20:52:55,116 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407575
2022-02-09T20:52:55,117 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:16.10934066772461|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407575
2022-02-09T20:52:55,126 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:216.73538208007812|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407575
2022-02-09T20:52:55,143 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:93.1|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407575
2022-02-09T20:52:55,151 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:4740.1640625|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407575
2022-02-09T20:52:55,172 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:11437.8671875|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407575
2022-02-09T20:52:55,180 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:70.7|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407575
2022-02-09T20:52:55,957 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=37848)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 37848

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('37848',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('37848', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=42376)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 42376

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('42376',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('42376', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=36972)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 36972

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('36972',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('36972', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=39580)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 39580

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('39580',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('39580', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=11164)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 11164

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('11164',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('11164', 0)
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='cp949'>
OSError: [Errno 22] Invalid argument

2022-02-09T20:52:55,957 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=37848)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 37848

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('37848',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('37848', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=42376)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 42376

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('42376',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('42376', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=36972)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 36972

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('36972',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('36972', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=39580)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 39580

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('39580',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('39580', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=11164)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 11164

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('11164',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('11164', 0)
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='cp949'>
OSError: [Errno 22] Invalid argument

2022-02-09T20:55:15,378 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-09T20:55:15,378 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-09T20:55:15,471 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-09T20:55:15,471 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-09T20:55:16,283 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages
Current directory: C:\Users\enoch9\Desktop\개발\sentencEmoji
Temp directory: C:\Users\enoch9\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4046 M
Python executable: c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe
Config file: logs\config\20220209205139729-startup.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Initial Models: sentencemoji=sentencemoji.mar
Log dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Metrics dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Model config: N/A
2022-02-09T20:55:16,283 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages
Current directory: C:\Users\enoch9\Desktop\개발\sentencEmoji
Temp directory: C:\Users\enoch9\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4046 M
Python executable: c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe
Config file: logs\config\20220209205139729-startup.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Initial Models: sentencemoji=sentencemoji.mar
Log dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Metrics dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Model config: N/A
2022-02-09T20:55:16,304 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220209205139729-startup.cfg",
  "modelCount": 1,
  "created": 1644407499730,
  "models": {
    "sentencemoji": {
      "1.1": {
        "defaultVersion": true,
        "marName": "sentencemoji.mar",
        "minWorkers": 8,
        "maxWorkers": 8,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-02-09T20:55:16,304 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220209205139729-startup.cfg",
  "modelCount": 1,
  "created": 1644407499730,
  "models": {
    "sentencemoji": {
      "1.1": {
        "defaultVersion": true,
        "marName": "sentencemoji.mar",
        "minWorkers": 8,
        "maxWorkers": 8,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-02-09T20:55:16,326 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220209205139729-startup.cfg
2022-02-09T20:55:16,326 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220209205139729-startup.cfg
2022-02-09T20:55:16,338 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220209205139729-startup.cfg validated successfully
2022-02-09T20:55:16,338 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220209205139729-startup.cfg validated successfully
2022-02-09T20:55:57,089 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.1 for model sentencemoji
2022-02-09T20:55:57,089 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.1 for model sentencemoji
2022-02-09T20:56:15,153 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T20:56:15,153 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T20:56:15,165 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T20:56:15,165 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T20:56:15,167 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model sentencemoji loaded.
2022-02-09T20:56:15,167 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model sentencemoji loaded.
2022-02-09T20:56:15,168 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: sentencemoji, count: 8
2022-02-09T20:56:15,168 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: sentencemoji, count: 8
2022-02-09T20:56:15,195 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:56:15,195 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:56:15,195 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:56:15,196 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:56:15,196 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:56:15,196 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:56:15,195 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:56:15,198 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:56:15,195 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:56:15,195 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:56:15,198 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:56:15,202 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:56:15,203 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-02-09T20:56:15,196 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:56:15,196 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:56:15,196 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:56:15,202 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:56:15,203 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-02-09T20:56:15,923 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-09T20:56:15,923 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-09T20:56:15,924 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-02-09T20:56:15,924 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-02-09T20:56:15,953 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-09T20:56:15,953 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-09T20:56:15,954 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-02-09T20:56:15,954 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-02-09T20:56:15,973 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-09T20:56:15,973 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-09T20:56:17,007 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-09T20:56:17,007 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-09T20:56:18,420 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407778
2022-02-09T20:56:18,422 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:15.576919555664062|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407778
2022-02-09T20:56:18,430 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:217.26780319213867|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407778
2022-02-09T20:56:18,447 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:93.3|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407778
2022-02-09T20:56:18,455 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:3888.1796875|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407778
2022-02-09T20:56:18,474 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:12289.8515625|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407778
2022-02-09T20:56:18,482 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:76.0|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407778
2022-02-09T20:56:21,818 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:21,821 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]43604
2022-02-09T20:56:21,828 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:21,829 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:21,830 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:21,829 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:21,863 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:56:21,863 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:56:21,897 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:56:21,909 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407781909
2022-02-09T20:56:21,909 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407781909
2022-02-09T20:56:22,017 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:22,074 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:22,076 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]35648
2022-02-09T20:56:22,083 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:22,082 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:22,083 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:22,119 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:56:22,097 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:22,119 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:56:22,153 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407782153
2022-02-09T20:56:22,153 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407782153
2022-02-09T20:56:22,153 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:56:22,183 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:22,246 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:22,264 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]32376
2022-02-09T20:56:22,265 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:22,265 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:22,271 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:22,265 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:22,281 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:56:22,281 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:56:22,302 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407782302
2022-02-09T20:56:22,302 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:56:22,302 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407782302
2022-02-09T20:56:22,356 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:22,379 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:22,381 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]39712
2022-02-09T20:56:22,395 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:22,394 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:22,395 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:22,429 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:56:22,419 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:22,429 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:56:22,471 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407782471
2022-02-09T20:56:22,471 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:56:22,471 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407782471
2022-02-09T20:56:22,494 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:22,497 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]2932
2022-02-09T20:56:22,498 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:22,498 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:22,498 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:22,552 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:56:22,532 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:22,524 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:22,552 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:56:22,618 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407782618
2022-02-09T20:56:22,618 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:56:22,618 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407782618
2022-02-09T20:56:22,729 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:22,820 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:22,822 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]34836
2022-02-09T20:56:22,831 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:22,831 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:22,831 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:22,853 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:56:22,847 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:22,853 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:56:22,885 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407782885
2022-02-09T20:56:22,885 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:56:22,885 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407782885
2022-02-09T20:56:22,923 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:22,948 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:22,951 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]28956
2022-02-09T20:56:22,961 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:22,961 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:22,961 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:22,968 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:22,989 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:56:22,969 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:22,989 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:56:22,989 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]46132
2022-02-09T20:56:22,998 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:23,001 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407783001
2022-02-09T20:56:22,998 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:23,001 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407783001
2022-02-09T20:56:23,001 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:56:22,998 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:56:23,032 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:56:23,017 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:23,032 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:56:23,063 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:23,084 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407783084
2022-02-09T20:56:23,084 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:56:23,084 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407783084
2022-02-09T20:56:23,121 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:25,535 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:25,539 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:25,537 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:25,539 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:25,566 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:25,551 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:25,566 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:25,589 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:25,599 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:25,591 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:25,618 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:56:25,591 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:25,626 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:56:25,664 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:25,661 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:56:25,664 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:25,695 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:25,673 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:56:25,702 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:56:25,695 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:25,724 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:56:25,728 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:56:25,728 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:56:25,730 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:56:25,728 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:56:25,730 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:56:25,734 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T20:56:25,731 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:56:25,756 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:56:25,756 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:56:25,762 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:25,766 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:56:25,734 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T20:56:25,766 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:56:25,762 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:25,761 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:25,790 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:25,790 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:25,803 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:25,790 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:25,803 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:25,826 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:25,823 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:25,826 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:25,856 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:25,842 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:25,856 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:25,868 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:56:25,868 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:56:25,869 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:56:25,869 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:56:25,886 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T20:56:25,862 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:25,887 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:56:25,886 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T20:56:25,887 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:56:25,941 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:25,942 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:25,943 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:25,967 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:56:25,942 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:25,974 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:25,967 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:56:25,958 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:25,974 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:26,039 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:26,057 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:26,039 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:26,057 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:26,067 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:26,056 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:26,039 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:26,099 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:26,066 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:26,067 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:26,133 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:26,099 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:26,158 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:26,087 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:26,133 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:26,164 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:26,177 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:26,117 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:56:26,160 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:26,158 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:26,224 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:56:26,177 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:26,231 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:26,176 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:26,164 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:26,259 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:26,260 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:56:26,231 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:26,275 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:26,224 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:56:26,279 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:56:26,204 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:56:26,204 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:26,279 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:56:26,335 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T20:56:26,275 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:26,338 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:26,260 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:56:26,259 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:26,360 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:56:26,251 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:26,389 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:26,338 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:26,335 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T20:56:26,396 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:26,397 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:26,408 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:56:26,301 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:26,286 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:56:26,455 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:56:26,408 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:56:26,397 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:26,484 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:26,396 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:26,495 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:56:26,396 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:26,395 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:26,360 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:56:26,523 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:56:26,509 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:26,538 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:56:26,495 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:56:26,564 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:56:26,484 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:26,565 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:26,455 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:56:26,429 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:56:26,565 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:26,610 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:26,564 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:56:26,627 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T20:56:26,538 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:56:26,525 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:56:26,652 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:56:26,523 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:56:26,669 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T20:56:26,512 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:26,652 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:56:26,627 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T20:56:26,699 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:26,610 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:26,726 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:26,592 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:56:26,726 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:56:26,699 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:26,728 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:26,697 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:26,740 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:26,760 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:56:26,693 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:26,669 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T20:56:26,760 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:56:26,740 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:26,794 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:26,739 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:26,739 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:26,728 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:26,835 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:26,726 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:56:26,726 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:26,865 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:56:26,835 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:26,884 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:26,830 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:26,805 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:26,902 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:56:26,907 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:56:26,794 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:26,921 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:26,762 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:26,907 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:56:26,901 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:26,885 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:26,884 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:26,993 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:26,865 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:56:27,010 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:56:26,991 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:26,964 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:26,902 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:56:26,934 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:26,921 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:27,064 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:27,037 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:56:27,021 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:27,010 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:56:27,105 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T20:56:26,993 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:27,116 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:56:27,091 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:56:27,077 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:56:27,064 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:27,170 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:56:27,042 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:56:27,172 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:56:27,176 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:27,132 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:56:27,116 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:56:27,190 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:56:27,105 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T20:56:27,182 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:56:27,130 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:56:27,176 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:27,172 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:56:27,212 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:56:27,170 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:56:27,208 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:56:27,207 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:56:27,190 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:56:27,251 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T20:56:27,239 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:56:27,253 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:56:27,285 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:56:27,212 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:56:27,286 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:56:27,251 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T20:56:27,242 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:56:27,286 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:56:27,326 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T20:56:27,285 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:56:27,344 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:56:27,320 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:56:27,354 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:56:27,253 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:56:27,326 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T20:56:27,354 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:56:27,344 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:56:27,654 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:56:27,654 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:56:27,680 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:56:27,680 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:56:28,122 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:56:28,122 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:56:28,282 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:56:28,282 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:56:28,350 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:56:28,350 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:56:33,605 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:33,607 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]35104
2022-02-09T20:56:33,629 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:33,629 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:33,629 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:33,633 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:56:33,631 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:33,633 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:56:33,674 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407793674
2022-02-09T20:56:33,674 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:56:33,674 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407793674
2022-02-09T20:56:33,726 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:34,410 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:34,412 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]36060
2022-02-09T20:56:34,431 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:34,430 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:34,431 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:34,434 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:56:34,432 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:34,434 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:56:34,453 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407794453
2022-02-09T20:56:34,453 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407794453
2022-02-09T20:56:34,453 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:56:34,502 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:34,532 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:34,534 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]32624
2022-02-09T20:56:34,542 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:34,542 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:34,542 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:34,568 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:56:34,548 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:34,568 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:56:34,601 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407794601
2022-02-09T20:56:34,601 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407794601
2022-02-09T20:56:34,601 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:56:34,642 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:34,701 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:34,703 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]43476
2022-02-09T20:56:34,711 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:34,711 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:34,728 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:34,711 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:34,757 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:56:34,757 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:56:34,785 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407794785
2022-02-09T20:56:34,784 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:56:34,785 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407794785
2022-02-09T20:56:34,846 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:34,898 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:34,900 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]44244
2022-02-09T20:56:34,906 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:34,906 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:34,907 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:34,906 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:34,948 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:56:34,923 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:34,932 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]27764
2022-02-09T20:56:34,951 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:34,948 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:56:34,951 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:34,966 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:56:34,967 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407794967
2022-02-09T20:56:34,950 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:34,967 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:56:34,967 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407794967
2022-02-09T20:56:34,966 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:56:34,980 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:35,004 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407795004
2022-02-09T20:56:35,003 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:35,004 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:56:35,004 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407795004
2022-02-09T20:56:35,065 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:35,674 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:35,799 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:56:36,605 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]22580
2022-02-09T20:56:36,644 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:36,644 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:36,644 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:36,644 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]44040
2022-02-09T20:56:36,650 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:36,650 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:56:36,647 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:36,650 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:56:36,650 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:56:36,650 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:56:36,674 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:56:36,676 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407796676
2022-02-09T20:56:36,672 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:56:36,676 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407796676
2022-02-09T20:56:36,676 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:56:36,674 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:56:36,743 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407796742
2022-02-09T20:56:36,741 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:36,743 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407796742
2022-02-09T20:56:36,742 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:56:36,815 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:56:37,075 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:37,076 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:37,076 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:37,076 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:37,101 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:37,099 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:37,101 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:37,126 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:37,109 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:37,126 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:37,151 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:37,134 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:37,151 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:37,182 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:37,168 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:56:37,182 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:37,209 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:56:37,192 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:56:37,209 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:56:37,217 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:56:37,211 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:56:37,217 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:56:37,237 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T20:56:37,235 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:56:37,237 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:56:37,237 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T20:56:37,237 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:56:37,249 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:56:37,249 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:56:37,926 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:37,927 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:37,927 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:37,927 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:37,954 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:37,937 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:37,954 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:37,958 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:37,956 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:37,978 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:37,958 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:37,978 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:37,989 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:37,990 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:37,977 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:37,969 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:37,990 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:38,033 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:37,989 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:38,050 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:38,020 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:56:38,005 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:38,072 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:38,050 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:38,080 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:38,033 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:38,072 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:38,083 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:38,084 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:56:38,071 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:38,062 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:38,060 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:56:38,134 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:56:38,104 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:38,084 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:56:38,155 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:56:38,083 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:38,165 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:38,080 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:38,171 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:38,171 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:38,189 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:56:38,155 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:56:38,195 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T20:56:38,142 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:38,134 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:56:38,218 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:56:38,126 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:56:38,243 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:56:38,260 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:56:38,117 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:38,218 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:56:38,211 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:38,195 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T20:56:38,189 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:56:38,307 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:56:38,165 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:38,329 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:38,280 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:38,270 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:38,260 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:56:38,243 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:56:38,361 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:56:38,344 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:56:38,329 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:38,397 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:38,307 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:56:38,406 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T20:56:38,395 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:56:38,375 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:56:38,428 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:56:38,397 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:38,432 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:56:38,406 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T20:56:38,426 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:56:38,432 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:56:38,461 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:56:38,428 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:56:38,442 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:56:38,461 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:56:38,488 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T20:56:38,488 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:56:38,492 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:56:38,494 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:56:38,488 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T20:56:38,494 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:56:38,526 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:38,492 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:56:38,526 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:38,568 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:38,525 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:38,568 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:38,597 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:38,577 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:38,597 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:38,627 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:38,608 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:38,627 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:38,655 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:38,640 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:38,655 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:38,658 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:38,660 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:56:38,660 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:56:38,672 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:56:38,661 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:56:38,672 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:56:38,692 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T20:56:38,697 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:56:38,689 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:56:38,701 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:56:38,697 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:56:38,692 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T20:56:38,701 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:56:38,788 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:38,789 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:38,790 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:38,789 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:38,820 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:38,804 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:38,820 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:38,847 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:38,831 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:38,847 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:38,874 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:38,871 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:38,874 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:38,878 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:38,876 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:56:38,878 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:38,933 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:56:38,937 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:56:38,937 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:56:38,937 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:56:38,942 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:56:38,939 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:56:38,942 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:56:38,970 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T20:56:38,970 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T20:56:38,981 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:56:38,971 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:56:39,002 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:56:38,981 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:56:39,002 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:56:39,214 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:56:39,416 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:56:39,499 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:56:39,584 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:39,214 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:56:39,584 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:39,639 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:39,583 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:39,499 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:56:39,416 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:56:39,640 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:39,639 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:39,674 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:39,659 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:39,674 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:39,679 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:39,676 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:39,709 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:56:39,679 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:39,735 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:39,709 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:56:39,681 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:39,736 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:56:39,752 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:56:39,767 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:56:39,782 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:56:39,735 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:39,872 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:56:39,872 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:56:39,873 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:56:39,873 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:56:39,881 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T20:56:39,881 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T20:56:39,873 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:56:39,932 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:56:39,932 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:56:39,976 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:56:39,976 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:56:40,006 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:56:40,006 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:56:40,352 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\190ca15d306c44e58fa9b9642fa4c902', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:56:40,353 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:40,353 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\190ca15d306c44e58fa9b9642fa4c902
2022-02-09T20:56:40,353 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:56:40,377 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:40,362 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:56:40,377 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:56:40,402 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:40,384 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:56:40,402 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:56:40,410 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:56:40,411 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:40,411 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:56:40,411 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:56:40,427 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:40,425 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:56:40,427 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:56:40,429 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:56:40,432 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:56:40,431 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:56:40,432 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:56:40,450 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:56:40,464 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:56:40,450 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:56:40,464 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:56:40,450 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:56:40,536 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T20:56:40,466 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:56:40,537 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:56:40,536 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T20:56:40,537 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:56:40,904 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:56:40,904 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:56:48,576 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-09T20:56:48,576 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-09T20:56:48,796 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-09T20:56:48,796 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-09T20:56:50,231 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages
Current directory: C:\Users\enoch9\Desktop\개발\sentencEmoji
Temp directory: C:\Users\enoch9\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4046 M
Python executable: c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe
Config file: logs\config\20220209205641426-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Initial Models: sentencemoji=sentencemoji.mar
Log dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Metrics dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Model config: N/A
2022-02-09T20:56:50,231 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages
Current directory: C:\Users\enoch9\Desktop\개발\sentencEmoji
Temp directory: C:\Users\enoch9\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4046 M
Python executable: c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe
Config file: logs\config\20220209205641426-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Initial Models: sentencemoji=sentencemoji.mar
Log dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Metrics dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Model config: N/A
2022-02-09T20:56:50,285 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220209205641426-shutdown.cfg",
  "modelCount": 1,
  "created": 1644407801426,
  "models": {
    "sentencemoji": {
      "1.1": {
        "defaultVersion": true,
        "marName": "sentencemoji.mar",
        "minWorkers": 8,
        "maxWorkers": 8,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-02-09T20:56:50,285 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220209205641426-shutdown.cfg",
  "modelCount": 1,
  "created": 1644407801426,
  "models": {
    "sentencemoji": {
      "1.1": {
        "defaultVersion": true,
        "marName": "sentencemoji.mar",
        "minWorkers": 8,
        "maxWorkers": 8,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-02-09T20:56:50,314 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220209205641426-shutdown.cfg
2022-02-09T20:56:50,314 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220209205641426-shutdown.cfg
2022-02-09T20:56:50,322 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220209205641426-shutdown.cfg validated successfully
2022-02-09T20:56:50,322 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220209205641426-shutdown.cfg validated successfully
2022-02-09T20:57:46,387 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.1 for model sentencemoji
2022-02-09T20:57:46,387 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.1 for model sentencemoji
2022-02-09T20:57:46,410 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T20:57:46,410 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T20:57:46,415 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T20:57:46,415 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T20:57:46,419 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model sentencemoji loaded.
2022-02-09T20:57:46,419 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model sentencemoji loaded.
2022-02-09T20:57:46,424 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: sentencemoji, count: 8
2022-02-09T20:57:46,424 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: sentencemoji, count: 8
2022-02-09T20:57:46,477 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:57:46,478 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:57:46,480 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:57:46,484 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-02-09T20:57:46,484 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:57:46,477 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:57:46,484 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:57:46,484 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-02-09T20:57:46,535 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:57:46,534 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:57:46,480 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:57:46,478 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:57:46,565 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:57:46,534 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:57:46,582 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:57:46,535 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:57:46,565 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:57:46,582 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:57:47,152 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-09T20:57:47,152 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-09T20:57:47,162 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-02-09T20:57:47,162 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-02-09T20:57:47,187 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-09T20:57:47,187 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-09T20:57:47,191 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-02-09T20:57:47,191 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-02-09T20:57:47,205 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-09T20:57:47,205 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-09T20:57:48,139 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-09T20:57:48,139 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-09T20:57:51,949 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407871
2022-02-09T20:57:51,957 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:15.578681945800781|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407871
2022-02-09T20:57:51,965 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:217.26604080200195|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407871
2022-02-09T20:57:51,981 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:93.3|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407871
2022-02-09T20:57:51,983 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5025.046875|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407871
2022-02-09T20:57:51,994 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:11153.1171875|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407871
2022-02-09T20:57:52,009 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:68.9|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407871
2022-02-09T20:57:54,118 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:57:54,124 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]46580
2022-02-09T20:57:54,142 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:57:54,144 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:54,143 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:57:54,144 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:54,155 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:57:54,155 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:57:54,186 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:57:54,195 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407874195
2022-02-09T20:57:54,195 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407874195
2022-02-09T20:57:54,279 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:57:55,182 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:57:55,195 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]13288
2022-02-09T20:57:55,202 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:55,202 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:57:55,202 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:55,226 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:57:55,218 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:57:55,226 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:57:55,247 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407875247
2022-02-09T20:57:55,247 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407875247
2022-02-09T20:57:55,247 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:57:55,293 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:57:55,363 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:57:55,389 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:57:55,400 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]43684
2022-02-09T20:57:55,429 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:55,411 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]48060
2022-02-09T20:57:55,445 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:55,429 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:57:55,429 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:55,449 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:57:55,445 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:55,452 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:57:55,445 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:57:55,449 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:57:55,516 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:57:55,447 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:57:55,452 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:57:55,526 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407875526
2022-02-09T20:57:55,526 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:57:55,545 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407875545
2022-02-09T20:57:55,526 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407875526
2022-02-09T20:57:55,545 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407875545
2022-02-09T20:57:55,545 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:57:55,583 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:57:55,585 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:57:55,914 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:57:55,921 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]47532
2022-02-09T20:57:55,945 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:55,945 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:57:55,945 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:55,962 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:57:55,951 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:57:55,962 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:57:56,018 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407876018
2022-02-09T20:57:56,018 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:57:56,018 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407876018
2022-02-09T20:57:56,052 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:57:56,565 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:57:56,583 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]42600
2022-02-09T20:57:56,591 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:56,591 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:57:56,591 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:56,612 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:57:56,592 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:57:56,612 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:57:56,650 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407876650
2022-02-09T20:57:56,650 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407876650
2022-02-09T20:57:56,651 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:57:56,688 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:57:57,501 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:57:57,511 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]43224
2022-02-09T20:57:57,519 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:57,519 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:57:57,520 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:57:57,519 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:57,539 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:57:57,539 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:57:57,549 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407877549
2022-02-09T20:57:57,549 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407877549
2022-02-09T20:57:57,549 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:57:57,594 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:57:57,921 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:57:57,917 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:57:57,921 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:57:57,930 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:57:57,928 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:57:57,931 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:57:57,930 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:57:57,933 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:57:57,942 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:57:57,944 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:57:57,945 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:57:57,947 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:57:57,937 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:57:57,949 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:57:57,937 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:57:57,972 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:57:57,970 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:57:57,972 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:57:57,974 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:57:57,973 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:57:57,974 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:57:57,983 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:57:57,976 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:57:57,997 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:57:57,983 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:57:58,024 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:57:58,015 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:57:58,024 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:57:58,043 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:57:58,057 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T20:57:58,048 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T20:57:58,065 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:57:58,068 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:57:58,056 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]42336
2022-02-09T20:57:58,093 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:58,068 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:57:58,057 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T20:57:58,065 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:57:58,093 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T20:57:58,192 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:57:58,093 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:57:58,192 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:57:58,215 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:57:58,257 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407878257
2022-02-09T20:57:58,257 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:57:58,257 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407878257
2022-02-09T20:57:58,357 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:57:58,549 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:57:58,551 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:57:58,551 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:57:58,551 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:57:58,564 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:57:58,560 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:57:58,566 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:57:58,564 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:57:58,572 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:57:58,569 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:57:58,572 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:57:58,579 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:57:58,576 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:57:58,579 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:57:58,630 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:57:58,580 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:57:58,630 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:57:58,638 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:57:58,633 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:57:58,638 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:57:58,645 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:57:58,643 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:57:58,645 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:57:58,649 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T20:57:58,649 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T20:57:58,649 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:57:58,652 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:57:58,652 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:57:58,694 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:57:58,694 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:57:58,992 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:57:58,991 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:57:58,992 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:57:59,007 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:57:58,995 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:57:59,007 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:57:59,025 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:57:59,010 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:57:59,063 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:57:59,025 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:57:59,084 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:57:59,063 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:57:59,050 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:57:59,084 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:57:59,120 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:57:59,160 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:57:59,120 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:57:59,173 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:57:59,160 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:57:59,196 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:57:59,158 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:57:59,120 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:57:59,196 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:57:59,204 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:57:59,173 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:57:59,206 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:57:59,203 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:57:59,198 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:57:59,209 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:57:59,206 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:57:59,213 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T20:57:59,204 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:57:59,216 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:57:59,212 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:57:59,208 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:57:59,220 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:57:59,216 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:57:59,221 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:57:59,213 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T20:57:59,218 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:57:59,220 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:57:59,221 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:57:59,225 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:57:59,228 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:57:59,231 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:57:59,230 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:57:59,231 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:57:59,234 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:57:59,232 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:57:59,234 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:57:59,238 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T20:57:59,237 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:57:59,240 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:57:59,238 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T20:57:59,240 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:57:59,277 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:57:59,277 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:57:59,372 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:57:59,372 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:57:59,432 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:57:59,431 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:57:59,432 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:57:59,448 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:57:59,439 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:57:59,464 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:57:59,465 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:57:59,448 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:57:59,477 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:57:59,467 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:57:59,477 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:57:59,495 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:57:59,479 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:57:59,497 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:57:59,499 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:57:59,495 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:57:59,504 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:57:59,501 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:57:59,504 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:57:59,529 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:57:59,511 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:57:59,541 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:57:59,543 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:57:59,529 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:57:59,553 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:57:59,545 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:57:59,553 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:57:59,584 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T20:57:59,584 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T20:57:59,589 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:57:59,582 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:57:59,598 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:57:59,589 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:57:59,598 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:57:59,653 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:57:59,653 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:58:00,153 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:00,152 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:00,153 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:00,161 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:00,158 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:00,164 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:00,161 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:00,168 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:00,166 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:00,168 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:00,173 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:00,169 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:00,175 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:00,173 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:00,178 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:00,176 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:00,178 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:00,185 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:00,180 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:00,185 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:00,211 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:00,202 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:00,228 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:00,238 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:58:00,211 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:00,248 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T20:58:00,238 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:58:00,251 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:58:00,228 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:00,220 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:58:00,294 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:00,251 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:58:00,248 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T20:58:00,294 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:00,594 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:58:00,594 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:58:00,759 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:00,758 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:00,759 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:00,783 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:00,759 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:00,783 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:00,787 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:00,784 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:00,787 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:00,790 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:00,789 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:00,790 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:00,826 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:00,808 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:00,826 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:00,855 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:00,850 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:00,855 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:00,888 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:00,863 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:00,919 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:00,888 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:00,925 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T20:58:00,919 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:00,896 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:00,956 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:00,925 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T20:58:00,956 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:01,153 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:01,152 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:01,153 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:01,163 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:01,154 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:01,163 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:01,190 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:01,183 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:01,190 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:01,211 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:01,207 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:01,211 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:01,228 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:01,213 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:01,228 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:01,258 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:01,261 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:58:01,251 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:01,261 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:58:01,289 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:01,258 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:01,304 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:01,280 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:01,289 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:01,304 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:01,336 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T20:58:01,318 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:01,350 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:01,336 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T20:58:01,350 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:01,936 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:58:01,936 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:58:02,348 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:58:02,348 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:58:04,857 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:04,858 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]27712
2022-02-09T20:58:04,903 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:04,903 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:04,903 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:04,910 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:58:04,908 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:04,910 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:58:04,935 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407884935
2022-02-09T20:58:04,935 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:58:04,935 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407884935
2022-02-09T20:58:05,003 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:06,736 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:06,738 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]33476
2022-02-09T20:58:06,755 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:06,754 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:06,755 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:06,758 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:58:06,756 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:06,758 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:58:06,797 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:58:06,798 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407886798
2022-02-09T20:58:06,798 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407886798
2022-02-09T20:58:06,853 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:07,221 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:07,224 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]33772
2022-02-09T20:58:07,232 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:07,231 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:07,232 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:07,255 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:58:07,247 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:07,255 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:58:07,277 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407887277
2022-02-09T20:58:07,277 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:58:07,277 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407887277
2022-02-09T20:58:07,325 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:07,371 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:07,373 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]32948
2022-02-09T20:58:07,374 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:07,374 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:07,374 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:07,377 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:58:07,376 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:07,377 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:58:07,412 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:07,411 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:07,412 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:07,423 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:07,479 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:07,479 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407887479
2022-02-09T20:58:07,477 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:07,479 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:58:07,479 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407887479
2022-02-09T20:58:07,479 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:07,533 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:07,533 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:07,556 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:07,478 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]24724
2022-02-09T20:58:07,562 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:07,529 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:07,483 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:07,562 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:07,568 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:58:07,568 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:58:07,561 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:07,556 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:07,574 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:07,577 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407887577
2022-02-09T20:58:07,566 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:07,571 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:07,577 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407887577
2022-02-09T20:58:07,574 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:07,626 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:07,610 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:58:07,579 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:07,626 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:07,671 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:07,694 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:07,634 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:07,662 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:07,694 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:07,676 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:07,671 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:07,747 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T20:58:07,729 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:07,789 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:07,732 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]33640
2022-02-09T20:58:07,814 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:07,747 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T20:58:07,789 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:07,814 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:07,861 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:58:07,813 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:07,861 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:58:07,896 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:07,940 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407887940
2022-02-09T20:58:07,940 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:58:07,940 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407887940
2022-02-09T20:58:08,001 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:08,476 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:08,495 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]19812
2022-02-09T20:58:08,502 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:08,502 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:08,518 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:08,502 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:08,541 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:58:08,541 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:58:08,547 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407888547
2022-02-09T20:58:08,547 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:58:08,547 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407888547
2022-02-09T20:58:08,593 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:08,754 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:58:08,754 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:58:08,969 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:08,971 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]19356
2022-02-09T20:58:08,981 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:08,980 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:08,981 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:09,002 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:58:08,992 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:09,002 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:58:09,041 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407889041
2022-02-09T20:58:09,041 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407889041
2022-02-09T20:58:09,041 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:58:09,084 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:09,888 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:09,887 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:09,888 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:09,892 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:09,891 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:09,892 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:09,904 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:09,902 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:09,904 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:09,929 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:09,919 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:09,929 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:09,949 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:09,944 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:09,966 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:09,979 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:09,993 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:09,949 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:09,995 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:10,013 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:10,013 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:10,018 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:10,016 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:58:10,018 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:10,023 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T20:58:10,021 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:58:10,024 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:10,023 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T20:58:10,024 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:10,085 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:10,085 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:10,502 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:10,503 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:10,503 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:10,503 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:10,524 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:10,515 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:10,524 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:10,540 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:10,568 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:10,569 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:10,568 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:10,570 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:10,628 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:10,628 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:10,628 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:10,631 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:10,630 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:10,631 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:10,633 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:10,636 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:10,636 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:10,638 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:10,637 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:58:10,638 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:10,642 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T20:58:10,641 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:58:10,644 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:10,642 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T20:58:10,644 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:10,681 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:10,682 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:10,680 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:10,682 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:10,681 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:10,700 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:10,688 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:10,700 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:10,732 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:10,724 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:10,732 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:10,765 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:10,755 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:10,780 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:10,765 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:10,800 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:10,783 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:10,805 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:10,802 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:10,805 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:10,827 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:10,804 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:10,800 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:10,825 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:10,828 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:10,831 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:10,827 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:10,833 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:10,830 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:10,887 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:10,832 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:10,831 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:10,898 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:10,889 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:58:10,887 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:10,833 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:10,903 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:10,898 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:10,920 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T20:58:10,896 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:10,903 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:10,988 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:10,900 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:58:10,988 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:10,941 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:10,920 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T20:58:10,988 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:10,988 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:10,991 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:11,000 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:11,000 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:11,001 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:11,000 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:11,001 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:11,005 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T20:58:11,003 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:11,006 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:11,005 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T20:58:11,006 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:11,032 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:58:11,032 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:58:11,073 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:11,073 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:11,149 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:11,150 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:11,150 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:11,150 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:11,173 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:11,158 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:11,173 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:11,194 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:11,193 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:11,194 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:11,199 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:11,196 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:11,199 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:11,202 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:11,201 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:11,202 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:11,204 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:11,208 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:11,208 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:11,208 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:11,230 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:11,211 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:11,230 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:11,244 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T20:58:11,233 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:58:11,246 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:11,246 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:11,244 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T20:58:11,274 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:11,274 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:11,336 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:11,335 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:11,336 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:11,359 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:11,344 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:11,359 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:11,385 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:11,373 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:11,385 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:11,405 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:11,389 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:11,405 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:11,438 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:11,432 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:11,438 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:11,467 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:11,439 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:11,473 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:11,467 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:11,491 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:11,491 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:11,491 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:11,497 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:11,497 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T20:58:11,494 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:11,504 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:11,497 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:11,497 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T20:58:11,504 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:11,655 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:58:11,655 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:58:11,764 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:11,763 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:11,764 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:11,794 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:11,771 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:11,794 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:11,797 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:11,795 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:11,797 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:11,801 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:11,799 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:11,801 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:11,860 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:11,836 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:11,864 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:11,860 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:11,902 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:11,874 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:11,902 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:11,931 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:11,931 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:58:11,937 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:11,909 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:11,937 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:11,931 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:58:11,931 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:12,007 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T20:58:12,011 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:58:11,938 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:12,025 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:12,011 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:58:12,007 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T20:58:12,025 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:12,258 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:58:12,258 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:58:12,522 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:58:12,522 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:58:13,017 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:58:13,017 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:58:13,791 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:13,820 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]17576
2022-02-09T20:58:13,840 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:13,840 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:13,841 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:58:13,841 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:58:13,845 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407893845
2022-02-09T20:58:13,840 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:13,845 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407893845
2022-02-09T20:58:13,868 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:13,882 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:58:13,894 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:15,936 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:15,966 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:15,939 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]38824
2022-02-09T20:58:15,967 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:15,966 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:15,974 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:15,965 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:15,967 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:16,006 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:58:16,006 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:58:16,055 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407896055
2022-02-09T20:58:15,967 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:15,981 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:15,974 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:16,106 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:16,065 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:16,055 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407896055
2022-02-09T20:58:16,106 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:16,169 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:16,083 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:16,114 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:58:16,169 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:16,186 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:16,178 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:16,178 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:16,186 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:16,241 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:16,200 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:16,241 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:16,264 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:16,244 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:16,264 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:16,273 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2022-02-09T20:58:16,271 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:16,299 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:16,273 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2022-02-09T20:58:16,325 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:16,299 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:16,325 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:17,650 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:17,652 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]46356
2022-02-09T20:58:17,659 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:17,658 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:17,659 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:17,675 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:58:17,673 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:17,675 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:58:17,681 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407897681
2022-02-09T20:58:17,681 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:58:17,681 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407897681
2022-02-09T20:58:17,702 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:17,852 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:17,861 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]12032
2022-02-09T20:58:17,868 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:17,867 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:17,868 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:17,899 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:58:17,882 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:17,899 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:58:17,904 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407897904
2022-02-09T20:58:17,904 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407897904
2022-02-09T20:58:17,904 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:58:17,907 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:18,053 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:18,055 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]16908
2022-02-09T20:58:18,062 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:18,062 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:18,062 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:18,078 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:58:18,076 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:18,078 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:58:18,083 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407898083
2022-02-09T20:58:18,083 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:58:18,083 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407898083
2022-02-09T20:58:18,096 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:18,135 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:18,141 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]22624
2022-02-09T20:58:18,143 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:18,143 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:18,143 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:18,147 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:58:18,144 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:18,147 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:58:18,175 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407898175
2022-02-09T20:58:18,174 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:58:18,175 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407898175
2022-02-09T20:58:18,179 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:18,222 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:18,221 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:18,222 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:18,239 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:18,224 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:18,240 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:18,243 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:18,245 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:18,247 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:18,239 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:18,250 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:18,248 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:18,267 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:18,250 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:18,284 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:18,281 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:18,284 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:18,288 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:18,296 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:58:18,286 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:58:18,296 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:58:18,288 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:18,333 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:18,307 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:58:18,333 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:18,349 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:18,349 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:18,360 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.
2022-02-09T20:58:18,347 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:58:18,362 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:18,362 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:18,387 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:18,360 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.
2022-02-09T20:58:18,387 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:18,619 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:18,630 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]25412
2022-02-09T20:58:18,639 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:18,638 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:18,639 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:18,656 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:58:18,655 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:18,656 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:58:18,684 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407898684
2022-02-09T20:58:18,684 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:58:18,684 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407898684
2022-02-09T20:58:18,705 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:19,251 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:19,260 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]43328
2022-02-09T20:58:19,268 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:19,267 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:19,269 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:19,268 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:19,287 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:58:19,287 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:58:19,291 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407899291
2022-02-09T20:58:19,291 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:58:19,291 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407899291
2022-02-09T20:58:19,293 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:19,696 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:19,695 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:19,696 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:19,714 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:19,706 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:19,714 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:19,737 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:19,730 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:19,737 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:19,756 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:19,753 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:19,756 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:19,772 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:19,758 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:19,772 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:19,794 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:19,786 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:19,813 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:19,794 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:19,819 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:19,813 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:19,833 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:19,812 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:19,802 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:19,856 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:19,833 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:19,866 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:19,819 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:19,881 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2022-02-09T20:58:19,856 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:19,854 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:19,900 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:19,834 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:19,881 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2022-02-09T20:58:19,866 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:19,937 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:19,932 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:19,900 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:19,937 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:20,012 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:19,951 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:20,012 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:20,038 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:20,043 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:20,042 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:20,043 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:20,046 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:20,045 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:20,046 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:20,050 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2022-02-09T20:58:20,048 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:20,053 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:20,050 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2022-02-09T20:58:20,053 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:20,099 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:20,099 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:20,339 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:20,338 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:20,366 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:58:20,339 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:20,368 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:20,366 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:58:20,344 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:20,368 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:20,417 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:20,405 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:20,417 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:20,447 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:20,443 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:20,447 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:20,474 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:20,461 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:20,474 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:20,479 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:20,480 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:20,476 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:20,480 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:20,512 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:20,480 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:20,514 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:20,479 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:20,516 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:20,512 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:20,516 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:20,512 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:20,514 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:20,514 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:20,516 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:20,544 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 2 seconds.
2022-02-09T20:58:20,518 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:20,544 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:20,516 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:20,557 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:20,524 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:20,544 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:20,544 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 2 seconds.
2022-02-09T20:58:20,570 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:20,557 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:20,644 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:20,644 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:20,655 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:20,655 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:20,664 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:20,664 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:20,680 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 2 seconds.
2022-02-09T20:58:20,701 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:20,626 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:20,707 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:20,701 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:20,680 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 2 seconds.
2022-02-09T20:58:20,707 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:20,868 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:20,869 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:20,869 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:20,878 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:20,869 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:20,878 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:20,897 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:20,924 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:20,923 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:20,925 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:20,923 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:20,930 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:20,927 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:20,930 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:20,966 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:20,944 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:20,991 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:20,966 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:21,003 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:21,021 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:21,021 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:21,022 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:58:21,063 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:21,063 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:21,082 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2022-02-09T20:58:21,063 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:58:21,097 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:21,097 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:21,112 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:21,082 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2022-02-09T20:58:21,112 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:21,481 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:21,480 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:21,481 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:21,497 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:21,486 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:21,497 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:21,538 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:21,512 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:21,538 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:21,584 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:21,574 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:21,584 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:21,643 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:21,616 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:21,643 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:21,679 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:21,652 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:21,709 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:21,679 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:21,711 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:21,709 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:21,686 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:21,711 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:21,756 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-02-09T20:58:21,721 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:21,778 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:21,756 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-02-09T20:58:21,778 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:21,887 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:58:21,887 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:58:22,061 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:58:22,061 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:58:22,556 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:58:22,556 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:58:22,699 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:58:22,699 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:58:23,127 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:58:23,127 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:58:23,707 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:23,709 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]38880
2022-02-09T20:58:23,717 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:23,717 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:23,717 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:23,758 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:58:23,733 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:23,763 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:58:23,758 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:58:23,763 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:58:23,768 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407903768
2022-02-09T20:58:23,768 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:58:23,768 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407903768
2022-02-09T20:58:23,798 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:25,915 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:25,914 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:25,915 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:25,940 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:25,920 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:25,940 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:25,945 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:25,943 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:25,945 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:25,951 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:25,947 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:25,951 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:25,983 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:25,966 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:25,990 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:25,983 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:26,018 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:26,067 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:26,062 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:26,076 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:26,079 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:58:26,067 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:26,086 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:26,084 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:58:26,086 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:26,097 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 3 seconds.
2022-02-09T20:58:26,087 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:58:26,113 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:26,114 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:26,097 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 3 seconds.
2022-02-09T20:58:26,114 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:26,113 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:26,206 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:26,208 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]28312
2022-02-09T20:58:26,215 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:26,215 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:26,215 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:26,232 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:58:26,230 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:26,232 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:58:26,258 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407906258
2022-02-09T20:58:26,258 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407906258
2022-02-09T20:58:26,258 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:58:26,276 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:27,399 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:27,403 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]30016
2022-02-09T20:58:27,410 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:27,410 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:27,426 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:27,410 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:27,449 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:58:27,449 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:58:27,459 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407907459
2022-02-09T20:58:27,459 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407907459
2022-02-09T20:58:27,459 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:58:27,482 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:27,659 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:27,665 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]34628
2022-02-09T20:58:27,671 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:27,671 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:27,687 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:27,671 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:27,750 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:58:27,750 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:58:27,810 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407907810
2022-02-09T20:58:27,810 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407907810
2022-02-09T20:58:27,810 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:58:27,813 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:28,183 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:28,188 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]44700
2022-02-09T20:58:28,209 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:28,209 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:28,209 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:28,214 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:58:28,211 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:28,214 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:58:28,220 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407908220
2022-02-09T20:58:28,220 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:58:28,220 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407908220
2022-02-09T20:58:28,248 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:28,268 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:28,267 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:28,268 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:28,289 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:28,279 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:28,289 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:28,316 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:28,312 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:28,316 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:28,323 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:28,323 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:28,326 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:28,326 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:28,319 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:28,330 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:28,330 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:28,330 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:28,355 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:28,339 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:28,355 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:28,373 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.
2022-02-09T20:58:28,370 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:28,375 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:28,373 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.
2022-02-09T20:58:28,375 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:28,404 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:28,404 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:28,856 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:28,858 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]30620
2022-02-09T20:58:28,881 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:28,881 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:28,881 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:28,884 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:58:28,883 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:28,884 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:58:28,889 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407908889
2022-02-09T20:58:28,889 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407908889
2022-02-09T20:58:28,889 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:58:28,891 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:29,034 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:29,036 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]36780
2022-02-09T20:58:29,043 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:29,043 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:29,043 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:29,071 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:58:29,059 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:29,071 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:58:29,094 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:58:29,095 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407909095
2022-02-09T20:58:29,095 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407909095
2022-02-09T20:58:29,103 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:29,123 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:58:29,123 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:58:29,663 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:29,662 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:29,663 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:29,683 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:29,675 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:29,683 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:29,700 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:29,684 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:29,700 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:29,704 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:29,702 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:29,704 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:29,708 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:29,706 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:29,708 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:29,710 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:29,713 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:29,713 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:29,713 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:29,716 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:29,714 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:29,716 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:29,719 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.
2022-02-09T20:58:29,718 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:29,721 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:29,719 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.
2022-02-09T20:58:29,721 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:29,755 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:29,755 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:29,863 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:29,867 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]47756
2022-02-09T20:58:29,875 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:29,875 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:29,875 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:29,893 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:58:29,876 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:29,893 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:58:29,923 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407909923
2022-02-09T20:58:29,922 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:58:29,923 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407909923
2022-02-09T20:58:29,960 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:30,329 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:30,328 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:30,329 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:30,343 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:30,333 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:30,343 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:30,380 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:30,359 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:30,380 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:30,411 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:30,386 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:30,411 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:30,446 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:30,424 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:30,446 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:30,479 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:30,454 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:30,479 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:30,509 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:30,484 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:30,509 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:30,522 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 3 seconds.
2022-02-09T20:58:30,530 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:30,541 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:30,518 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:30,545 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:30,541 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:30,580 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:30,540 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:30,530 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:30,522 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 3 seconds.
2022-02-09T20:58:30,588 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:30,580 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:30,705 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:30,545 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:30,705 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:30,705 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:30,752 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:30,743 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:30,752 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:30,779 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:30,766 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:30,779 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:30,798 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:30,794 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:30,798 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:30,826 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:30,809 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:30,852 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:30,826 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:30,854 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 3 seconds.
2022-02-09T20:58:30,852 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:30,827 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:30,857 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:30,854 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 3 seconds.
2022-02-09T20:58:30,857 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:31,053 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:31,054 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:31,054 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:31,054 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:31,086 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:31,079 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:31,086 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:31,106 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:31,088 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:31,106 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:31,111 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:31,108 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:31,111 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:31,181 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:31,156 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:31,181 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:31,217 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:31,188 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:31,217 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:31,226 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:31,225 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:31,259 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:31,226 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:31,281 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 3 seconds.
2022-02-09T20:58:31,259 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:31,251 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:31,292 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:31,281 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 3 seconds.
2022-02-09T20:58:31,292 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:31,391 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:58:31,391 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:58:31,424 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:31,425 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:31,425 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:31,428 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:31,425 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:31,428 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:31,432 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:31,430 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:31,432 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:31,453 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:31,435 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:31,453 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:31,486 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:31,471 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:31,486 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:31,515 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:31,501 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:31,515 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:31,539 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:31,545 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:31,529 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:31,545 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:31,539 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:31,581 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.
2022-02-09T20:58:31,559 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:31,581 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:31,581 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.
2022-02-09T20:58:31,581 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:31,852 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:31,851 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:31,852 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:31,877 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:31,852 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:31,877 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:31,899 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:31,883 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:31,899 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:31,933 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:31,911 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:31,933 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:31,955 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:31,953 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:31,955 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:31,991 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:31,959 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:31,991 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:31,995 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:32,012 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:31,993 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:32,012 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:31,995 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:32,061 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-02-09T20:58:32,030 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:32,083 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:32,061 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2022-02-09T20:58:32,083 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:32,645 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:32,649 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]12628
2022-02-09T20:58:32,664 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:32,664 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:32,664 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:32,667 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:58:32,666 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:32,667 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:58:32,699 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407912699
2022-02-09T20:58:32,698 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:58:32,699 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407912699
2022-02-09T20:58:32,710 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:32,733 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:58:32,733 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:58:33,526 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:58:33,526 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:58:33,858 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:58:33,858 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:58:33,881 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:33,879 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:33,881 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:33,892 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:33,890 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:33,893 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:33,895 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:33,896 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:33,892 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:33,900 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:33,898 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:33,900 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:33,904 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:33,901 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:33,930 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:33,943 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:33,946 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:58:33,946 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:58:33,948 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:58:33,950 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:58:33,952 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:58:33,904 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:33,986 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:33,964 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T20:58:33,986 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:33,994 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn = lambda ctx: entry_point(None, ctx)
2022-02-09T20:58:34,019 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:34,019 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57\handler.py", line 123, in handle
2022-02-09T20:58:34,019 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:34,022 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:34,021 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     raise e
2022-02-09T20:58:34,022 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:34,025 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 5 seconds.
2022-02-09T20:58:34,024 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57\handler.py", line 112, in handle
2022-02-09T20:58:34,036 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:34,025 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 5 seconds.
2022-02-09T20:58:34,036 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:34,064 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:34,064 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:34,212 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:34,213 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]41100
2022-02-09T20:58:34,221 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:34,221 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:34,221 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:34,247 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:58:34,240 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:34,247 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:58:34,267 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407914267
2022-02-09T20:58:34,267 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407914267
2022-02-09T20:58:34,267 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:58:34,284 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:34,302 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:58:34,302 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:58:34,597 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:58:34,597 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:58:35,078 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:58:35,078 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:58:35,942 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:35,943 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:35,943 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:35,952 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:35,943 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:35,952 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:35,984 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:35,967 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:35,984 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:35,988 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:35,986 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:35,988 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:35,991 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:35,989 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:35,991 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:35,993 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:35,997 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:35,996 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:35,997 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:36,022 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:35,998 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:36,022 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:36,026 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.
2022-02-09T20:58:36,024 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:36,027 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:36,026 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.
2022-02-09T20:58:36,027 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:36,067 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:36,067 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:36,268 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:36,272 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]45088
2022-02-09T20:58:36,279 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:36,279 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:36,279 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:36,318 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:58:36,295 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:36,318 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:58:36,323 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407916323
2022-02-09T20:58:36,323 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407916323
2022-02-09T20:58:36,323 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:58:36,326 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:37,576 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:37,577 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]42996
2022-02-09T20:58:37,585 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:37,584 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:37,586 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:37,585 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:37,607 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:58:37,607 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:58:37,615 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407917615
2022-02-09T20:58:37,615 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:58:37,615 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407917615
2022-02-09T20:58:37,627 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:37,977 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:37,976 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:37,977 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:37,986 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:37,977 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:37,986 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:38,011 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:38,003 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:38,011 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:38,030 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:38,027 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:38,030 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:38,052 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:38,045 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:38,052 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:38,064 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:38,054 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:38,091 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:38,064 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:38,094 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:38,093 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:38,102 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:38,105 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:58:38,110 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:38,107 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:58:38,094 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:38,115 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.
2022-02-09T20:58:38,110 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:38,115 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.
2022-02-09T20:58:38,115 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:58:38,171 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:38,171 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:38,229 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:38,230 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]34108
2022-02-09T20:58:38,261 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:38,260 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:38,261 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:38,294 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:58:38,268 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:38,294 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:58:38,341 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407918341
2022-02-09T20:58:38,341 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407918341
2022-02-09T20:58:38,341 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:58:38,343 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:38,589 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:38,593 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]44964
2022-02-09T20:58:38,600 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:38,600 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:38,600 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:38,615 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:38,625 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:58:38,625 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:58:38,644 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407918644
2022-02-09T20:58:38,644 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407918644
2022-02-09T20:58:38,644 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:58:38,659 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:38,829 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:38,830 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]37416
2022-02-09T20:58:38,852 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:38,852 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:38,852 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:38,875 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:58:38,858 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:38,875 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:58:38,887 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407918887
2022-02-09T20:58:38,887 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:58:38,887 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407918887
2022-02-09T20:58:38,923 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:39,038 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:58:39,038 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:58:39,566 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:39,565 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:39,566 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:39,589 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:39,580 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:39,604 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:39,589 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:39,615 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:39,606 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:39,615 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:39,668 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:39,631 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:39,668 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:39,672 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:39,670 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:39,672 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:39,705 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:39,694 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:39,725 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:39,734 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:39,705 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:39,767 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:39,767 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:39,768 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 5 seconds.
2022-02-09T20:58:39,768 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 5 seconds.
2022-02-09T20:58:39,795 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:39,758 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:58:39,824 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:39,819 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:39,795 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:39,824 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:39,833 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]34812
2022-02-09T20:58:39,889 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:39,888 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:39,889 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:39,911 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:58:39,895 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:39,911 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:58:39,927 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407919927
2022-02-09T20:58:39,927 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:58:39,927 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407919927
2022-02-09T20:58:39,946 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:40,300 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:40,299 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:40,300 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:40,317 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:40,313 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:40,317 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:40,322 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:40,322 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:40,322 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:40,327 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:40,324 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:40,327 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:40,330 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:40,329 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:40,330 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:40,332 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:40,335 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:40,335 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:40,335 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:40,337 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:40,364 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:40,364 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:40,365 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 5 seconds.
2022-02-09T20:58:40,364 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:40,367 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:40,365 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 5 seconds.
2022-02-09T20:58:40,367 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:40,378 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:40,378 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:40,555 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:40,554 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:40,555 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:40,563 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:40,555 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:40,563 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:40,581 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:40,579 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:40,581 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:40,586 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:40,583 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:40,586 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:40,625 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:40,600 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:40,625 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:40,647 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:40,627 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:40,647 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:40,664 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:40,664 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:40,703 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 5 seconds.
2022-02-09T20:58:40,715 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:40,650 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:40,724 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:40,703 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 5 seconds.
2022-02-09T20:58:40,715 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:40,724 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:40,900 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:40,901 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:40,901 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:40,901 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:40,924 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:40,922 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:40,924 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:40,927 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:40,925 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:40,927 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:40,931 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:40,928 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:40,931 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:40,964 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:40,962 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:40,964 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:40,997 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:40,971 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:41,023 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:40,997 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:41,029 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:41,023 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:41,043 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:58:41,029 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:41,060 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.
2022-02-09T20:58:41,002 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:41,043 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:58:41,067 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:41,060 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.
2022-02-09T20:58:41,067 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:41,740 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:41,739 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:41,740 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:41,749 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:41,740 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:41,749 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:41,772 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:41,764 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:41,772 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:41,797 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:41,789 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:41,797 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:41,818 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:41,815 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:41,818 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:41,834 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:41,830 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:41,834 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:41,849 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:41,846 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:41,849 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:41,855 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-02-09T20:58:41,853 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:41,865 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:41,855 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2022-02-09T20:58:41,865 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:41,913 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:41,913 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:43,122 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:58:43,122 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:58:43,317 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:43,326 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]7760
2022-02-09T20:58:43,327 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:43,327 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:43,327 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:43,329 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:58:43,329 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:43,329 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:58:43,334 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407923334
2022-02-09T20:58:43,334 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407923334
2022-02-09T20:58:43,334 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:58:43,337 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:44,778 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:58:44,778 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:58:44,843 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:44,844 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]26788
2022-02-09T20:58:44,851 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:44,851 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:44,851 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:44,869 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:58:44,867 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:44,869 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:58:44,896 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407924896
2022-02-09T20:58:44,896 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407924896
2022-02-09T20:58:44,910 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:44,896 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:58:44,910 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:44,933 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:44,909 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:44,933 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:44,950 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:44,925 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:44,934 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:44,950 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:44,992 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:44,978 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:44,992 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:45,009 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:45,007 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:45,009 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:45,030 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:45,034 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:45,034 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:45,034 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:45,037 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:45,036 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:45,037 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:45,040 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 8 seconds.
2022-02-09T20:58:45,063 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:45,039 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:45,067 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:45,063 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:45,040 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 8 seconds.
2022-02-09T20:58:45,067 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:45,384 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:58:45,384 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:58:45,713 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:58:45,713 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:58:46,070 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:58:46,070 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:58:46,773 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:46,772 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:46,773 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:46,795 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:46,776 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:46,795 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:46,822 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:46,813 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:46,822 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:46,841 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:46,825 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:46,841 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:46,868 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:46,865 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:46,870 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:58:46,868 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:46,870 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:58:46,874 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:46,870 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:46,874 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:46,879 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:46,876 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:46,906 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:46,879 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:46,908 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.
2022-02-09T20:58:46,906 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:46,894 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:46,929 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:46,908 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.
2022-02-09T20:58:46,929 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:48,229 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:48,230 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]44764
2022-02-09T20:58:48,238 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:48,237 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:48,251 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:48,238 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:48,262 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:58:48,262 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:58:48,302 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407928302
2022-02-09T20:58:48,302 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:58:48,302 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407928302
2022-02-09T20:58:48,337 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:49,892 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407929
2022-02-09T20:58:49,901 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:15.575206756591797|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407929
2022-02-09T20:58:49,908 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:217.26951599121094|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407929
2022-02-09T20:58:49,924 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:93.3|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407929
2022-02-09T20:58:49,932 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:4777.30859375|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407929
2022-02-09T20:58:49,948 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:11400.72265625|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407929
2022-02-09T20:58:49,950 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:70.5|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644407929
2022-02-09T20:58:50,571 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:50,573 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]35668
2022-02-09T20:58:50,580 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:50,579 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:50,580 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:50,605 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:58:50,597 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:50,605 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:58:50,642 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407930642
2022-02-09T20:58:50,641 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:58:50,642 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407930642
2022-02-09T20:58:50,666 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:50,898 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:50,897 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:50,898 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:50,932 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:50,911 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:50,932 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:50,935 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:50,933 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:50,935 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:50,939 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:50,936 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:50,939 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:50,964 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:50,962 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:50,964 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:50,965 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:50,969 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:50,968 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:50,969 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:50,996 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:50,978 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:50,996 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:50,999 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.
2022-02-09T20:58:50,997 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:51,000 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:50,999 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.
2022-02-09T20:58:51,000 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:58:51,010 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:51,010 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:58:51,115 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=7760)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 7760

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('7760',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('7760', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=26788)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 26788

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('26788',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('26788', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=42996)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 42996

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('42996',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('42996', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=44964)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 44964

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('44964',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('44964', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=37416)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 37416

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('37416',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('37416', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=34812)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 34812

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('34812',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('34812', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=34108)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 34108

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('34108',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('34108', 0)
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='cp949'>
OSError: [Errno 22] Invalid argument

2022-02-09T20:58:51,115 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=7760)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 7760

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('7760',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('7760', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=26788)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 26788

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('26788',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('26788', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=42996)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 42996

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('42996',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('42996', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=44964)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 44964

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('44964',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('44964', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=37416)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 37416

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('37416',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('37416', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=34812)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 34812

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('34812',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('34812', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=34108)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 34108

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('34108',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('34108', 0)
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='cp949'>
OSError: [Errno 22] Invalid argument

2022-02-09T20:58:51,547 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:51,551 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]44708
2022-02-09T20:58:51,558 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:51,558 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:51,558 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:51,574 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:58:51,573 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:51,574 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:58:51,580 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:58:51,582 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407931582
2022-02-09T20:58:51,582 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407931582
2022-02-09T20:58:51,584 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:51,648 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:51,652 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]47244
2022-02-09T20:58:51,663 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:51,662 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:51,663 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:51,680 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:58:51,678 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:51,680 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:58:51,686 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407931686
2022-02-09T20:58:51,685 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:58:51,686 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407931686
2022-02-09T20:58:51,700 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:52,465 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:52,491 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:52,513 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]30820
2022-02-09T20:58:52,519 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:52,517 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]27652
2022-02-09T20:58:52,520 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:52,520 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:52,519 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:52,524 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:58:52,518 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:52,522 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:52,520 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:52,529 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:58:52,525 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:52,529 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:58:52,524 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:58:52,534 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407932534
2022-02-09T20:58:52,534 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407932534
2022-02-09T20:58:52,536 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:58:52,539 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407932539
2022-02-09T20:58:52,534 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:58:52,541 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:52,539 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407932539
2022-02-09T20:58:52,551 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:52,621 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:52,621 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:52,621 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:52,647 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:52,639 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:52,647 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:52,648 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:52,682 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:52,682 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:52,682 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:52,706 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:52,684 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:52,706 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:52,709 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:52,708 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:52,709 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:52,711 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:52,745 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:52,744 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:52,745 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:52,770 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:52,772 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:52,763 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:52,772 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:58:52,770 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:52,817 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 8 seconds.
2022-02-09T20:58:52,795 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:58:52,826 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:52,826 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:58:52,817 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 8 seconds.
2022-02-09T20:58:53,056 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:58:53,056 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:58:53,246 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:53,245 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:53,246 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:53,272 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:53,247 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:53,272 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:53,275 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:53,273 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:53,282 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:53,275 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:53,284 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:53,282 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:53,300 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:53,282 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:53,281 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:53,300 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:53,306 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:53,284 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:53,336 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:53,304 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:53,302 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:53,336 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:53,348 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:53,306 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:53,366 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:53,344 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:53,396 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:53,337 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:53,366 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:53,417 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:53,348 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:53,436 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:53,412 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:53,396 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:58:53,379 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:53,445 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:53,436 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:53,496 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 8 seconds.
2022-02-09T20:58:53,417 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:53,487 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:53,515 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:53,515 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:53,515 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:58:53,471 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:53,515 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:53,546 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:53,496 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 8 seconds.
2022-02-09T20:58:53,544 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:53,546 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:53,549 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 8 seconds.
2022-02-09T20:58:53,549 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:53,550 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:53,549 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 8 seconds.
2022-02-09T20:58:53,550 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:58:53,567 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:53,567 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:58:53,972 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:53,972 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:53,972 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:53,981 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:53,973 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:53,981 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:53,996 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:53,995 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:53,996 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:54,010 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:54,019 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:53,999 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:54,019 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:54,024 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:54,018 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:54,010 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:54,027 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:54,024 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:54,022 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:54,029 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:54,027 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:54,032 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:54,026 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:54,058 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:54,029 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:54,074 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:54,029 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:54,058 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:54,047 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:54,032 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:58:54,123 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:54,094 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:54,074 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:54,145 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:54,123 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:54,146 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-02-09T20:58:54,122 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:54,145 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:54,127 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:54,151 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:54,151 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:54,149 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:54,146 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2022-02-09T20:58:54,151 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:54,158 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:54,151 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:58:54,167 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:54,153 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:54,158 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:54,182 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.
2022-02-09T20:58:54,167 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:58:54,180 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:54,185 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:54,182 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.
2022-02-09T20:58:54,185 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:58:54,923 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:58:54,923 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:58:55,466 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:55,472 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]43204
2022-02-09T20:58:55,473 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:55,473 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:55,473 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:55,473 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:55,473 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:58:55,473 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:58:55,475 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407935475
2022-02-09T20:58:55,475 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407935475
2022-02-09T20:58:55,476 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:58:55,486 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:56,486 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:56,485 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:56,486 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:56,486 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:56,495 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:56,495 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:56,495 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:56,496 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:56,496 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:56,496 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:56,498 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:56,497 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:56,498 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:56,498 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:56,498 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:56,498 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:56,498 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:56,499 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:56,499 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:56,501 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:56,501 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:56,501 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:56,501 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:58:56,501 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:56,502 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 13 seconds.
2022-02-09T20:58:56,502 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:58:56,502 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:56,502 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 13 seconds.
2022-02-09T20:58:56,502 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:58:56,524 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:56,524 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:58:57,206 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:58:57,207 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]38500
2022-02-09T20:58:57,211 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:57,211 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:58:57,211 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:58:57,212 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:58:57,212 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:58:57,212 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:58:57,214 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407937214
2022-02-09T20:58:57,214 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407937214
2022-02-09T20:58:57,214 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:58:57,215 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:58:58,273 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:58,272 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:58:58,273 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:58:58,276 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:58,276 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:58:58,276 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:58:58,276 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:58:58,276 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:58,277 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:58:58,276 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:58:58,278 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:58,277 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:58:58,278 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:58:58,278 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:58,278 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:58:58,278 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:58:58,279 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:58:58,279 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:58:58,279 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:58:58,280 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:58,280 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:58,281 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:58,280 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:58:58,281 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:58,281 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.
2022-02-09T20:58:58,281 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:58:58,281 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:58,281 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.
2022-02-09T20:58:58,281 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:58:58,293 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:58,293 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:58:59,012 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:58:59,012 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:59:00,820 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:59:00,820 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:59:01,272 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:01,285 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]43912
2022-02-09T20:59:01,285 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:01,285 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:01,285 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:01,286 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:59:01,286 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:01,286 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:59:01,289 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407941289
2022-02-09T20:59:01,289 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407941289
2022-02-09T20:59:01,289 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:59:01,316 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:01,507 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:59:01,507 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:59:01,576 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:59:01,576 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:59:02,164 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:59:02,164 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:59:02,195 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:59:02,195 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:59:03,293 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:03,292 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:03,293 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:03,294 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:03,295 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:03,295 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:03,295 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:03,296 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:03,296 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:03,295 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:03,296 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:03,297 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:03,297 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:03,298 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:03,297 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:03,301 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:03,300 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:03,301 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:03,303 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:03,302 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:03,303 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:03,306 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:59:03,303 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:59:03,306 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:59:03,329 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:59:03,328 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:59:03,329 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:59:03,329 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.
2022-02-09T20:59:03,329 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:59:03,330 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:59:03,329 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.
2022-02-09T20:59:03,330 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:59:03,350 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:59:03,350 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:59:05,277 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:05,288 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]46876
2022-02-09T20:59:05,295 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:05,295 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:05,295 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:05,297 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:59:05,296 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:05,297 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:59:05,300 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407945300
2022-02-09T20:59:05,299 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:59:05,300 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407945300
2022-02-09T20:59:05,306 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:06,369 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:06,371 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]35484
2022-02-09T20:59:06,373 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:06,373 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:06,373 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:06,373 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:06,374 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:59:06,374 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:59:06,377 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407946377
2022-02-09T20:59:06,377 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407946377
2022-02-09T20:59:06,377 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:59:06,379 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:06,483 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:06,485 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]27216
2022-02-09T20:59:06,493 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:06,493 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:06,493 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:06,494 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:59:06,494 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:06,494 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:59:06,498 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407946498
2022-02-09T20:59:06,497 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:59:06,498 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407946498
2022-02-09T20:59:06,499 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:07,222 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:07,222 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:07,227 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:07,222 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:07,235 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:07,228 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:07,235 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]45540
2022-02-09T20:59:07,236 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:07,235 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:07,236 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:07,236 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:07,236 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:07,236 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:59:07,236 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:07,236 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:59:07,236 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:07,237 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:07,238 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:07,236 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:07,238 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:07,239 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407947239
2022-02-09T20:59:07,238 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:07,239 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:07,239 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:59:07,239 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407947239
2022-02-09T20:59:07,239 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:07,239 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:07,240 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:07,241 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:07,241 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:07,241 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:07,242 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:59:07,242 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:59:07,243 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:59:07,243 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:07,243 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:59:07,244 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 13 seconds.
2022-02-09T20:59:07,243 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:07,244 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:59:07,244 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 13 seconds.
2022-02-09T20:59:07,244 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:59:07,252 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:07,289 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]37896
2022-02-09T20:59:07,290 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:07,289 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:07,290 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:07,290 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:59:07,290 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:07,290 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:59:07,292 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407947292
2022-02-09T20:59:07,292 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407947292
2022-02-09T20:59:07,292 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:59:07,294 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:07,296 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:59:07,296 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:59:08,323 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:08,322 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:08,323 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:08,331 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:08,324 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:08,331 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:08,331 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:08,331 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:08,331 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:08,331 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:08,332 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:08,333 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:08,334 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:08,334 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:08,334 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:08,334 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:08,335 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:08,335 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:08,335 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:08,336 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:08,337 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:08,337 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:59:08,338 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:59:08,338 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:59:08,338 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:59:08,339 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:59:08,339 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:59:08,339 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:59:08,341 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 13 seconds.
2022-02-09T20:59:08,339 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T20:59:08,341 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:59:08,341 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 13 seconds.
2022-02-09T20:59:08,341 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:59:08,368 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:59:08,368 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:59:08,564 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:08,563 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:08,564 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:08,566 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:08,576 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:08,576 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:08,576 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:08,576 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:08,577 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:08,576 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:08,578 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:08,577 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:08,578 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:08,579 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:08,579 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:08,579 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:08,579 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:08,579 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:08,580 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:08,581 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:59:08,581 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:59:08,582 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:59:08,582 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:08,582 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:59:08,583 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 13 seconds.
2022-02-09T20:59:08,583 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:08,584 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:59:08,583 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 13 seconds.
2022-02-09T20:59:08,584 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:59:08,612 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:59:08,612 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:59:09,156 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:09,156 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:09,156 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:09,156 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:09,163 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:09,163 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:09,163 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:09,163 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:09,163 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:09,163 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:09,165 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:09,164 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:09,165 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:09,166 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:09,165 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:09,166 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:09,166 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:09,166 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:09,167 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:09,168 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:59:09,168 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:59:09,169 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:59:09,168 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:09,169 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:59:09,169 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-02-09T20:59:09,169 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:09,169 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:59:09,169 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2022-02-09T20:59:09,169 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:59:09,186 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:59:09,187 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:09,186 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:59:09,187 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:09,201 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:09,187 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:09,201 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:09,201 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:09,202 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:09,202 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:09,202 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:09,203 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:09,205 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:09,205 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:09,205 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:09,205 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:09,205 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:09,205 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:09,206 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:09,207 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:09,208 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:09,208 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:59:09,208 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:59:09,210 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:59:09,209 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:09,210 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:59:09,211 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.
2022-02-09T20:59:09,210 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:09,211 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:59:09,211 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.
2022-02-09T20:59:09,211 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:59:09,231 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:59:09,231 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:59:09,515 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:59:09,515 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:59:11,289 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:59:11,289 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:59:12,898 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:12,899 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]48020
2022-02-09T20:59:12,906 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:12,906 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:12,906 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:12,906 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:12,907 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:59:12,907 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:59:12,910 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407952910
2022-02-09T20:59:12,910 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407952910
2022-02-09T20:59:12,910 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:59:12,912 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:14,856 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:14,856 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:14,856 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:14,865 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:14,857 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:14,865 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:14,866 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:14,866 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:14,867 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:14,866 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:14,868 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:14,867 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:14,868 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:14,870 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:14,869 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:14,870 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:14,871 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:14,873 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:59:14,873 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:14,873 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:59:14,876 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:59:14,875 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:14,876 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:59:14,896 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 21 seconds.
2022-02-09T20:59:14,879 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:14,896 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:59:14,896 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 21 seconds.
2022-02-09T20:59:14,896 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:59:14,902 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:59:14,902 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:59:15,604 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:15,606 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]28064
2022-02-09T20:59:15,614 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:15,614 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:15,614 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:15,614 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:15,615 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:59:15,615 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:59:15,618 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407955618
2022-02-09T20:59:15,618 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407955618
2022-02-09T20:59:15,618 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:59:15,620 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:16,347 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:59:16,347 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:59:17,026 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:17,027 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:17,027 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:17,027 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:17,034 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:17,034 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:17,034 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:17,034 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:17,034 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:17,037 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:17,035 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:17,037 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:17,037 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:17,037 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:17,037 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:17,038 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:17,038 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:17,039 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:17,039 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:17,040 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:59:17,040 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:59:17,040 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:59:17,040 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:59:17,041 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.
2022-02-09T20:59:17,041 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:17,041 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:59:17,041 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.
2022-02-09T20:59:17,041 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:59:17,081 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:59:17,081 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:59:18,913 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:18,921 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]45036
2022-02-09T20:59:18,922 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:18,922 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:18,922 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:18,923 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:59:18,922 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:18,923 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:59:18,924 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407958924
2022-02-09T20:59:18,924 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407958924
2022-02-09T20:59:18,925 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:59:18,931 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:19,899 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:19,899 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:19,899 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:19,899 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:19,906 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:19,905 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:19,906 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:19,906 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:19,906 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:19,906 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:19,906 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:19,907 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:19,908 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:19,908 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:19,908 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:19,908 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:19,908 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:19,908 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:19,909 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:19,910 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:59:19,910 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:19,910 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:59:19,911 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:59:19,911 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:19,911 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:59:19,912 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.
2022-02-09T20:59:19,912 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:59:19,912 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:59:19,912 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.
2022-02-09T20:59:19,912 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:59:19,928 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:59:19,928 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:59:20,259 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:59:20,259 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:59:21,351 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:59:21,351 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:59:21,588 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:59:21,588 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:59:22,174 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:59:22,174 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:59:22,226 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:59:22,226 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:59:22,904 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:22,906 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]32968
2022-02-09T20:59:22,911 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:22,911 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:22,911 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:22,911 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:22,912 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:59:22,912 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:59:22,914 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407962914
2022-02-09T20:59:22,914 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407962914
2022-02-09T20:59:22,914 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:59:22,916 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:24,822 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:24,822 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:24,822 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:24,865 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:24,864 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:24,865 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:24,866 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:24,866 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:24,867 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:24,866 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:24,869 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:24,867 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:24,869 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:24,870 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:24,869 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:24,870 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:24,870 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:24,871 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:24,871 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:24,872 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:59:24,872 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:59:24,872 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:59:24,872 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:24,872 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:59:24,874 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 21 seconds.
2022-02-09T20:59:24,873 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:24,875 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:59:24,874 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 21 seconds.
2022-02-09T20:59:24,875 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:59:24,911 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:59:24,911 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:59:25,138 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:25,152 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]38832
2022-02-09T20:59:25,153 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:25,153 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:25,153 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:25,153 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:25,154 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:59:25,154 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:59:25,157 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407965157
2022-02-09T20:59:25,157 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:59:25,157 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407965157
2022-02-09T20:59:25,159 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:25,557 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:25,557 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]37496
2022-02-09T20:59:25,564 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:25,563 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:25,564 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:25,564 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:25,564 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:59:25,564 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:59:25,568 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407965568
2022-02-09T20:59:25,567 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:59:25,568 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407965568
2022-02-09T20:59:25,570 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:26,054 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:26,055 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]47808
2022-02-09T20:59:26,060 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:26,060 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:26,060 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:26,061 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:26,061 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:59:26,061 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:59:26,064 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407966064
2022-02-09T20:59:26,064 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:59:26,064 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407966064
2022-02-09T20:59:26,066 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:26,092 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:26,095 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]8692
2022-02-09T20:59:26,099 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:26,098 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:26,099 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:26,102 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:59:26,101 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:26,102 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:59:26,110 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407966110
2022-02-09T20:59:26,110 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:59:26,110 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407966110
2022-02-09T20:59:26,140 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:26,543 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:26,544 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:26,543 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:26,544 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:26,551 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:26,551 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:26,552 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:26,552 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:26,552 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:26,553 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:26,554 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:26,555 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:26,551 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:26,556 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:26,556 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:26,557 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:26,556 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:26,559 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:26,557 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:59:26,562 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:59:26,563 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:59:26,563 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T20:59:26,564 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn = lambda ctx: entry_point(None, ctx)
2022-02-09T20:59:26,559 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:26,565 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:26,565 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57\handler.py", line 123, in handle
2022-02-09T20:59:26,565 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     raise e
2022-02-09T20:59:26,566 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57\handler.py", line 112, in handle
2022-02-09T20:59:26,565 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:26,591 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     _service.initialize(context)
2022-02-09T20:59:26,592 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57\handler.py", line 37, in initialize
2022-02-09T20:59:26,593 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:59:26,593 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:59:26,594 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:59:26,594 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)
2022-02-09T20:59:26,594 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:59:26,595 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 21 seconds.
2022-02-09T20:59:26,595 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\transformers\models\auto\auto_factory.py", line 419, in from_pretrained
2022-02-09T20:59:26,595 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:59:26,595 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 21 seconds.
2022-02-09T20:59:26,595 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:59:26,623 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:59:26,623 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:59:27,001 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:27,000 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:27,001 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:27,009 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:27,002 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:27,009 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:27,009 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:27,009 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:27,010 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:27,009 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:27,011 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:27,010 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:27,011 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:27,012 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:27,012 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:27,012 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:27,012 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:27,014 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:27,014 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:59:27,014 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:27,014 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:59:27,015 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:59:27,015 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:59:27,016 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 21 seconds.
2022-02-09T20:59:27,016 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 21 seconds.
2022-02-09T20:59:27,016 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:27,016 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:59:27,016 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:59:27,043 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:59:27,043 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:59:27,360 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:27,361 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:27,361 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:27,361 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:27,366 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:27,366 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:27,366 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:27,367 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:27,367 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:27,367 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:27,367 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:27,367 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:27,369 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:27,369 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:27,369 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:27,369 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:27,369 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:27,369 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:27,370 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:27,371 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:59:27,371 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:59:27,372 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:59:27,371 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:27,372 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:59:27,372 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-02-09T20:59:27,372 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:27,373 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:59:27,372 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2022-02-09T20:59:27,373 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:59:27,401 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:59:27,401 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:59:27,440 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:27,441 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:27,440 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:27,441 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:27,441 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:27,441 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:27,441 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:27,441 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:27,442 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:27,442 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:27,442 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:27,442 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:27,443 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:27,443 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:27,444 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:27,443 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:27,444 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:27,444 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:27,444 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:27,445 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:27,446 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:27,446 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:59:27,446 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:59:27,446 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:59:27,447 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:59:27,447 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:59:27,447 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:59:27,447 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.
2022-02-09T20:59:27,447 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:59:27,447 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:59:27,447 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.
2022-02-09T20:59:27,447 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:59:27,469 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:59:27,469 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:59:35,906 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:59:35,906 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T20:59:38,046 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:59:38,046 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T20:59:38,056 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:38,057 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]41552
2022-02-09T20:59:38,057 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:38,057 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:38,057 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:38,058 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:59:38,057 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:38,058 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T20:59:38,060 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407978060
2022-02-09T20:59:38,060 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T20:59:38,060 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407978060
2022-02-09T20:59:38,070 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:39,310 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:39,309 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:39,310 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:39,310 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:39,317 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:39,317 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:39,317 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:39,317 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:39,317 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:39,317 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:39,319 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:39,318 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:39,319 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:39,320 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:39,319 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:39,320 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:39,320 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:39,320 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:39,321 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:39,322 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:59:39,322 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T20:59:39,322 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:59:39,322 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:39,322 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T20:59:39,323 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 34 seconds.
2022-02-09T20:59:39,323 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:39,323 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:59:39,323 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 34 seconds.
2022-02-09T20:59:39,323 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T20:59:39,354 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:59:39,354 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T20:59:40,477 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:40,477 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]41280
2022-02-09T20:59:40,481 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:40,481 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:40,481 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:40,481 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:59:40,481 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:40,481 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T20:59:40,483 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407980483
2022-02-09T20:59:40,483 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407980483
2022-02-09T20:59:40,483 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T20:59:40,484 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:40,928 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:59:40,928 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T20:59:42,066 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:42,065 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:42,066 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:42,071 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:42,085 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:42,085 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:42,085 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:42,085 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:42,085 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:42,085 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:42,085 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:42,086 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:42,088 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:42,087 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:42,088 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:42,088 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:42,088 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:42,088 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:42,088 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:42,090 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:42,091 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:59:42,091 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T20:59:42,091 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:59:42,091 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:42,091 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T20:59:42,092 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.
2022-02-09T20:59:42,092 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:59:42,092 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:59:42,092 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.
2022-02-09T20:59:42,092 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T20:59:42,128 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:59:42,128 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T20:59:44,783 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:44,784 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]44416
2022-02-09T20:59:44,789 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:44,789 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:44,789 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:44,790 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:59:44,789 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:44,790 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T20:59:44,792 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407984792
2022-02-09T20:59:44,792 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407984792
2022-02-09T20:59:44,792 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T20:59:44,794 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:45,885 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:59:45,885 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T20:59:46,484 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:46,485 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:46,485 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:46,491 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:46,485 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:46,491 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:46,492 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:46,492 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:46,492 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:46,492 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:46,494 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:46,492 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:46,494 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:46,494 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:46,494 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:46,494 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:46,494 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:46,495 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:46,495 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:46,496 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:46,496 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:46,496 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:59:46,496 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T20:59:46,497 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:59:46,496 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:59:46,497 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T20:59:46,497 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.
2022-02-09T20:59:46,497 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:59:46,498 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:59:46,497 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.
2022-02-09T20:59:46,498 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T20:59:46,519 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:59:46,519 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T20:59:47,600 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:59:47,600 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T20:59:48,028 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:59:48,028 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T20:59:48,381 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:59:48,381 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T20:59:48,459 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:59:48,459 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T20:59:48,578 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:48,579 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]40892
2022-02-09T20:59:48,580 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:48,579 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:48,580 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:48,580 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:59:48,580 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:48,580 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T20:59:48,583 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407988583
2022-02-09T20:59:48,583 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407988583
2022-02-09T20:59:48,583 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T20:59:48,591 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:50,543 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:50,542 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:50,543 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:50,544 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:50,551 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:50,551 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:50,551 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:50,552 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:50,552 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:50,552 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:50,553 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:50,555 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:50,556 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:50,555 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:50,556 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:50,556 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:50,556 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:50,556 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:50,557 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:50,558 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:50,559 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:50,560 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:59:50,560 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:59:50,560 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T20:59:50,561 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:59:50,561 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:59:50,561 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T20:59:50,562 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 34 seconds.
2022-02-09T20:59:50,561 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:59:50,562 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:59:50,562 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 34 seconds.
2022-02-09T20:59:50,562 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T20:59:50,586 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:59:50,586 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T20:59:51,341 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:51,342 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]36640
2022-02-09T20:59:51,342 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:51,342 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:51,342 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:51,342 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:51,343 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:59:51,343 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T20:59:51,346 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407991346
2022-02-09T20:59:51,346 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407991346
2022-02-09T20:59:51,346 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T20:59:51,396 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:51,768 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:51,769 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]34908
2022-02-09T20:59:51,774 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:51,774 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:51,774 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:51,775 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:59:51,774 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:51,775 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T20:59:51,777 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407991777
2022-02-09T20:59:51,777 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407991777
2022-02-09T20:59:51,777 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T20:59:51,779 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:52,186 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:52,187 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]45976
2022-02-09T20:59:52,191 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:52,191 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:52,191 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:52,191 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:59:52,191 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:52,191 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T20:59:52,193 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407992193
2022-02-09T20:59:52,193 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407992193
2022-02-09T20:59:52,193 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T20:59:52,194 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:52,301 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T20:59:52,301 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]40612
2022-02-09T20:59:52,306 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:52,306 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T20:59:52,306 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T20:59:52,306 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T20:59:52,306 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:59:52,306 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T20:59:52,307 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407992307
2022-02-09T20:59:52,307 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644407992307
2022-02-09T20:59:52,308 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T20:59:52,309 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T20:59:52,659 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:52,660 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:52,660 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:52,660 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:52,667 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:52,667 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:52,667 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:52,668 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:52,668 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:52,668 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:52,668 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:52,668 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:52,670 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:52,670 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:52,670 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:52,671 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:52,670 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:52,671 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:52,671 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:52,672 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:52,672 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:52,672 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:59:52,673 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:59:52,673 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:59:52,673 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T20:59:52,674 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:59:52,674 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:59:52,674 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T20:59:52,674 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 34 seconds.
2022-02-09T20:59:52,674 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T20:59:52,675 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:59:52,674 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 34 seconds.
2022-02-09T20:59:52,675 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T20:59:52,700 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:59:52,700 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T20:59:53,136 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:53,136 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:53,136 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:53,143 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:53,136 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:53,143 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:53,143 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:53,143 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:53,143 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:53,143 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:53,146 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:53,146 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:53,144 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:53,147 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:53,147 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:53,147 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:53,148 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:53,149 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:53,150 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:59:53,149 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:53,150 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T20:59:53,151 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:59:53,150 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:53,151 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T20:59:53,152 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 34 seconds.
2022-02-09T20:59:53,151 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:53,152 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:59:53,152 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 34 seconds.
2022-02-09T20:59:53,152 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T20:59:53,170 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:59:53,170 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T20:59:53,468 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:53,468 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:53,468 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:53,477 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:53,477 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:53,477 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:53,477 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:53,478 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:53,478 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:53,478 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:53,478 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:53,479 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:53,481 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:53,481 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:53,481 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:53,482 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:53,481 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:53,482 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:53,482 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:53,482 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:53,483 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:53,483 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:59:53,484 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:59:53,484 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:59:53,484 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T20:59:53,484 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T20:59:53,485 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:59:53,485 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T20:59:53,485 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T20:59:53,486 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.
2022-02-09T20:59:53,485 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn = lambda ctx: entry_point(None, ctx)
2022-02-09T20:59:53,486 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:59:53,486 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.
2022-02-09T20:59:53,486 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T20:59:53,501 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:59:53,501 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T20:59:53,591 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T20:59:53,591 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:53,591 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T20:59:53,591 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T20:59:53,596 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:53,596 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T20:59:53,596 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T20:59:53,596 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:53,596 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T20:59:53,596 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T20:59:53,598 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:53,598 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T20:59:53,598 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:53,598 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T20:59:53,598 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T20:59:53,598 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T20:59:53,598 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T20:59:53,599 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T20:59:53,599 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T20:59:53,599 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T20:59:53,600 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:59:53,600 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T20:59:53,600 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T20:59:53,600 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:59:53,600 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T20:59:53,600 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T20:59:53,601 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-02-09T20:59:53,601 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T20:59:53,601 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:59:53,601 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2022-02-09T20:59:53,601 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T20:59:53,616 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T20:59:53,616 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T21:00:13,339 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T21:00:13,339 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T21:00:15,337 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:00:15,337 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]39524
2022-02-09T21:00:15,342 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:15,342 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:00:15,342 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:15,342 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T21:00:15,342 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:00:15,342 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T21:00:15,344 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408015344
2022-02-09T21:00:15,344 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408015344
2022-02-09T21:00:15,344 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T21:00:15,344 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:00:16,099 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T21:00:16,099 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T21:00:16,202 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:00:16,202 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:16,202 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:16,213 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:16,202 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:00:16,213 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:16,213 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:00:16,213 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:16,213 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:00:16,213 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:16,215 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:16,214 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:00:16,215 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:16,216 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:16,215 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:00:16,216 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:16,216 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:00:16,216 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:00:16,218 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T21:00:16,216 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:00:16,218 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T21:00:16,218 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T21:00:16,218 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:00:16,218 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T21:00:16,219 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 55 seconds.
2022-02-09T21:00:16,218 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:00:16,219 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T21:00:16,219 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 55 seconds.
2022-02-09T21:00:16,219 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T21:00:16,234 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T21:00:16,234 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T21:00:18,219 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:00:18,220 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]42600
2022-02-09T21:00:18,223 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:18,223 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:00:18,223 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:18,224 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T21:00:18,223 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:00:18,224 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T21:00:18,225 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408018225
2022-02-09T21:00:18,225 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408018225
2022-02-09T21:00:18,225 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T21:00:18,226 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:00:20,196 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:20,196 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:00:20,196 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:20,208 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:20,208 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:00:20,208 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:20,209 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:00:20,210 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:00:20,210 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:20,210 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:00:20,210 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:20,211 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:00:20,213 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:20,213 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:00:20,213 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:00:20,213 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:20,214 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:20,213 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:00:20,214 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:20,214 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:00:20,214 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:00:20,214 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T21:00:20,215 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T21:00:20,215 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T21:00:20,216 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T21:00:20,216 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T21:00:20,216 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn = lambda ctx: entry_point(None, ctx)
2022-02-09T21:00:20,216 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T21:00:20,243 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T21:00:20,218 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57\handler.py", line 123, in handle
2022-02-09T21:00:20,243 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T21:00:20,244 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 55 seconds.
2022-02-09T21:00:20,243 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     raise e
2022-02-09T21:00:20,244 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T21:00:20,244 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 55 seconds.
2022-02-09T21:00:20,244 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T21:00:20,257 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T21:00:20,257 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T21:00:20,504 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T21:00:20,504 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T21:00:22,775 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:00:22,775 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]44684
2022-02-09T21:00:22,779 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:22,779 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:00:22,779 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:22,779 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:00:22,780 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T21:00:22,780 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T21:00:22,781 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408022781
2022-02-09T21:00:22,781 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408022781
2022-02-09T21:00:22,782 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T21:00:22,783 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:00:23,604 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:00:23,605 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:23,605 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:00:23,605 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:23,614 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:23,614 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:00:23,614 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:23,614 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:00:23,614 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:23,614 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:00:23,614 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:23,615 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:00:23,616 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:23,616 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:23,616 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:23,616 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:00:23,616 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:23,617 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:00:23,617 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:00:23,619 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T21:00:23,619 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T21:00:23,619 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T21:00:23,619 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:00:23,619 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T21:00:23,620 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 55 seconds.
2022-02-09T21:00:23,619 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:00:23,620 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T21:00:23,620 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 55 seconds.
2022-02-09T21:00:23,620 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T21:00:23,634 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T21:00:23,634 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T21:00:24,569 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T21:00:24,569 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T21:00:26,678 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T21:00:26,678 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T21:00:27,161 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T21:00:27,161 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T21:00:27,296 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:00:27,297 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]46652
2022-02-09T21:00:27,302 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:27,301 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:00:27,302 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:27,302 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:00:27,302 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T21:00:27,302 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T21:00:27,304 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408027304
2022-02-09T21:00:27,304 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408027304
2022-02-09T21:00:27,305 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T21:00:27,306 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:00:27,494 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T21:00:27,494 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T21:00:27,625 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T21:00:27,625 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T21:00:28,734 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:28,734 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:00:28,734 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:28,742 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:28,736 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:00:28,742 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:28,743 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:00:28,743 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:28,743 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:00:28,743 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:28,744 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:00:28,745 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:28,745 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:00:28,745 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:28,746 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:28,745 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:00:28,746 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:28,746 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:00:28,746 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:00:28,747 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:00:28,747 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:00:28,748 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T21:00:28,748 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T21:00:28,749 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T21:00:28,749 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T21:00:28,749 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T21:00:28,750 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T21:00:28,749 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T21:00:28,750 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T21:00:28,751 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 55 seconds.
2022-02-09T21:00:28,750 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn = lambda ctx: entry_point(None, ctx)
2022-02-09T21:00:28,751 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T21:00:28,751 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 55 seconds.
2022-02-09T21:00:28,751 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T21:00:28,774 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T21:00:28,774 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T21:00:29,758 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:00:29,759 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]47624
2022-02-09T21:00:29,764 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:29,764 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:00:29,764 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:29,765 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:00:29,765 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T21:00:29,765 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T21:00:29,767 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408029767
2022-02-09T21:00:29,767 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408029767
2022-02-09T21:00:29,767 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T21:00:29,768 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:00:30,228 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:00:30,233 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]43904
2022-02-09T21:00:30,233 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:30,233 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:00:30,233 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:30,234 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T21:00:30,233 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:00:30,234 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T21:00:30,237 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408030236
2022-02-09T21:00:30,237 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408030236
2022-02-09T21:00:30,237 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T21:00:30,238 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:00:30,597 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:00:30,598 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]31116
2022-02-09T21:00:30,604 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:30,604 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:00:30,604 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:30,604 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:00:30,605 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T21:00:30,605 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T21:00:30,607 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408030607
2022-02-09T21:00:30,607 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408030607
2022-02-09T21:00:30,607 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T21:00:30,609 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:00:30,651 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:00:30,654 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]28564
2022-02-09T21:00:30,660 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:30,660 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:00:30,660 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:00:30,661 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T21:00:30,660 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:00:30,661 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T21:00:30,663 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408030663
2022-02-09T21:00:30,663 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408030663
2022-02-09T21:00:30,663 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T21:00:30,664 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:00:31,033 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:31,032 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:00:31,033 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:31,035 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:31,035 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:00:31,035 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:31,035 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:00:31,036 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:31,036 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:00:31,036 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:31,038 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:31,036 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:00:31,038 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:31,038 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:31,038 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:00:31,038 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:31,038 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:00:31,039 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:00:31,040 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T21:00:31,068 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T21:00:31,039 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:00:31,068 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T21:00:31,040 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T21:00:31,078 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T21:00:31,077 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:00:31,078 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T21:00:31,079 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 55 seconds.
2022-02-09T21:00:31,078 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:00:31,079 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T21:00:31,079 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 55 seconds.
2022-02-09T21:00:31,079 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T21:00:31,476 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:00:31,477 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:31,477 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:31,483 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:31,477 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:00:31,483 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:31,483 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:00:31,483 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:31,484 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:00:31,483 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:31,486 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:31,484 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:00:31,486 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:31,486 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:31,486 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:00:31,486 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:31,486 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:00:31,487 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:00:31,487 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:00:31,487 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:00:31,488 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T21:00:31,488 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:00:31,488 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T21:00:31,489 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T21:00:31,488 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T21:00:31,489 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T21:00:31,489 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 55 seconds.
2022-02-09T21:00:31,489 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T21:00:31,490 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T21:00:31,489 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 55 seconds.
2022-02-09T21:00:31,490 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T21:00:31,511 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T21:00:31,511 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T21:00:31,772 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:31,772 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:00:31,772 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:31,773 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:00:31,779 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:31,779 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:31,779 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:00:31,779 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:31,779 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:00:31,779 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:31,781 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:31,780 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:00:31,781 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:31,781 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:31,781 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:00:31,781 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:31,781 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:00:31,782 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:00:31,782 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:00:31,782 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:00:31,782 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:00:31,783 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T21:00:31,783 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T21:00:31,783 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T21:00:31,784 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T21:00:31,783 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T21:00:31,784 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T21:00:31,784 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 55 seconds.
2022-02-09T21:00:31,784 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T21:00:31,784 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T21:00:31,784 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 55 seconds.
2022-02-09T21:00:31,784 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T21:00:31,813 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:31,814 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T21:00:31,812 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:00:31,814 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T21:00:31,813 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T21:00:31,820 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:00:31,821 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:00:31,822 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:31,821 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:00:31,822 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:00:31,822 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:31,822 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:00:31,822 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:00:31,826 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:31,823 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:00:31,826 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:00:31,828 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:31,828 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:00:31,827 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:00:31,830 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:00:31,831 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:00:31,832 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T21:00:31,832 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T21:00:31,832 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T21:00:31,832 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:00:31,832 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T21:00:31,833 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-02-09T21:00:31,833 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:00:31,834 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T21:00:31,833 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2022-02-09T21:00:31,834 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T21:00:31,856 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T21:00:31,856 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T21:01:11,231 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T21:01:11,231 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T21:01:13,231 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:01:13,231 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]34552
2022-02-09T21:01:13,235 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:13,235 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:01:13,235 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:13,235 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T21:01:13,235 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:01:13,235 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T21:01:13,237 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408073237
2022-02-09T21:01:13,237 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408073237
2022-02-09T21:01:13,237 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T21:01:13,237 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:01:13,988 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:01:13,988 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:13,988 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:13,988 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:01:13,995 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:13,995 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:01:13,995 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:13,996 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:01:13,996 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:13,996 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:01:13,996 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:13,997 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:01:13,998 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:13,998 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:13,999 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:13,998 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:01:13,999 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:13,999 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:01:14,000 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:01:14,000 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:01:14,000 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:01:14,001 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T21:01:14,001 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T21:01:14,001 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T21:01:14,001 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T21:01:14,001 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T21:01:14,002 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T21:01:14,002 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T21:01:14,002 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T21:01:14,003 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 89 seconds.
2022-02-09T21:01:14,002 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn = lambda ctx: entry_point(None, ctx)
2022-02-09T21:01:14,003 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T21:01:14,003 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 89 seconds.
2022-02-09T21:01:14,003 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T21:01:14,024 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T21:01:14,024 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T21:01:15,255 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T21:01:15,255 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T21:01:17,143 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:01:17,149 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]34388
2022-02-09T21:01:17,150 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:17,149 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:01:17,150 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:17,150 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T21:01:17,150 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:01:17,150 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T21:01:17,152 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408077152
2022-02-09T21:01:17,152 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408077152
2022-02-09T21:01:17,152 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T21:01:17,153 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:01:17,996 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:01:17,997 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:17,997 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:17,997 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:17,997 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:01:17,997 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:17,998 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:01:17,998 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:17,998 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:01:17,998 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:17,999 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:17,998 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:01:17,999 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:18,000 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:18,000 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:01:18,000 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:18,000 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:01:18,001 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:01:18,001 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:01:18,002 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T21:01:18,002 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:01:18,002 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T21:01:18,004 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T21:01:18,004 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:01:18,004 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T21:01:18,005 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 89 seconds.
2022-02-09T21:01:18,005 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T21:01:18,005 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T21:01:18,005 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 89 seconds.
2022-02-09T21:01:18,005 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T21:01:18,016 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T21:01:18,016 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T21:01:18,626 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T21:01:18,626 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T21:01:20,655 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:01:20,656 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]17572
2022-02-09T21:01:20,656 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:20,656 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:01:20,656 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:20,656 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T21:01:20,656 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:01:20,656 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T21:01:20,658 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408080658
2022-02-09T21:01:20,658 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408080658
2022-02-09T21:01:20,659 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T21:01:20,664 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:01:21,433 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:01:21,433 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:21,433 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:21,433 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:01:21,438 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:21,438 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:21,438 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:01:21,439 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:21,439 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:01:21,439 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:21,440 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:21,439 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:01:21,440 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:21,440 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:01:21,441 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:21,441 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:21,441 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:01:21,441 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:01:21,441 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:01:21,442 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:01:21,442 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T21:01:21,442 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:01:21,442 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T21:01:21,443 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T21:01:21,443 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T21:01:21,443 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T21:01:21,444 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 89 seconds.
2022-02-09T21:01:21,443 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T21:01:21,444 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T21:01:21,444 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 89 seconds.
2022-02-09T21:01:21,444 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T21:01:21,463 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T21:01:21,463 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T21:01:23,764 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T21:01:23,764 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T21:01:25,716 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:01:25,717 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]45900
2022-02-09T21:01:25,721 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:25,721 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:01:25,721 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:25,721 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T21:01:25,721 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:01:25,721 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T21:01:25,723 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408085723
2022-02-09T21:01:25,723 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408085723
2022-02-09T21:01:25,724 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T21:01:25,724 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:01:26,095 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T21:01:26,095 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T21:01:26,506 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T21:01:26,506 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T21:01:26,632 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:26,631 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:01:26,632 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:26,639 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:01:26,640 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:26,640 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:01:26,641 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:01:26,641 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:01:26,640 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:26,642 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:01:26,642 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:26,643 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:01:26,642 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:26,646 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:26,643 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:01:26,646 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:26,667 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:26,667 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:01:26,667 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:26,668 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:01:26,668 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:01:26,669 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T21:01:26,670 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T21:01:26,670 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T21:01:26,670 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T21:01:26,695 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T21:01:26,695 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T21:01:26,695 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T21:01:26,696 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 89 seconds.
2022-02-09T21:01:26,696 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T21:01:26,696 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T21:01:26,696 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 89 seconds.
2022-02-09T21:01:26,696 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T21:01:26,700 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T21:01:26,700 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T21:01:26,793 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T21:01:26,793 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T21:01:26,842 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T21:01:26,842 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T21:01:28,934 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:01:28,935 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]22832
2022-02-09T21:01:28,940 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:28,940 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:01:28,940 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:28,940 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T21:01:28,940 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:01:28,940 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T21:01:28,943 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408088943
2022-02-09T21:01:28,943 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T21:01:28,943 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408088943
2022-02-09T21:01:28,946 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:01:29,498 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:01:29,499 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]35652
2022-02-09T21:01:29,504 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:29,504 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:01:29,504 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:29,505 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T21:01:29,504 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:01:29,505 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T21:01:29,507 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408089507
2022-02-09T21:01:29,507 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T21:01:29,507 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408089507
2022-02-09T21:01:29,509 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:01:29,765 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:01:29,766 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]44672
2022-02-09T21:01:29,766 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:29,766 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:01:29,766 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:01:29,766 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:01:29,767 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]38916
2022-02-09T21:01:29,766 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:29,767 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:29,767 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T21:01:29,767 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:01:29,767 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T21:01:29,768 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:01:29,767 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:01:29,768 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T21:01:29,768 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T21:01:29,769 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408089769
2022-02-09T21:01:29,769 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T21:01:29,770 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408089770
2022-02-09T21:01:29,769 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408089769
2022-02-09T21:01:29,770 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408089770
2022-02-09T21:01:29,770 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T21:01:29,771 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:01:29,772 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:01:30,233 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:30,233 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:01:30,233 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:30,234 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:01:30,241 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:30,241 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:01:30,241 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:30,241 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:01:30,241 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:30,242 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:01:30,241 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:30,242 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:01:30,243 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:30,243 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:30,244 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:30,243 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:01:30,244 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:30,244 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:01:30,245 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:01:30,245 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:01:30,246 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:01:30,246 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T21:01:30,246 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T21:01:30,246 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T21:01:30,247 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T21:01:30,246 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T21:01:30,247 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T21:01:30,248 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 89 seconds.
2022-02-09T21:01:30,247 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T21:01:30,248 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T21:01:30,248 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 89 seconds.
2022-02-09T21:01:30,248 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T21:01:30,271 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T21:01:30,271 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T21:01:30,706 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:01:30,707 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:30,707 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:01:30,707 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:30,715 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:01:30,715 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:30,715 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:01:30,715 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:30,716 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:01:30,716 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:30,716 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:01:30,716 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:30,718 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:30,717 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:01:30,718 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:30,718 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:30,718 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:01:30,718 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:30,719 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:01:30,720 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:01:30,720 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:01:30,721 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T21:01:30,721 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T21:01:30,721 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T21:01:30,721 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T21:01:30,721 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T21:01:30,722 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T21:01:30,722 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T21:01:30,723 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 89 seconds.
2022-02-09T21:01:30,722 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T21:01:30,723 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T21:01:30,723 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 89 seconds.
2022-02-09T21:01:30,723 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T21:01:30,742 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T21:01:30,742 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T21:01:30,912 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:01:30,913 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:30,913 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:01:30,913 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:30,918 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:30,918 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:01:30,918 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:30,918 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:01:30,919 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:30,919 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:01:30,919 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:30,919 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:01:30,920 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:30,920 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:01:30,920 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:30,921 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:30,921 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:01:30,921 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:30,921 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:01:30,922 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:01:30,922 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:01:30,922 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T21:01:30,923 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T21:01:30,923 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T21:01:30,923 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T21:01:30,923 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T21:01:30,923 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T21:01:30,923 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T21:01:30,924 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2022-02-09T21:01:30,924 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T21:01:30,924 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T21:01:30,924 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2022-02-09T21:01:30,924 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T21:01:30,936 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:30,937 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T21:01:30,935 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:01:30,937 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T21:01:30,936 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T21:01:30,962 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:30,962 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:01:30,962 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:01:30,962 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:01:30,962 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:30,962 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:01:30,964 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:30,962 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:01:30,964 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:01:30,964 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:30,964 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:01:30,964 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:01:30,964 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:01:30,964 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:01:30,965 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:01:30,965 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:01:30,965 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:01:30,966 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T21:01:30,966 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:01:30,966 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T21:01:30,966 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T21:01:30,966 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T21:01:30,966 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T21:01:30,967 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 89 seconds.
2022-02-09T21:01:30,966 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T21:01:30,967 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T21:01:30,967 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 89 seconds.
2022-02-09T21:01:30,967 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T21:01:30,984 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T21:01:30,984 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T21:02:43,011 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T21:02:43,011 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T21:02:45,431 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:02:45,431 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]42932
2022-02-09T21:02:45,435 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:02:45,435 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:02:45,435 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:02:45,435 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:02:45,435 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T21:02:45,435 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T21:02:45,437 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408165437
2022-02-09T21:02:45,437 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644408165437
2022-02-09T21:02:45,437 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T21:02:45,438 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:02:46,254 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T21:02:46,253 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\75f0adf03c4e42548284a6611c731b57', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:02:46,254 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T21:02:46,266 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:02:46,265 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\75f0adf03c4e42548284a6611c731b57
2022-02-09T21:02:46,266 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:02:46,266 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:02:46,266 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:02:46,266 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:02:46,268 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:02:46,266 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:02:46,268 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:02:46,268 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:02:46,268 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:02:46,268 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:02:46,269 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:02:46,269 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:02:46,270 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T21:02:46,283 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T21:02:46,269 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:02:46,283 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T21:02:46,270 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T21:02:46,306 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T21:02:46,306 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:02:46,306 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T21:02:46,307 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 144 seconds.
2022-02-09T21:02:46,307 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:02:46,307 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T21:02:46,307 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 144 seconds.
2022-02-09T21:02:46,307 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T21:02:47,009 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T21:02:50,450 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T21:04:19,979 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-09T21:04:19,979 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2022-02-09T21:04:20,033 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-09T21:04:20,033 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2022-02-09T21:04:20,389 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages
Current directory: C:\Users\enoch9\Desktop\개발\sentencEmoji
Temp directory: C:\Users\enoch9\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4046 M
Python executable: c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe
Config file: logs\config\20220209205747206-startup.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Initial Models: sentencemoji=sentencemoji.mar
Log dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Metrics dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Model config: N/A
2022-02-09T21:04:20,389 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.5.2
TS Home: C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages
Current directory: C:\Users\enoch9\Desktop\개발\sentencEmoji
Temp directory: C:\Users\enoch9\AppData\Local\Temp
Number of GPUs: 0
Number of CPUs: 8
Max heap size: 4046 M
Python executable: c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe
Config file: logs\config\20220209205747206-startup.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Initial Models: sentencemoji=sentencemoji.mar
Log dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Metrics dir: C:\Users\enoch9\Desktop\개발\sentencEmoji\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 8
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: C:\Users\enoch9\Desktop\개발\sentencEmoji\model_store
Model config: N/A
2022-02-09T21:04:20,402 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220209205747206-startup.cfg",
  "modelCount": 1,
  "created": 1644407867207,
  "models": {
    "sentencemoji": {
      "1.1": {
        "defaultVersion": true,
        "marName": "sentencemoji.mar",
        "minWorkers": 8,
        "maxWorkers": 8,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-02-09T21:04:20,402 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20220209205747206-startup.cfg",
  "modelCount": 1,
  "created": 1644407867207,
  "models": {
    "sentencemoji": {
      "1.1": {
        "defaultVersion": true,
        "marName": "sentencemoji.mar",
        "minWorkers": 8,
        "maxWorkers": 8,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2022-02-09T21:04:20,411 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220209205747206-startup.cfg
2022-02-09T21:04:20,411 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20220209205747206-startup.cfg
2022-02-09T21:04:20,414 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220209205747206-startup.cfg validated successfully
2022-02-09T21:04:20,414 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20220209205747206-startup.cfg validated successfully
2022-02-09T21:04:40,155 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.1 for model sentencemoji
2022-02-09T21:04:40,155 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.1 for model sentencemoji
2022-02-09T21:59:04,609 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T21:59:04,609 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T21:59:04,612 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T21:59:04,612 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.1 for model sentencemoji
2022-02-09T21:59:04,613 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model sentencemoji loaded.
2022-02-09T21:59:04,613 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model sentencemoji loaded.
2022-02-09T21:59:04,615 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: sentencemoji, count: 8
2022-02-09T21:59:04,615 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: sentencemoji, count: 8
2022-02-09T21:59:04,688 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T21:59:04,688 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T21:59:04,688 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T21:59:04,688 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T21:59:04,688 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T21:59:04,688 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T21:59:04,688 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T21:59:04,688 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T21:59:04,688 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T21:59:04,688 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T21:59:04,688 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T21:59:04,688 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T21:59:04,693 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-02-09T21:59:04,688 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T21:59:04,688 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T21:59:04,688 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T21:59:04,693 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2022-02-09T21:59:04,688 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T21:59:05,233 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-09T21:59:05,233 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2022-02-09T21:59:05,233 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-02-09T21:59:05,233 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2022-02-09T21:59:05,236 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-09T21:59:05,236 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2022-02-09T21:59:05,236 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-02-09T21:59:05,236 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2022-02-09T21:59:05,238 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-09T21:59:05,238 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2022-02-09T21:59:06,114 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-09T21:59:06,114 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2022-02-09T21:59:06,952 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644411546
2022-02-09T21:59:06,953 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:14.066055297851562|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644411546
2022-02-09T21:59:06,954 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:218.77866744995117|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644411546
2022-02-09T21:59:06,955 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:94.0|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644411546
2022-02-09T21:59:06,956 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:3614.1640625|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644411546
2022-02-09T21:59:06,957 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:12563.87109375|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644411546
2022-02-09T21:59:06,957 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:77.7|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644411546
2022-02-09T21:59:08,602 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:08,602 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:08,604 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]6832
2022-02-09T21:59:08,605 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]30020
2022-02-09T21:59:08,605 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:08,605 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:08,605 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:08,606 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:08,607 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,607 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,607 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,607 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,617 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T21:59:08,617 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T21:59:08,617 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T21:59:08,617 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T21:59:08,624 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:08,627 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]47768
2022-02-09T21:59:08,627 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,627 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:08,627 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,628 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:08,628 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T21:59:08,628 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T21:59:08,644 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T21:59:08,644 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T21:59:08,644 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T21:59:08,647 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:08,648 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]39120
2022-02-09T21:59:08,648 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,648 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:08,648 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,650 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548650
2022-02-09T21:59:08,650 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548650
2022-02-09T21:59:08,650 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548649
2022-02-09T21:59:08,650 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T21:59:08,649 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:08,650 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T21:59:08,650 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548649
2022-02-09T21:59:08,650 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548650
2022-02-09T21:59:08,650 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548650
2022-02-09T21:59:08,655 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548655
2022-02-09T21:59:08,655 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548655
2022-02-09T21:59:08,658 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T21:59:08,720 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:08,721 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:08,722 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:08,722 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]45108
2022-02-09T21:59:08,726 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,721 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:08,721 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:08,726 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:08,726 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,737 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:08,742 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T21:59:08,742 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T21:59:08,760 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548759
2022-02-09T21:59:08,760 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T21:59:08,760 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548759
2022-02-09T21:59:08,786 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:08,808 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:08,810 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]40608
2022-02-09T21:59:08,812 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,812 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:08,813 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:08,812 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,816 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T21:59:08,816 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T21:59:08,822 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548821
2022-02-09T21:59:08,821 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T21:59:08,822 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548821
2022-02-09T21:59:08,847 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:08,878 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:08,880 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]34852
2022-02-09T21:59:08,880 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,880 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:08,880 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,882 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T21:59:08,881 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:08,882 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T21:59:08,887 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T21:59:08,888 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548888
2022-02-09T21:59:08,888 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548888
2022-02-09T21:59:08,938 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:08,948 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:08,950 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]35540
2022-02-09T21:59:08,951 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,950 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:08,951 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change null -> WORKER_STARTED
2022-02-09T21:59:08,957 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T21:59:08,952 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:08,957 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T21:59:08,962 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548962
2022-02-09T21:59:08,962 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411548962
2022-02-09T21:59:08,962 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T21:59:09,002 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:10,233 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:10,236 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,236 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,236 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,233 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:10,237 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:10,243 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,246 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,247 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,237 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:10,236 [INFO ] nioEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,255 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,236 [INFO ] nioEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,236 [INFO ] nioEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,257 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,257 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,263 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,255 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,254 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:10,246 [INFO ] nioEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,272 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,246 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:10,246 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:10,243 [INFO ] nioEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,277 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,242 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:10,239 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:10,238 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:10,286 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:10,279 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:10,289 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:10,276 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:10,274 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:10,271 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,247 [INFO ] nioEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,272 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,294 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,294 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,263 [INFO ] nioEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,302 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,262 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:10,257 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,305 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,302 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,257 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,306 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,306 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,294 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,294 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,309 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,309 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,271 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,312 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,292 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:10,271 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:10,291 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:10,277 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,290 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:10,315 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,287 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:10,280 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:10,315 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,319 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,314 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:10,313 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:10,313 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:10,312 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,324 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,309 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,327 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,309 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,338 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,306 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,342 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,350 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,305 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,375 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,303 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:10,350 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:10,377 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,349 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:10,342 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,381 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,338 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,306 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,382 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,327 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,384 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,384 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T21:59:10,324 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,322 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:10,387 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T21:59:10,321 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:10,320 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:10,315 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:10,317 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:10,316 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:10,415 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:59:10,319 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,419 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,430 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T21:59:10,387 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T21:59:10,452 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T21:59:10,387 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:59:10,384 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T21:59:10,467 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T21:59:10,384 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,382 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,470 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,470 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T21:59:10,381 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,379 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:10,473 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T21:59:10,375 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:10,375 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,476 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,473 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:10,377 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:10,477 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,470 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T21:59:10,478 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T21:59:10,470 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,467 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T21:59:10,481 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T21:59:10,482 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T21:59:10,452 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T21:59:10,496 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T21:59:10,430 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T21:59:10,496 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T21:59:10,460 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:59:10,499 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T21:59:10,419 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:59:10,505 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T21:59:10,506 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T21:59:10,419 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,517 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T21:59:10,415 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:59:10,418 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:59:10,526 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T21:59:10,526 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T21:59:10,540 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T21:59:10,417 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:59:10,416 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:59:10,540 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T21:59:10,526 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T21:59:10,526 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T21:59:10,506 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T21:59:10,503 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:59:10,499 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T21:59:10,498 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:59:10,496 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T21:59:10,496 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T21:59:10,482 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T21:59:10,481 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T21:59:10,582 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T21:59:10,478 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T21:59:10,584 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T21:59:10,477 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:10,586 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,477 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:10,476 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,475 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:10,473 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T21:59:10,589 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T21:59:10,589 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T21:59:10,587 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:10,584 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T21:59:10,602 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T21:59:10,582 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T21:59:10,609 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T21:59:10,573 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:59:10,570 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:59:10,517 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T21:59:10,505 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T21:59:10,612 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T21:59:10,612 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T21:59:10,549 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:59:10,548 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:59:10,612 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T21:59:10,612 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T21:59:10,616 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T21:59:10,612 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:59:10,609 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T21:59:10,602 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T21:59:10,591 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:59:10,586 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:10,620 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,589 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T21:59:10,621 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T21:59:10,589 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:10,623 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T21:59:10,589 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T21:59:10,633 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T21:59:10,621 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T21:59:10,634 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T21:59:10,620 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:10,620 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:59:10,636 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T21:59:10,617 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:59:10,637 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T21:59:10,616 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T21:59:10,614 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:59:10,639 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T21:59:10,613 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:59:10,640 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T21:59:10,637 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T21:59:10,636 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T21:59:10,642 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T21:59:10,636 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:59:10,634 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T21:59:10,633 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T21:59:10,623 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T21:59:10,643 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:59:10,642 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T21:59:10,648 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T21:59:10,649 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T21:59:10,640 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T21:59:10,639 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T21:59:10,649 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T21:59:10,648 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T21:59:10,648 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:59:10,670 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T21:59:10,670 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T21:59:11,495 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T21:59:11,495 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T21:59:11,510 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T21:59:11,510 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T21:59:11,587 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T21:59:11,587 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T21:59:11,618 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T21:59:11,618 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T21:59:11,618 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T21:59:11,618 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T21:59:11,651 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T21:59:11,651 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T21:59:11,651 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T21:59:11,651 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T21:59:11,651 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T21:59:11,651 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T21:59:14,647 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:14,649 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - [PID]19552
2022-02-09T21:59:14,652 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:14,652 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:14,653 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:14,652 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:14,657 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T21:59:14,657 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9004
2022-02-09T21:59:14,662 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411554662
2022-02-09T21:59:14,662 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9004).
2022-02-09T21:59:14,662 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411554662
2022-02-09T21:59:14,699 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:14,708 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:14,709 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]41108
2022-02-09T21:59:14,710 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:14,710 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:14,710 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:14,713 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T21:59:14,711 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:14,713 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9005
2022-02-09T21:59:14,718 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411554718
2022-02-09T21:59:14,718 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411554718
2022-02-09T21:59:14,718 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9005).
2022-02-09T21:59:14,758 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:14,799 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:14,799 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]29480
2022-02-09T21:59:14,804 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:14,803 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:14,805 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:14,804 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:14,808 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T21:59:14,808 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2022-02-09T21:59:14,812 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411554812
2022-02-09T21:59:14,812 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2022-02-09T21:59:14,812 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411554812
2022-02-09T21:59:14,850 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:14,849 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:14,883 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]1408
2022-02-09T21:59:14,887 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:14,887 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:14,889 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T21:59:14,888 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:14,889 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9002
2022-02-09T21:59:14,890 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:14,894 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411554894
2022-02-09T21:59:14,894 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9002).
2022-02-09T21:59:14,894 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411554894
2022-02-09T21:59:14,902 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:14,899 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:14,906 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]39416
2022-02-09T21:59:14,912 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - [PID]45060
2022-02-09T21:59:14,912 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:14,914 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:14,915 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:14,914 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:14,915 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:14,920 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T21:59:14,914 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:14,920 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9003
2022-02-09T21:59:14,921 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:14,914 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:14,924 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T21:59:14,924 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9007
2022-02-09T21:59:14,943 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9003).
2022-02-09T21:59:14,945 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411554944
2022-02-09T21:59:14,944 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:14,945 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411554944
2022-02-09T21:59:14,958 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9007).
2022-02-09T21:59:14,963 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411554963
2022-02-09T21:59:14,963 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411554963
2022-02-09T21:59:14,977 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:14,978 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:14,978 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]16904
2022-02-09T21:59:14,981 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:14,982 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:14,984 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:14,984 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:14,986 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T21:59:14,986 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9006
2022-02-09T21:59:14,991 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411554991
2022-02-09T21:59:14,990 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:14,991 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411554991
2022-02-09T21:59:14,991 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9006).
2022-02-09T21:59:15,030 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:15,080 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:15,081 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]41876
2022-02-09T21:59:15,085 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:15,085 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T21:59:15,085 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T21:59:15,088 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T21:59:15,086 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T21:59:15,088 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T21:59:15,093 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411555093
2022-02-09T21:59:15,093 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644411555093
2022-02-09T21:59:15,093 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T21:59:15,118 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T21:59:16,000 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:16,000 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,000 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:16,000 [INFO ] nioEventLoopGroup-5-9 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,006 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,004 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:16,006 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,008 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,007 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:16,010 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:16,011 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:59:16,012 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:59:16,014 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:59:16,008 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,020 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:59:16,027 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,027 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,028 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:59:16,040 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:59:16,042 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,041 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     batch_size, envelope, limit_max_image_pixels)
2022-02-09T21:59:16,042 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,050 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 112, in load
2022-02-09T21:59:16,080 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T21:59:16,079 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn(service.context)
2022-02-09T21:59:16,083 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\model_loader.py", line 139, in <lambda>
2022-02-09T21:59:16,084 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     initialize_fn = lambda ctx: entry_point(None, ctx)
2022-02-09T21:59:16,085 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414\handler.py", line 123, in handle
2022-02-09T21:59:16,087 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     raise e
2022-02-09T21:59:16,080 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T21:59:16,097 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T21:59:16,097 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414\handler.py", line 112, in handle
2022-02-09T21:59:16,097 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     _service.initialize(context)
2022-02-09T21:59:16,098 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414\handler.py", line 37, in initialize
2022-02-09T21:59:16,097 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T21:59:16,102 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T21:59:16,101 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG -     self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)
2022-02-09T21:59:16,102 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T21:59:16,102 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T21:59:16,107 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T21:59:16,102 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.
2022-02-09T21:59:16,107 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T21:59:16,143 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:16,144 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,144 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:16,144 [INFO ] nioEventLoopGroup-5-10 org.pytorch.serve.wlm.WorkerThread - 9005 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,149 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,148 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:16,149 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,152 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,151 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:16,152 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,156 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,154 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:16,156 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,158 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,157 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:59:16,158 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,159 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:59:16,162 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T21:59:16,162 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:59:16,162 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T21:59:16,165 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T21:59:16,163 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:59:16,165 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T21:59:16,167 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T21:59:16,167 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:59:16,169 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T21:59:16,167 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 1 seconds.
2022-02-09T21:59:16,169 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T21:59:16,182 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:16,183 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,184 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T21:59:16,183 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:16,184 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T21:59:16,183 [INFO ] nioEventLoopGroup-5-11 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,185 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:16,187 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,187 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,187 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:16,190 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,190 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:16,190 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,206 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,191 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:59:16,206 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,214 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:59:16,217 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,217 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,217 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:59:16,220 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T21:59:16,220 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:59:16,220 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T21:59:16,222 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T21:59:16,222 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:59:16,222 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T21:59:16,225 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T21:59:16,224 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T21:59:16,226 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T21:59:16,225 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2022-02-09T21:59:16,226 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T21:59:16,237 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T21:59:16,237 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T21:59:16,249 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:16,250 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,252 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,250 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:16,252 [INFO ] nioEventLoopGroup-5-12 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,254 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,251 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:16,250 [INFO ] nioEventLoopGroup-5-14 org.pytorch.serve.wlm.WorkerThread - 9007 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,256 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,254 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,257 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,252 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:16,255 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:16,257 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,262 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,259 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:16,256 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,263 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,262 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,275 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,260 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:16,263 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,278 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,264 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:16,276 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:16,275 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,280 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:59:16,284 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T21:59:16,278 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,285 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,281 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:16,284 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T21:59:16,287 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T21:59:16,284 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:59:16,286 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:59:16,285 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,289 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:59:16,293 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T21:59:16,287 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T21:59:16,294 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T21:59:16,302 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T21:59:16,290 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:59:16,306 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T21:59:16,293 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T21:59:16,307 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T21:59:16,302 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T21:59:16,294 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.
2022-02-09T21:59:16,313 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T21:59:16,307 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T21:59:16,315 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T21:59:16,292 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:59:16,306 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T21:59:16,332 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T21:59:16,315 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 1 seconds.
2022-02-09T21:59:16,340 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,313 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T21:59:16,339 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:16,340 [INFO ] nioEventLoopGroup-5-15 org.pytorch.serve.wlm.WorkerThread - 9006 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,344 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,332 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T21:59:16,343 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:16,344 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,347 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,346 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:16,347 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,351 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,348 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:16,351 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,353 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,351 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:16,353 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,354 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:59:16,357 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T21:59:16,356 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:59:16,357 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T21:59:16,368 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T21:59:16,358 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:59:16,371 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T21:59:16,368 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T21:59:16,377 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T21:59:16,371 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T21:59:16,369 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:59:16,382 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T21:59:16,377 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 1 seconds.
2022-02-09T21:59:16,382 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T21:59:16,385 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:16,386 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,386 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:16,386 [INFO ] nioEventLoopGroup-5-13 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,388 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,387 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:16,389 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:16,388 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,403 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,401 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:16,403 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,406 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,404 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:59:16,406 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,412 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,412 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,411 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T21:59:16,411 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T21:59:16,412 [INFO ] nioEventLoopGroup-5-16 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T21:59:16,415 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,412 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,414 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T21:59:16,418 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T21:59:16,413 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T21:59:16,415 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T21:59:16,419 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,418 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T21:59:16,421 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T21:59:16,417 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T21:59:16,419 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T21:59:16,428 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,418 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T21:59:16,429 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T21:59:16,421 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T21:59:16,437 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T21:59:16,428 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T21:59:16,445 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,430 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T21:59:16,426 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T21:59:16,446 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T21:59:16,445 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T21:59:16,449 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T21:59:16,437 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.
2022-02-09T21:59:16,429 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T21:59:16,449 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T21:59:16,451 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T21:59:16,446 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T21:59:16,446 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T21:59:16,451 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T21:59:16,458 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T21:59:16,457 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T21:59:16,460 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T21:59:16,460 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T21:59:16,458 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2022-02-09T21:59:16,460 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T21:59:16,460 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T21:59:17,111 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T21:59:17,111 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T21:59:17,175 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T21:59:17,175 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T21:59:17,238 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T21:59:17,238 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T21:59:17,299 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T21:59:17,299 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T21:59:17,330 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T21:59:17,330 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T21:59:17,393 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T21:59:17,393 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T21:59:17,445 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T21:59:17,445 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T21:59:17,474 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T21:59:50,686 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T21:59:50,872 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T21:59:50,901 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T21:59:50,941 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T21:59:50,972 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T21:59:51,004 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T21:59:51,026 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T22:01:17,138 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T22:01:17,199 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T22:01:17,269 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T22:01:17,319 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T22:01:17,366 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T22:01:17,427 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T22:01:17,458 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T22:01:17,199 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T22:01:17,366 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T21:59:17,474 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9001]
2022-02-09T22:01:17,269 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T22:00:06,502 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644411606
2022-02-09T21:59:50,972 [INFO ] W-9007-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stderr
2022-02-09T21:59:51,004 [INFO ] W-9006-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stderr
2022-02-09T21:59:50,901 [INFO ] W-9002-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stderr
2022-02-09T23:39:26,418 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T23:39:26,419 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T23:39:26,418 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T21:59:50,872 [INFO ] W-9005-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stderr
2022-02-09T23:39:26,419 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T21:59:20,123 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:20,131 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:20,063 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:20,023 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T21:59:20,101 [INFO ] W-9007-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T23:39:26,445 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T21:59:19,989 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T23:39:26,425 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:14.066993713378906|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644411606
2022-02-09T23:39:26,418 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T23:39:26,418 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T21:59:51,026 [INFO ] W-9003-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stderr
2022-02-09T22:01:17,138 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T23:39:26,449 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T21:59:50,941 [INFO ] W-9000-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stderr
2022-02-09T22:01:17,319 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T22:01:17,458 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T23:39:26,449 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T21:59:50,686 [INFO ] W-9004-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stderr
2022-02-09T23:39:26,449 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T23:39:26,449 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T22:01:17,427 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T23:39:26,450 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T23:39:26,450 [ERROR] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker startup time out.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:143) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:26,447 [ERROR] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker startup time out.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:143) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:26,448 [ERROR] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker startup time out.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:143) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:26,436 [ERROR] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker startup time out.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:143) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:26,449 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T23:39:26,459 [ERROR] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker startup time out.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:143) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:26,449 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T23:39:26,460 [ERROR] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker startup time out.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:143) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:26,449 [INFO ] W-9000-sentencemoji_1.1-stdout MODEL_LOG - [PID]38936
2022-02-09T23:39:26,462 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T23:39:26,462 [INFO ] W-9000-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-sentencemoji_1.1-stdout
2022-02-09T23:39:26,447 [INFO ] W-9005-sentencemoji_1.1-stdout MODEL_LOG - [PID]37272
2022-02-09T23:39:26,466 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T23:39:26,446 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:218.77772903442383|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644411606
2022-02-09T23:39:26,445 [INFO ] W-9007-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9007-sentencemoji_1.1-stdout
2022-02-09T21:59:19,883 [INFO ] W-9004-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T23:39:26,476 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T23:39:26,470 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:94.0|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644411606
2022-02-09T23:39:26,466 [INFO ] W-9005-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9005-sentencemoji_1.1-stdout
2022-02-09T23:39:26,460 [ERROR] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker startup time out.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:143) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:26,460 [INFO ] W-9003-sentencemoji_1.1-stdout MODEL_LOG - [PID]244
2022-02-09T23:39:26,481 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T23:39:26,459 [ERROR] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker startup time out.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:143) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:26,485 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STOPPED
2022-02-09T23:39:26,485 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STOPPED
2022-02-09T23:39:26,459 [INFO ] W-9002-sentencemoji_1.1-stdout MODEL_LOG - [PID]38916
2022-02-09T23:39:26,486 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T23:39:26,486 [INFO ] W-9002-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-sentencemoji_1.1-stdout
2022-02-09T23:39:26,436 [ERROR] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker startup time out.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:143) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:26,490 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STOPPED
2022-02-09T23:39:26,450 [ERROR] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker startup time out.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:143) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:26,491 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STOPPED
2022-02-09T23:39:26,448 [ERROR] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker startup time out.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:143) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:26,493 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STOPPED
2022-02-09T23:39:26,447 [ERROR] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker startup time out.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:143) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:26,496 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STOPPED
2022-02-09T23:39:26,450 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T23:39:26,498 [ERROR] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker startup time out.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:143) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:26,493 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9000-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STOPPED
2022-02-09T23:39:26,507 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T23:39:26,491 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9004-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STOPPED
2022-02-09T23:39:26,509 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T23:39:26,490 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9007-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STOPPED
2022-02-09T23:39:26,509 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T23:39:26,485 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9002-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STOPPED
2022-02-09T23:39:26,510 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T23:39:26,485 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9003-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STOPPED
2022-02-09T23:39:26,511 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T23:39:26,481 [INFO ] W-9003-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-sentencemoji_1.1-stdout
2022-02-09T23:39:26,477 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:3855.30859375|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644411606
2022-02-09T23:39:26,476 [INFO ] W-9004-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-sentencemoji_1.1-stdout
2022-02-09T23:39:26,511 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stderr
2022-02-09T23:39:26,515 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T23:39:26,510 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stderr
2022-02-09T23:39:26,516 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T23:39:26,509 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stderr
2022-02-09T23:39:26,517 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T23:39:26,509 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stderr
2022-02-09T23:39:26,518 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T23:39:26,507 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stderr
2022-02-09T23:39:26,519 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T23:39:26,498 [ERROR] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker startup time out.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:143) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:283) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:179) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:26,498 [INFO ] W-9006-sentencemoji_1.1-stdout MODEL_LOG - [PID]30208
2022-02-09T23:39:26,522 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STOPPED
2022-02-09T23:39:26,522 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T23:39:26,496 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9005-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STOPPED
2022-02-09T23:39:26,523 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T23:39:26,522 [INFO ] W-9006-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9006-sentencemoji_1.1-stdout
2022-02-09T23:39:26,519 [WARN ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-sentencemoji_1.1-stdout
2022-02-09T23:39:26,526 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-02-09T23:39:26,518 [WARN ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-sentencemoji_1.1-stdout
2022-02-09T23:39:26,527 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2022-02-09T23:39:26,517 [WARN ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9007-sentencemoji_1.1-stdout
2022-02-09T23:39:26,536 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 2 seconds.
2022-02-09T23:39:26,516 [WARN ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-sentencemoji_1.1-stdout
2022-02-09T23:39:26,538 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2022-02-09T23:39:26,515 [WARN ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-sentencemoji_1.1-stdout
2022-02-09T23:39:26,539 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2022-02-09T23:39:26,513 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:12322.72265625|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644411606
2022-02-09T23:39:26,538 [INFO ] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.
2022-02-09T23:39:26,536 [INFO ] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9007 in 2 seconds.
2022-02-09T23:39:26,527 [INFO ] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 2 seconds.
2022-02-09T23:39:26,526 [INFO ] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2022-02-09T23:39:26,522 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9006-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STOPPED
2022-02-09T23:39:26,523 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stderr
2022-02-09T23:39:26,545 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T23:39:26,545 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T23:39:26,540 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:76.2|#Level:Host|#hostname:DESKTOP-AOFSBVV,timestamp:1644411606
2022-02-09T23:39:26,539 [INFO ] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.
2022-02-09T23:39:26,545 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stderr
2022-02-09T23:39:26,548 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T23:39:26,545 [WARN ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9005-sentencemoji_1.1-stdout
2022-02-09T23:39:26,549 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2022-02-09T23:39:26,548 [WARN ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9006-sentencemoji_1.1-stdout
2022-02-09T23:39:26,550 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 2 seconds.
2022-02-09T23:39:26,549 [INFO ] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9005 in 2 seconds.
2022-02-09T23:39:26,550 [INFO ] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9006 in 2 seconds.
2022-02-09T23:39:27,054 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('45060', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=29480)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 29480

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('29480',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('29480', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=39416)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 39416

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('39416',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('39416', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=16904)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 16904

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('16904',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('16904', 0)
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='cp949'>
OSError: [Errno 22] Invalid argument

2022-02-09T23:39:27,054 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('45060', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=29480)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 29480

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('29480',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('29480', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=39416)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 39416

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('39416',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('39416', 0)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 679, in wrapper
    return fun(self, *args, **kwargs)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 933, in create_time
    user, system, created = cext.proc_times(self.pid)
ProcessLookupError: [Errno 3] assume no such process (originated from GetExitCodeProcess != STILL_ACTIVE)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 354, in _init
    self.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 710, in create_time
    self._create_time = self._proc.create_time()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\_pswindows.py", line 681, in wrapper
    raise convert_oserror(err, pid=self.pid, name=self._name)
psutil.NoSuchProcess: psutil.NoSuchProcess process no longer exists (pid=16904)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 20, in get_cpu_usage
    process = psutil.Process(int(pid))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 326, in __init__
    self._init(pid)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\psutil\__init__.py", line 367, in _init
    raise NoSuchProcess(pid, None, msg)
psutil.NoSuchProcess: psutil.NoSuchProcess no process found with pid 16904

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 22, in get_cpu_usage
    logging.error("Failed get process for pid: %s", pid, exc_info=True)
Message: 'Failed get process for pid: %s'
Arguments: ('16904',)
--- Logging error ---
Traceback (most recent call last):
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1038, in emit
    self.flush()
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\logging\__init__.py", line 1018, in flush
    self.stream.flush()
OSError: [Errno 22] Invalid argument
Call stack:
  File "ts/metrics/metric_collector.py", line 18, in <module>
    check_process_mem_usage(sys.stdin)
  File "c:\users\enoch9\appdata\local\continuum\anaconda3\lib\site-packages\ts\metrics\process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('16904', 0)
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='cp949'>
OSError: [Errno 22] Invalid argument

2022-02-09T23:39:28,005 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Listening on port: None
2022-02-09T23:39:28,006 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - [PID]22360
2022-02-09T23:39:28,008 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T23:39:28,008 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Torch worker started.
2022-02-09T23:39:28,008 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STOPPED -> WORKER_STARTED
2022-02-09T23:39:28,009 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Python runtime: 3.7.3
2022-02-09T23:39:28,012 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T23:39:28,012 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9001
2022-02-09T23:39:28,022 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9001).
2022-02-09T23:39:28,023 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644417568023
2022-02-09T23:39:28,023 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1644417568023
2022-02-09T23:39:28,027 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - model_name: sentencemoji, batchSize: 1
2022-02-09T23:39:28,535 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T23:39:28,535 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T23:39:28,535 [DEBUG] W-9000-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000]
2022-02-09T23:39:28,535 [DEBUG] W-9004-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9004]
2022-02-09T23:39:28,551 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T23:39:28,552 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T23:39:28,552 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T23:39:28,551 [DEBUG] W-9002-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9002]
2022-02-09T23:39:28,552 [DEBUG] W-9007-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9007]
2022-02-09T23:39:28,552 [DEBUG] W-9003-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9003]
2022-02-09T23:39:28,568 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T23:39:28,568 [DEBUG] W-9005-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9005]
2022-02-09T23:39:28,572 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T23:39:28,572 [DEBUG] W-9006-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [c:\users\enoch9\appdata\local\continuum\anaconda3\python.exe, C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9006]
2022-02-09T23:39:28,743 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - {'model_dir': 'C:\\Users\\enoch9\\AppData\\Local\\Temp\\models\\5c86bc825e4e4867a27e97d281f9f414', 'gpu_id': None, 'batch_size': 1, 'server_name': 'MMS', 'server_version': '0.5.2', 'limit_max_image_pixels': True}
2022-02-09T23:39:28,745 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T23:39:28,744 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - C:\Users\enoch9\AppData\Local\Temp\models\5c86bc825e4e4867a27e97d281f9f414
2022-02-09T23:39:28,745 [INFO ] nioEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2022-02-09T23:39:28,748 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T23:39:28,748 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Backend worker process died.
2022-02-09T23:39:28,748 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2022-02-09T23:39:28,750 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG - Traceback (most recent call last):
2022-02-09T23:39:28,750 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:28,751 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 189, in <module>
2022-02-09T23:39:28,750 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:189) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2022-02-09T23:39:28,753 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     worker.run_server()
2022-02-09T23:39:28,754 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T23:39:28,754 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.BatchAggregator - Load model failed: sentencemoji, error: Worker died.
2022-02-09T23:39:28,756 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T23:39:28,754 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 161, in run_server
2022-02-09T23:39:28,756 [DEBUG] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - W-9001-sentencemoji_1.1 State change WORKER_STARTED -> WORKER_STOPPED
2022-02-09T23:39:28,758 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2022-02-09T23:39:28,758 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 123, in handle_connection
2022-02-09T23:39:28,760 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T23:39:28,760 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stderr
2022-02-09T23:39:28,761 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T23:39:28,760 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2022-02-09T23:39:28,761 [WARN ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-sentencemoji_1.1-stdout
2022-02-09T23:39:28,761 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.
2022-02-09T23:39:28,761 [INFO ] W-9001-sentencemoji_1.1-stdout MODEL_LOG -   File "C:\Users\enoch9\AppData\Local\Continuum\anaconda3\Lib\site-packages\ts\model_service_worker.py", line 96, in load_model
2022-02-09T23:39:28,763 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T23:39:28,761 [INFO ] W-9001-sentencemoji_1.1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.
2022-02-09T23:39:28,763 [INFO ] W-9001-sentencemoji_1.1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stdout
2022-02-09T23:39:28,813 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
2022-02-09T23:39:28,813 [INFO ] W-9001-sentencemoji_1.1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-sentencemoji_1.1-stderr
